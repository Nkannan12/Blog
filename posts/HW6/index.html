<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.542">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Narayanan Kannan">
<meta name="dcterms.date" content="2024-03-11">

<title>myblog - Identifying Fake News Through Text Classification and Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">myblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Identifying Fake News Through Text Classification and Machine Learning</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">HW</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Narayanan Kannan </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Today, we’re going to sift through loads of news data in an attempt to create a model that can accurately identify “fake news” from a host of articles. To do this, we’re going to use keras and tensorflow to create models for text classification. We’ll use most of the following imports below:</p>
<div id="cell-2" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"KERAS_BACKEND"</span>] <span class="op">=</span> <span class="st">"tensorflow"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> string</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> keras</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> layers, losses</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> TextVectorization</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_datasets <span class="im">as</span> tfds</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="dataset-prep" class="level1">
<h1>Dataset Prep</h1>
<p>We’re going to take data from the below url, and create a function that creates a dataset from a dataframe made from the csv.</p>
<div id="cell-4" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>train_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(train_url)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-5" class="cell" data-outputid="d571add5-6705-4352-9c18-32672f932beb" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">

  <div id="df-7d4f9420-999b-4079-a1cb-3e88bc0463e5" class="colab-df-container">
    <div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Unnamed: 0</th>
<th data-quarto-table-cell-role="th">title</th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">fake</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>17366</td>
<td>Merkel: Strong result for Austria's FPO 'big c...</td>
<td>German Chancellor Angela Merkel said on Monday...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>5634</td>
<td>Trump says Pence will lead voter fraud panel</td>
<td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>17487</td>
<td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
<td>On December 5, 2017, Circa s Sara Carter warne...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>12217</td>
<td>Thyssenkrupp has offered help to Argentina ove...</td>
<td>Germany s Thyssenkrupp, has offered assistance...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>5535</td>
<td>Trump say appeals court decision on travel ban...</td>
<td>President Donald Trump on Thursday called the ...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22444</td>
<td>10709</td>
<td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
<td>If Clinton and Lynch just talked about grandki...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">22445</td>
<td>8731</td>
<td>Can Pence's vow not to sling mud survive a Tru...</td>
<td>() - In 1990, during a close and bitter congre...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22446</td>
<td>4733</td>
<td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
<td>A new ad by the Hillary Clinton SuperPac Prior...</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">22447</td>
<td>3993</td>
<td>Trump celebrates first 100 days as president, ...</td>
<td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">22448</td>
<td>12896</td>
<td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
<td>MELBOURNE, FL is a town with a population of 7...</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>22449 rows × 4 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-7d4f9420-999b-4079-a1cb-3e88bc0463e5')" title="Convert this dataframe to an interactive table." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"></path>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-7d4f9420-999b-4079-a1cb-3e88bc0463e5 button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-7d4f9420-999b-4079-a1cb-3e88bc0463e5');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-133c1251-a18a-46d6-95ef-4184e35f6b27">
  <button class="colab-df-quickchart" onclick="quickchart('df-133c1251-a18a-46d6-95ef-4184e35f6b27')" title="Suggest charts" style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"></path>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-133c1251-a18a-46d6-95ef-4184e35f6b27 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>

  <div id="id_2649f719-e298-4bc5-9ab3-782c58f9a64f">
    <style>
      .colab-df-generate {
        background-color: #E8F0FE;
        border: none;
        border-radius: 50%;
        cursor: pointer;
        display: none;
        fill: #1967D2;
        height: 32px;
        padding: 0 0 0 0;
        width: 32px;
      }

      .colab-df-generate:hover {
        background-color: #E2EBFA;
        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
        fill: #174EA6;
      }

      [theme=dark] .colab-df-generate {
        background-color: #3B4455;
        fill: #D2E3FC;
      }

      [theme=dark] .colab-df-generate:hover {
        background-color: #434B5C;
        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
        fill: #FFFFFF;
      }
    </style>
    <button class="colab-df-generate" onclick="generateWithVariable('df')" title="Generate code using this dataframe." style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewbox="0 0 24 24" width="24px">
    <path d="M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z"></path>
  </svg>
    </button>
    <script>
      (() => {
      const buttonEl =
        document.querySelector('#id_2649f719-e298-4bc5-9ab3-782c58f9a64f button.colab-df-generate');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      buttonEl.onclick = () => {
        google.colab.notebook.generateWithVariable('df');
      }
      })();
    </script>
  </div>

    </div>
  </div>
</div>
</div>
<p>The two things we want to do before creating the dataset are converting all letters in the dataframe columns “text” and “title” to lowercase and removing stopwords, such as “the,” “at,” and more. To remove stopwords, we need a database of stopwords to watch out for, which is found when importing the nltk library and downloading stopwords.</p>
<div id="cell-7" class="cell" data-outputid="c83d6016-d55e-441c-fa73-2c80dced06b9" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to /root/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>True</code></pre>
</div>
</div>
<div id="cell-8" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>stop <span class="op">=</span> stopwords.words(<span class="st">'english'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Our function starts by converting the aforementioned text to lowercase using the .lower() function (after .str() to make sure our values are strings). To remove the stopwords, we use the below lambda function in the .apply() function which only keeps words that aren’t in stop, our list of bad words. Finally, we create a tensorflow dataset out of a tuple of dictionaries named based off the column names in our dataframe. For ease of use, we implement the .batch(100) function so that our dataset unloads data 100 elements at a time, allowing for a streamlined training process with negligible accuracy costs.</p>
<div id="cell-10" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_dataset(df):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'title'</span>] <span class="op">=</span> df[<span class="st">'title'</span>].<span class="bu">str</span>.lower()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">str</span>.lower()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'title'</span>] <span class="op">=</span> df[<span class="st">'title'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  df[<span class="st">'text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join([word <span class="cf">for</span> word <span class="kw">in</span> x.split() <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop]))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  dataset <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>      {<span class="st">'title'</span> : df[<span class="st">'title'</span>], <span class="st">'text'</span> : df[<span class="st">'text'</span>]}, {<span class="st">'fake'</span> : df[<span class="st">'fake'</span>]}</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>  dataset <span class="op">=</span> dataset.shuffle(buffer_size <span class="op">=</span> <span class="bu">len</span>(df), reshuffle_each_iteration<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  dataset <span class="op">=</span> dataset.batch(<span class="dv">100</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> dataset</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We make a dataset, and want to split it into training and validation. A good split would be 80% train, 20% validation. This process is done below.</p>
<div id="cell-12" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> make_dataset(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-13" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>train_val_split <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.2</span><span class="op">*</span><span class="bu">len</span>(dataset))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> dataset.skip(train_val_split)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> dataset.take(train_val_split)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you may recall, a baseline machine learning model in theory just guesses the majority class in the set. We can calculate the base_rate by counting the number of fake news citings in the training set, and dividing it by the total number of elements in the training set. It turns out fake news citings are the majority (barely). Thus, our base rate is approximately 52%.</p>
<div id="cell-15" class="cell" data-outputid="a8a063d5-615a-4604-ad68-eb9f24f86451" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>num_fake <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, label <span class="kw">in</span> train_ds:</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    num_fake <span class="op">+=</span> np.<span class="bu">sum</span>(label[<span class="st">'fake'</span>])</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>base_rate <span class="op">=</span> num_fake<span class="op">/</span>(<span class="bu">len</span>(train_ds)<span class="op">*</span><span class="dv">100</span>) <span class="co"># number of fakes over total number of elements in the training set</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>base_rate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>0.5247777777777778</code></pre>
</div>
</div>
<p>The next thing to do is prepare a vectorization layer that can be implemented into our models. This is made up of a standardization step, and multiple parameters outlining the total number of words to deal with, the output type, and sequence length. Below, we create a vectorization layer for the “title” aspect of each entry in our dataset. If we wanted to, we could create a vectorization layer for the “title” as well (which is shown in the comments below, it’s pretty much the same thing), but this would be redundant, as we would be vectorizing the same piece of data multiple times over, then embedding it twice as well.</p>
<div id="cell-17" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">#preparing a text vectorization layer for tf model</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>size_vocabulary <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> standardization(input_data):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    lowercase <span class="op">=</span> tf.strings.lower(input_data)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    no_punctuation <span class="op">=</span> tf.strings.regex_replace(lowercase,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="st">'[</span><span class="sc">%s</span><span class="st">]'</span> <span class="op">%</span> re.escape(string.punctuation),<span class="st">''</span>)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> no_punctuation</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer <span class="op">=</span> TextVectorization(</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    standardize<span class="op">=</span>standardization,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    max_tokens<span class="op">=</span>size_vocabulary, <span class="co"># only consider this many words</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    output_mode<span class="op">=</span><span class="st">'int'</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    output_sequence_length<span class="op">=</span><span class="dv">500</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># text_vectorize_layer = TextVectorization(</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># standardize=standardization,</span></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># max_tokens=size_vocabulary,</span></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output_mode='int',</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># output_sequence_length=500)</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>title_vectorize_layer.adapt(train_ds.<span class="bu">map</span>(<span class="kw">lambda</span> x, y: x[<span class="st">'title'</span>]))</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># text_vectorize_layer.adapt(train_ds.map(lambda x, y: x['text']))</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we have to outline the inputs to our function. On one hand, we have the “title” input, everything seen in the title column of our dataset. On the other hand, we have the “text” input. Since they are both long strings, the semantics are the same.</p>
<div id="cell-19" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># inputs</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>title_input <span class="op">=</span> keras.Input(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    shape <span class="op">=</span> (<span class="dv">1</span>,),</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"title"</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    dtype <span class="op">=</span> <span class="st">"string"</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>text_input <span class="op">=</span> keras.Input(</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    shape <span class="op">=</span> (<span class="dv">1</span>,),</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    name <span class="op">=</span> <span class="st">"text"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    dtype <span class="op">=</span> <span class="st">"string"</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="model-creation-our-first-title-based-model" class="level1">
<h1>Model Creation – Our First, “Title”-based, Model</h1>
<p>Today, we want to answer the question “<em>When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?</em>” We’ll do this by creating three models. Our first model is only going to process features found in the “title” category of our input data. We use the pipeline seen below. The most important parts are the vectorization layer and the embedding layer, as these help us the most by clarifying what is what in the data. We end with an output layer that classifys what each training sample is.</p>
<div id="cell-21" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># layers for processing the title</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> title_vectorize_layer(title_input)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>, name <span class="op">=</span> <span class="st">"embedding_title"</span>)(title_features)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.GlobalAveragePooling1D()(title_features)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(title_features)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>title_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(title_features)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>title_output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name<span class="op">=</span><span class="st">'fake'</span>)(title_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Instead of using the keras.Sequential API, we use the keras.Functional API to create these models. These consists of inputs which are passed through our features to an output. The hyperparameters and elements such as the optimizer and loss function as the same as always and held constant through our whole experimentation process.</p>
<div id="cell-23" class="cell" data-outputid="19e4600d-09ed-4386-b232-ecbd6f528c26" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>model1 <span class="op">=</span> keras.Model(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> title_input,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> title_output</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>model1.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>model1.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 title (InputLayer)          [(None, 1)]               0         
                                                                 
 text_vectorization (TextVe  (None, 500)               0         
 ctorization)                                                    
                                                                 
 embedding_title (Embedding  (None, 500, 3)            6000      
 )                                                               
                                                                 
 dropout (Dropout)           (None, 500, 3)            0         
                                                                 
 global_average_pooling1d (  (None, 3)                 0         
 GlobalAveragePooling1D)                                         
                                                                 
 dropout_1 (Dropout)         (None, 3)                 0         
                                                                 
 dense (Dense)               (None, 32)                128       
                                                                 
 fake (Dense)                (None, 2)                 66        
                                                                 
=================================================================
Total params: 6194 (24.20 KB)
Trainable params: 6194 (24.20 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
<p>As you can see, when just processing the title, we achieve an accuracy of around 92% to 95%. This is pretty good! But I think we can do better.</p>
<div id="cell-25" class="cell" data-outputid="ffce0c84-334b-4a02-9c45-1d8b20dd23a5" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>callback <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">'val_loss'</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model1.fit(train_ds,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val_ds,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[callback],</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/50
180/180 [==============================] - 23s 91ms/step - loss: 0.6918 - accuracy: 0.5262 - val_loss: 0.6945 - val_accuracy: 0.5098
Epoch 2/50
180/180 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6917 - val_accuracy: 0.5098
Epoch 3/50
180/180 [==============================] - 1s 6ms/step - loss: 0.6828 - accuracy: 0.5473 - val_loss: 0.6748 - val_accuracy: 0.5098
Epoch 4/50
180/180 [==============================] - 1s 8ms/step - loss: 0.6420 - accuracy: 0.7066 - val_loss: 0.5940 - val_accuracy: 0.8536
Epoch 5/50
180/180 [==============================] - 1s 6ms/step - loss: 0.5359 - accuracy: 0.8229 - val_loss: 0.4660 - val_accuracy: 0.8664
Epoch 6/50
180/180 [==============================] - 1s 5ms/step - loss: 0.4290 - accuracy: 0.8579 - val_loss: 0.3704 - val_accuracy: 0.8824
Epoch 7/50
180/180 [==============================] - 1s 6ms/step - loss: 0.3601 - accuracy: 0.8706 - val_loss: 0.3100 - val_accuracy: 0.8987
Epoch 8/50
180/180 [==============================] - 1s 6ms/step - loss: 0.3128 - accuracy: 0.8821 - val_loss: 0.2698 - val_accuracy: 0.9100
Epoch 9/50
180/180 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8969 - val_loss: 0.2389 - val_accuracy: 0.9173
Epoch 10/50
180/180 [==============================] - 1s 6ms/step - loss: 0.2565 - accuracy: 0.9039 - val_loss: 0.2209 - val_accuracy: 0.9209
Epoch 11/50
180/180 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.9101 - val_loss: 0.2015 - val_accuracy: 0.9260
Epoch 12/50
180/180 [==============================] - 1s 6ms/step - loss: 0.2252 - accuracy: 0.9134 - val_loss: 0.1895 - val_accuracy: 0.9293
Epoch 13/50
180/180 [==============================] - 1s 7ms/step - loss: 0.2101 - accuracy: 0.9194 - val_loss: 0.1823 - val_accuracy: 0.9296
Epoch 14/50
180/180 [==============================] - 2s 9ms/step - loss: 0.1986 - accuracy: 0.9259 - val_loss: 0.1724 - val_accuracy: 0.9358
Epoch 15/50
180/180 [==============================] - 2s 10ms/step - loss: 0.1888 - accuracy: 0.9256 - val_loss: 0.1665 - val_accuracy: 0.9389
Epoch 16/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1833 - accuracy: 0.9296 - val_loss: 0.1636 - val_accuracy: 0.9373
Epoch 17/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1788 - accuracy: 0.9304 - val_loss: 0.1575 - val_accuracy: 0.9424
Epoch 18/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1725 - accuracy: 0.9323 - val_loss: 0.1542 - val_accuracy: 0.9427
Epoch 19/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1670 - accuracy: 0.9341 - val_loss: 0.1518 - val_accuracy: 0.9431
Epoch 20/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1633 - accuracy: 0.9343 - val_loss: 0.1560 - val_accuracy: 0.9391
Epoch 21/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1609 - accuracy: 0.9382 - val_loss: 0.1488 - val_accuracy: 0.9436
Epoch 22/50
180/180 [==============================] - 1s 7ms/step - loss: 0.1571 - accuracy: 0.9386 - val_loss: 0.1473 - val_accuracy: 0.9440
Epoch 23/50
180/180 [==============================] - 2s 8ms/step - loss: 0.1536 - accuracy: 0.9395 - val_loss: 0.1484 - val_accuracy: 0.9420
Epoch 24/50
180/180 [==============================] - 1s 7ms/step - loss: 0.1531 - accuracy: 0.9368 - val_loss: 0.1479 - val_accuracy: 0.9418
Epoch 25/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1477 - accuracy: 0.9403 - val_loss: 0.1457 - val_accuracy: 0.9424
Epoch 26/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1509 - accuracy: 0.9397 - val_loss: 0.1442 - val_accuracy: 0.9440
Epoch 27/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1441 - accuracy: 0.9429 - val_loss: 0.1433 - val_accuracy: 0.9447
Epoch 28/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1426 - accuracy: 0.9428 - val_loss: 0.1446 - val_accuracy: 0.9438
Epoch 29/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1405 - accuracy: 0.9448 - val_loss: 0.1435 - val_accuracy: 0.9449
Epoch 30/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1391 - accuracy: 0.9444 - val_loss: 0.1449 - val_accuracy: 0.9427
Epoch 31/50
180/180 [==============================] - 1s 5ms/step - loss: 0.1380 - accuracy: 0.9461 - val_loss: 0.1426 - val_accuracy: 0.9451
Epoch 32/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1356 - accuracy: 0.9440 - val_loss: 0.1491 - val_accuracy: 0.9402
Epoch 33/50
180/180 [==============================] - 2s 12ms/step - loss: 0.1335 - accuracy: 0.9464 - val_loss: 0.1490 - val_accuracy: 0.9411
Epoch 34/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1335 - accuracy: 0.9460 - val_loss: 0.1470 - val_accuracy: 0.9436
Epoch 35/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1345 - accuracy: 0.9456 - val_loss: 0.1438 - val_accuracy: 0.9440
Epoch 36/50
180/180 [==============================] - 1s 6ms/step - loss: 0.1307 - accuracy: 0.9485 - val_loss: 0.1473 - val_accuracy: 0.9433</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-outputid="ab5ba6d9-4462-4f2c-9d6b-1df75771a9a6" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Visualizing Our First Model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Text(0.5, 1.0, 'Visualizing Our First Model')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Before we jump into the next model, here’s a streamlined look at the current one. It’s really not that complex, but it more than gets the job done.</p>
<div id="cell-28" class="cell" data-outputid="34b363b1-cbb2-4c4b-c8a3-7a6c90d3c407" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model1, <span class="st">"model1.png"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="a-model-that-only-looks-at-text-data" class="level1">
<h1>A Model That Only Looks at Text Data</h1>
<p>Now we’ll tackle the second idea posed in the question – how good is a model that only processes the text of an article. I hypothesize that this will be a little bit better than just looking at the title. The body of the article is a lot longer, and gives us a better idea of what’s going on. We use the same model structure for this model, only changing the input and embedding layer.</p>
<div id="cell-30" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># layers for processing the text</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> title_vectorize_layer(text_input)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Embedding(size_vocabulary, <span class="dv">3</span>, name <span class="op">=</span> <span class="st">"embedding_text"</span>)(text_features)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.GlobalAveragePooling1D()(text_features)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dropout(<span class="fl">0.2</span>)(text_features)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> layers.Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">'relu'</span>)(text_features)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>text_output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name<span class="op">=</span><span class="st">'fake'</span>)(text_features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-31" class="cell" data-outputid="220bddaf-5a76-4557-cd0a-45da60d3d842" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>model2 <span class="op">=</span> keras.Model(</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> text_input,</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> text_output</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>model2.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>model2.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text (InputLayer)           [(None, 1)]               0         
                                                                 
 text_vectorization (TextVe  (None, 500)               0         
 ctorization)                                                    
                                                                 
 embedding_text (Embedding)  (None, 500, 3)            6000      
                                                                 
 dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                 
 global_average_pooling1d_1  (None, 3)                 0         
  (GlobalAveragePooling1D)                                       
                                                                 
 dropout_3 (Dropout)         (None, 3)                 0         
                                                                 
 dense_1 (Dense)             (None, 32)                128       
                                                                 
 fake (Dense)                (None, 2)                 66        
                                                                 
=================================================================
Total params: 6194 (24.20 KB)
Trainable params: 6194 (24.20 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________</code></pre>
</div>
</div>
<p>At its peak, this model reaches around 97% accuracy, which is incrementally better than our original one. So my hypothesis was correct! Still, I think we can do <em>even</em> better.</p>
<div id="cell-33" class="cell" data-outputid="a37b2993-c873-4d22-d09a-2a729987aca4" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>callback <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">'val_loss'</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model2.fit(train_ds,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val_ds,</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[callback],</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/50
180/180 [==============================] - 18s 92ms/step - loss: 0.6766 - accuracy: 0.5603 - val_loss: 0.6369 - val_accuracy: 0.6402
Epoch 2/50
180/180 [==============================] - 2s 12ms/step - loss: 0.5056 - accuracy: 0.8534 - val_loss: 0.3630 - val_accuracy: 0.9171
Epoch 3/50
180/180 [==============================] - 2s 11ms/step - loss: 0.2993 - accuracy: 0.9259 - val_loss: 0.2372 - val_accuracy: 0.9431
Epoch 4/50
180/180 [==============================] - 2s 11ms/step - loss: 0.2190 - accuracy: 0.9435 - val_loss: 0.1884 - val_accuracy: 0.9513
Epoch 5/50
180/180 [==============================] - 3s 15ms/step - loss: 0.1835 - accuracy: 0.9523 - val_loss: 0.1628 - val_accuracy: 0.9582
Epoch 6/50
180/180 [==============================] - 4s 20ms/step - loss: 0.1601 - accuracy: 0.9588 - val_loss: 0.1463 - val_accuracy: 0.9620
Epoch 7/50
180/180 [==============================] - 2s 13ms/step - loss: 0.1445 - accuracy: 0.9609 - val_loss: 0.1366 - val_accuracy: 0.9644
Epoch 8/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1318 - accuracy: 0.9646 - val_loss: 0.1285 - val_accuracy: 0.9656
Epoch 9/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1242 - accuracy: 0.9656 - val_loss: 0.1227 - val_accuracy: 0.9669
Epoch 10/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1169 - accuracy: 0.9674 - val_loss: 0.1187 - val_accuracy: 0.9680
Epoch 11/50
180/180 [==============================] - 3s 17ms/step - loss: 0.1111 - accuracy: 0.9677 - val_loss: 0.1152 - val_accuracy: 0.9689
Epoch 12/50
180/180 [==============================] - 2s 11ms/step - loss: 0.1051 - accuracy: 0.9708 - val_loss: 0.1121 - val_accuracy: 0.9709
Epoch 13/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0993 - accuracy: 0.9712 - val_loss: 0.1097 - val_accuracy: 0.9709
Epoch 14/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0944 - accuracy: 0.9749 - val_loss: 0.1076 - val_accuracy: 0.9713
Epoch 15/50
180/180 [==============================] - 3s 14ms/step - loss: 0.0914 - accuracy: 0.9747 - val_loss: 0.1063 - val_accuracy: 0.9736
Epoch 16/50
180/180 [==============================] - 3s 15ms/step - loss: 0.0877 - accuracy: 0.9743 - val_loss: 0.1048 - val_accuracy: 0.9742
Epoch 17/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0851 - accuracy: 0.9759 - val_loss: 0.1042 - val_accuracy: 0.9740
Epoch 18/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0799 - accuracy: 0.9766 - val_loss: 0.1024 - val_accuracy: 0.9742
Epoch 19/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0795 - accuracy: 0.9765 - val_loss: 0.1021 - val_accuracy: 0.9742
Epoch 20/50
180/180 [==============================] - 3s 17ms/step - loss: 0.0754 - accuracy: 0.9779 - val_loss: 0.1024 - val_accuracy: 0.9736
Epoch 21/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0748 - accuracy: 0.9787 - val_loss: 0.1016 - val_accuracy: 0.9736
Epoch 22/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0715 - accuracy: 0.9782 - val_loss: 0.1031 - val_accuracy: 0.9716
Epoch 23/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0685 - accuracy: 0.9781 - val_loss: 0.1023 - val_accuracy: 0.9720
Epoch 24/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0682 - accuracy: 0.9791 - val_loss: 0.1013 - val_accuracy: 0.9718
Epoch 25/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0672 - accuracy: 0.9797 - val_loss: 0.1054 - val_accuracy: 0.9711
Epoch 26/50
180/180 [==============================] - 3s 15ms/step - loss: 0.0642 - accuracy: 0.9806 - val_loss: 0.1023 - val_accuracy: 0.9718
Epoch 27/50
180/180 [==============================] - 2s 11ms/step - loss: 0.0635 - accuracy: 0.9803 - val_loss: 0.1019 - val_accuracy: 0.9727
Epoch 28/50
180/180 [==============================] - 3s 15ms/step - loss: 0.0604 - accuracy: 0.9822 - val_loss: 0.1042 - val_accuracy: 0.9704
Epoch 29/50
180/180 [==============================] - 3s 16ms/step - loss: 0.0621 - accuracy: 0.9809 - val_loss: 0.1049 - val_accuracy: 0.9709</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-outputid="7b4f14da-6b04-4e1d-bda6-0f30e5b2d610" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Visualizing Our Second Model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>Text(0.5, 1.0, 'Visualizing Our Second Model')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As I said before, the backbone of this model is the same as the model processing the titles.</p>
<div id="cell-36" class="cell" data-outputid="944865ba-2910-4430-8797-2063302c7814" data-execution_count="23">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model2, <span class="st">"model1.png"</span>,</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-22-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="an-all-encompassing-model" class="level1">
<h1>An All-Encompassing Model</h1>
<p>Our final model should be the best. It takes into account both the title data and text data, and should provide us with the fullest picture of what’s happening in each article. In this model, we concatenate the features of both other models in order to process the title and text data in tandem before finally feeding to a dense layer for classification.</p>
<div id="cell-38" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># processing both the title and text together</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>main <span class="op">=</span> layers.concatenate([title_features, text_features], axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> layers.Dense(<span class="dv">2</span>, name <span class="op">=</span> <span class="st">'fake'</span>)(main)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-39" class="cell" data-outputid="6fe5321e-ca25-4ad8-d893-738e98e7907d" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>model3 <span class="op">=</span> keras.Model(</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> [title_input, text_input],</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> output</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>model3.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>model3.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_2"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 title (InputLayer)          [(None, 1)]                  0         []                            
                                                                                                  
 text (InputLayer)           [(None, 1)]                  0         []                            
                                                                                                  
 text_vectorization (TextVe  (None, 500)                  0         ['title[0][0]',               
 ctorization)                                                        'text[0][0]']                
                                                                                                  
 embedding_title (Embedding  (None, 500, 3)               6000      ['text_vectorization[0][0]']  
 )                                                                                                
                                                                                                  
 embedding_text (Embedding)  (None, 500, 3)               6000      ['text_vectorization[1][0]']  
                                                                                                  
 dropout (Dropout)           (None, 500, 3)               0         ['embedding_title[0][0]']     
                                                                                                  
 dropout_2 (Dropout)         (None, 500, 3)               0         ['embedding_text[0][0]']      
                                                                                                  
 global_average_pooling1d (  (None, 3)                    0         ['dropout[0][0]']             
 GlobalAveragePooling1D)                                                                          
                                                                                                  
 global_average_pooling1d_1  (None, 3)                    0         ['dropout_2[0][0]']           
  (GlobalAveragePooling1D)                                                                        
                                                                                                  
 dropout_1 (Dropout)         (None, 3)                    0         ['global_average_pooling1d[0][
                                                                    0]']                          
                                                                                                  
 dropout_3 (Dropout)         (None, 3)                    0         ['global_average_pooling1d_1[0
                                                                    ][0]']                        
                                                                                                  
 dense (Dense)               (None, 32)                   128       ['dropout_1[0][0]']           
                                                                                                  
 dense_1 (Dense)             (None, 32)                   128       ['dropout_3[0][0]']           
                                                                                                  
 concatenate (Concatenate)   (None, 64)                   0         ['dense[0][0]',               
                                                                     'dense_1[0][0]']             
                                                                                                  
 fake (Dense)                (None, 2)                    130       ['concatenate[0][0]']         
                                                                                                  
==================================================================================================
Total params: 12386 (48.38 KB)
Trainable params: 12386 (48.38 KB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________</code></pre>
</div>
</div>
<p>This model reaches 98%, and almost 99% validation accuracy. It is truly the culmination of all the work we’ve done so far. It is almost scarily accuracte. It could even be better than you or me!</p>
<div id="cell-41" class="cell" data-outputid="49fdf017-d214-4cf3-9e27-57d4e690bf29" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>callback <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">'val_loss'</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model3.fit(train_ds,</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>                    validation_data<span class="op">=</span>val_ds,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>                    epochs <span class="op">=</span> <span class="dv">50</span>,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>                    callbacks<span class="op">=</span>[callback],</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>                    verbose <span class="op">=</span> <span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/50
180/180 [==============================] - 21s 107ms/step - loss: 0.3944 - accuracy: 0.9381 - val_loss: 0.2057 - val_accuracy: 0.9820
Epoch 2/50
180/180 [==============================] - 2s 12ms/step - loss: 0.1435 - accuracy: 0.9875 - val_loss: 0.1038 - val_accuracy: 0.9820
Epoch 3/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0856 - accuracy: 0.9878 - val_loss: 0.0774 - val_accuracy: 0.9822
Epoch 4/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0647 - accuracy: 0.9890 - val_loss: 0.0656 - val_accuracy: 0.9818
Epoch 5/50
180/180 [==============================] - 3s 18ms/step - loss: 0.0528 - accuracy: 0.9894 - val_loss: 0.0593 - val_accuracy: 0.9820
Epoch 6/50
180/180 [==============================] - 3s 14ms/step - loss: 0.0462 - accuracy: 0.9894 - val_loss: 0.0556 - val_accuracy: 0.9831
Epoch 7/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0394 - accuracy: 0.9911 - val_loss: 0.0534 - val_accuracy: 0.9831
Epoch 8/50
180/180 [==============================] - 3s 14ms/step - loss: 0.0378 - accuracy: 0.9904 - val_loss: 0.0513 - val_accuracy: 0.9838
Epoch 9/50
180/180 [==============================] - 3s 19ms/step - loss: 0.0337 - accuracy: 0.9921 - val_loss: 0.0498 - val_accuracy: 0.9838
Epoch 10/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0321 - accuracy: 0.9918 - val_loss: 0.0487 - val_accuracy: 0.9838
Epoch 11/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0283 - accuracy: 0.9928 - val_loss: 0.0484 - val_accuracy: 0.9840
Epoch 12/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0265 - accuracy: 0.9930 - val_loss: 0.0473 - val_accuracy: 0.9838
Epoch 13/50
180/180 [==============================] - 3s 16ms/step - loss: 0.0244 - accuracy: 0.9938 - val_loss: 0.0465 - val_accuracy: 0.9844
Epoch 14/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0227 - accuracy: 0.9938 - val_loss: 0.0463 - val_accuracy: 0.9847
Epoch 15/50
180/180 [==============================] - 3s 14ms/step - loss: 0.0229 - accuracy: 0.9936 - val_loss: 0.0462 - val_accuracy: 0.9849
Epoch 16/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0200 - accuracy: 0.9941 - val_loss: 0.0480 - val_accuracy: 0.9833
Epoch 17/50
180/180 [==============================] - 3s 17ms/step - loss: 0.0209 - accuracy: 0.9944 - val_loss: 0.0466 - val_accuracy: 0.9856
Epoch 18/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0184 - accuracy: 0.9950 - val_loss: 0.0470 - val_accuracy: 0.9858
Epoch 19/50
180/180 [==============================] - 2s 13ms/step - loss: 0.0188 - accuracy: 0.9943 - val_loss: 0.0475 - val_accuracy: 0.9862
Epoch 20/50
180/180 [==============================] - 2s 12ms/step - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.0466 - val_accuracy: 0.9853</code></pre>
</div>
</div>
<div id="cell-42" class="cell" data-outputid="18bb7b2f-ed62-46da-9226-42b774e67a0d" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"accuracy"</span>], label <span class="op">=</span> <span class="st">"training"</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">"val_accuracy"</span>], label <span class="op">=</span> <span class="st">"validation"</span>)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>plt.gca().<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">"epoch"</span>, ylabel <span class="op">=</span> <span class="st">"accuracy"</span>)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Visualizing Our Third Model"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>Text(0.5, 1.0, 'Visualizing Our Third Model')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This model is more complex that both its predecessors because it’s a combination of them. It has about two times as many parameters and two times as many layers, with two separate embeddings for both title and text.</p>
<div id="cell-44" class="cell" data-outputid="4685d05c-5613-4005-a5bf-131b691645cf" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras <span class="im">import</span> utils</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>utils.plot_model(model2, <span class="st">"model1.png"</span>,</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>                       show_shapes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>                       show_layer_names<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="testing" class="level1">
<h1>Testing</h1>
<p>Of course, we want to see how our model works in a test run. Consider the following dataframe full of test data. Using the function we defined earlier, we can create a test dataset to evaluate our final model on.</p>
<div id="cell-46" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>test_url <span class="op">=</span> <span class="st">"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> pd.read_csv(test_url)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-47" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>test_ds <span class="op">=</span> make_dataset(test_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>It achieves around 98.6% validation accuracy. Pretty neat!</p>
<div id="cell-49" class="cell" data-outputid="05c4b47f-f082-469d-9710-352cf19f52ec" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>_, accuracy <span class="op">=</span> model3.evaluate(test_ds)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy on test set: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>225/225 [==============================] - 3s 12ms/step - loss: 0.0453 - accuracy: 0.9861
Accuracy on test set: 0.9861463904380798</code></pre>
</div>
</div>
</section>
<section id="considering-the-embeddings" class="level1">
<h1>Considering the Embeddings</h1>
<p>Now, lets consider how the model actualyl decides how to embed information. Below, we look at the embeddings of the title data.</p>
<div id="cell-51" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> model3.get_layer(<span class="st">'embedding_title'</span>).get_weights()[<span class="dv">0</span>] <span class="co"># get the weights from the embedding layer (only title here)</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> title_vectorize_layer.get_vocabulary() <span class="co"># retreieve all the words so visualization can be interactive</span></span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>) <span class="co"># reducing to 2 dimensions for easy visualization</span></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pca.fit_transform(weights)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>embedding_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'word'</span> : vocab,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'x-component'</span>   : weights[:,<span class="dv">0</span>],</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'y-component'</span>   : weights[:,<span class="dv">1</span>]</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-52" class="cell" data-outputid="65c1262c-2b46-42b5-95e2-7538a73f01f4" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="co"># make scatter plot of embeddings</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(embedding_df,</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>                 x <span class="op">=</span> <span class="st">"x-component"</span>,</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>                 y <span class="op">=</span> <span class="st">"y-component"</span>,</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>                 size <span class="op">=</span> <span class="bu">list</span>(np.ones(<span class="bu">len</span>(embedding_df))),</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>                 size_max <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>                 hover_name <span class="op">=</span> <span class="st">"word"</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">

<meta charset="utf-8">

    <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script charset="utf-8" src="https://cdn.plot.ly/plotly-2.24.1.min.js"></script>                <div id="1fab01ce-27d3-48b8-a5a3-68414ca80170" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("1fab01ce-27d3-48b8-a5a3-68414ca80170")) {                    Plotly.newPlot(                        "1fab01ce-27d3-48b8-a5a3-68414ca80170",                        [{"hovertemplate":"\u003cb\u003e%{hovertext}\u003c\u002fb\u003e\u003cbr\u003e\u003cbr\u003ex-component=%{x}\u003cbr\u003ey-component=%{y}\u003cbr\u003esize=%{marker.size}\u003cextra\u003e\u003c\u002fextra\u003e","hovertext":["","[UNK]","trump","video","us","says","obama","watch","house","hillary","new","president","trump\u2019s","clinton","white","bill","russia","state","republican","north","court","senate","election","donald","news","black","breaking","media","korea","calls","vote","republicans","police","\u2013","tax","gop","campaign","muslim","trumps","one","deal","may","gets","democrats","china","iran","obama\u2019s","tweets","former","un","first","fbi","government","russian","party","talks","people","back","security","congress","attack","pm","eu","syria","chief","top","fox","america","cnn","leader","judge","senator","tells","speech","democrat","ban","war","plan","twitter","law","sanders","man","military","say","minister","million","south","it\u2019s","would","makes","could","presidential","official","probe","report","shows","racist","factbox","make","goes","tweet","take","supreme","two","like","hillary\u2019s","brexit","americans","liberal","get","nuclear","border","healthcare","cruz","foreign","american","rally","support","sanctions","woman","wants","host","governor","political","gun","wow","debate","putin","syrian","illegal","bernie","islamic","help","want","time","fight","attacks","response","women","meet","visit","urges","rights","german","supporters","go","show","poll","killed","conservative","world","he\u2019s","race","trade","refugees","national","lawmakers","call","see","next","leaders","gives","big","turkey","states","group","day","budget","antitrump","travel","obamacare","win","uk","policy","details","ryan","don\u2019t","opposition","meeting","candidate","school","must","crisis","saudi","right","asks","room","old","mexico","got","death","ted","stop","texas","students","air","panel","going","fake","claims","won\u2019t","takes","sources","department","democratic","climate","left","warns","pick","hilarious","way","reporter","plans","officials","general","defense","cops","protesters","immigration","ties","mayor","caught","can\u2019t","voters","end","comey","rules","press","arrested","seeks","here\u2019s","city","administration","years","still","really","move","leftist","justice","huge","tillerson","free","emails","email","violence","washington","interview","wall","secretary","paul","aid","open","myanmar","made","head","federal","merkel","live","key","exclusive","last","boiler","voter","reason","push","presidency","money","major","john","work","secret","iraq","health","threatens","family","business","use","threat","team","reform","case","ahead","year","shocking","george","supporter","director","conservatives","truth","give","florida","run","kill","dead","times","said","need","lives","ep","coalition","york","never","legal","investigation","college","post","high","ever","change","attorney","lol","know","flag","bomb","social","shooting","sexual","list","lies","lawyer","home","germany","forces","control","britain","nominee","jerusalem","isis","image","force","boom","army","told","rohingya","lawmaker","japan","fire","2016","tv","story","order","doesn\u2019t","cut","speaker","protest","peace","mccain","lie","korean","funding","even","discuss","bid","puerto","cuba","british","ready","latest","job","israel","face","trying","terror","keep","dnc","destroys","chair","asked","another","\u201ci","used","terrorist","sean","release","power","flynn","terrorists","office","msnbc","message","jobs","catalan","billion","votes","ruling","pence","muslims","iraqi","hate","week","macron","kills","great","decision","chicago","busted","amid","agency","adviser","seek","refugee","needs","california","blasts","violent","using","statement","rico","protests","congressman","charges","tries","source","senators","radical","kremlin","intelligence","best","ad","special","slams","called","behind","aide","\u201cthe","public","pay","internet","didn\u2019t","bush","bombshell","bad","backs","young","three","parliament","nyc","nfl","march","ambassador","wins","victims","son","sign","shut","rep","orders","france","defends","claim","benghazi","admits","action","near","missile","matter","independence","hollywood","full","cop","believe","away","3","tell","rule","rubio","reveals","rejects","possible","lead","inauguration","dc","chinas","voting","turkish","transgender","shot","set","reports","london","fired","destroy","working","wife","think","refuses","epic","cuts","children","calling","audio","approves","thing","review","real","questions","proves","planned","kids","hurricane","epa","despite","clinton\u2019s","blames","arrest","yet","sex","senior","saying","pentagon","paris","lying","king","ivanka","found","comments","al","whoa","threats","rape","pressure","likely","hold","girl","french","forced","final","female","exposes","dem","days","awesome","act","spokesman","service","sarah","program","murder","much","moore","michelle","making","least","images","hit","envoy","disgusting","cia","assault","announces","10","water","scandal","rips","liberals","let","kelly","facebook","drops","denies","committee","brutal","university","trip","start","sessions","seen","second","return","rant","pope","oil","ny","middle","men","mcconnell","good","gay","gave","fraud","erdogan","break","without","vows","victory","venezuela","released","puts","put","look","leave","jail","in","hard","five","east","corruption","convention","barack","activist","united","stunning","strike","nato","members","jr","joe","history","executive","evidence","every","elections","debt","chris","challenge","xi","workers","turn","street","spain","she\u2019s","sees","running","role","repeal","question","protect","millions","massive","kurdish","it","hilariously","groups","energy","doj","citizens","ben","battle","angry","agree","4","words","talk","suspected","strikes","russias","mike","kellyanne","join","issues","hits","hack","future","friday","biden","activists","2018","thousands","they\u2019re","terrorism","target","star","pelosi","passes","offers","member","loses","killing","issue","hell","getting","fed","defend","church","catalonia","blame","vp","ukraine","thinks","test","summit","step","perfect","part","little","life","car","alien","5","yemen","warning","town","photo","michael","flashback","finally","fear","faces","effort","demands","criminal","country","conway","cabinet","breaks","arabia","aliens","agenda","accuses","1","\u201cwe","yr","warren","uses","since","ria","private","oops","no","megyn","illegals","global","exposed","explains","dangerous","crooked","confirms","close","arms","announcement","abortion","wikileaks","sunday","student","soros","sht","schools","picks","out","nancy","far","cyber","child","ceo","canada","brilliant","anchor","alabama","actually","viral","trial","transition","taking","stage","staff","spicer","speak","name","militants","migrants","michigan","manager","letter","leaves","immigrants","hopes","hannity","friend","four","food","fans","economic","crowd","congressional","conference","company","come","boy","bank","around","15","union","spending","rich","mueller","laws","irma","insane","immigrant","guy","guest","funds","foundation","fighting","fck","cnn\u2019s","civil","christmas","afghan","waters","urge","something","responds","ohio","offer","navy","names","might","mattis","line","labor","find","europe","economy","carson","block","wrong","vietnam","tried","resign","releases","picture","philippines","oregon","mexican","melania","mass","lose","literally","johnson","intel","highlights","hearing","coming","chairman","care","australia","antifa","2","weapons","turkeys","signs","request","politics","palin","o\u2019reilly","night","moscow","illinois","heads","guns","embassy","detroit","destroyed","daughter","council","conspiracy","christie","capital","better","ask","allies","911","tucker","today","system","syrias","stand","sheriff","revealed","remarks","referendum","percent","parenthood","months","mocks","meets","kurds","journalist","james","fund","frances","everyone","deputy","choice","candidates","bathroom","accused","absolutely","well","unhinged","try","thugs","telling","stay","racism","play","pass","owner","news\u2019","nafta","jeanine","israeli","happened","financial","elizabeth","egypt","due","class","bundy","bring","boost","become","bans","bangladesh","asia","allow","\u2018the","west","voted","sue","streets","steve","speaks","red","prison","parties","obamas","lebanon","ireland","iowa","hacking","game","eyes","education","dems","declares","criticism","comment","cities","charged","arrests","answer","actor","videos","victim","update","tough","study","sent","reporters","professor","perfectly","palestinian","pakistan","number","mom","meltdown","love","lawsuit","launches","journalists","human","held","giving","germanys","front","finance","embarrassing","drug","demand","dad","crazy","carlson","britains","boycott","agents","again","zimbabwes","zimbabwe","train","taiwan","steps","six","shuts","sanctuary","roy","residents","reelection","raises","prosecutor","proof","powerful","potential","pastor","moment","irish","industry","hurt","humiliates","girls","furious","eus","confederate","chinese","brutally","already","afghanistan","a","6","20","100","virginia","troops","throws","taxpayer","targets","seth","romney","raise","points","opens","northern","netanyahu","mugabe","movie","maxine","mark","lost","kim","inside","important","hosts","henningsen","hacked","freedom","flint","firm","ends","crime","continue","coal","charge","asking","appeals","america\u2019s","ally","allegations","african","abuse","you\u2019re","worst","who\u2019s","vice","tensions","teen","taxes","super","storm","soon","sends","send","save","rnc","resigns","replace","read","prime","praises","place","outside","moves","month","less","launch","koreas","india","hot","finds","event","documents","disaster","counsel","community","christian","book","base","baltimore","attacked","anthem","ads","2017","13","vs","totally","things","term","strong","spy","sick","sea","russians","returns","results","quit","prince","players","nra","nbc","migrant","looks","leadership","kushner","jeff","iranian","homeland","he\u2019ll","hariri","father","experts","employees","dollars","detained","desperate","dept","data","concerned","changes","central","carolina","borders","board","armed","appeal","aides","agencies","access","8","50","worse","women\u2019s","wearing","veteran","uks","turns","tuesday","trump\u201d","took","supporting","soldiers","sec","screenshots","scott","record","promises","progress","philippine","paid","others","nomination","nations","mother","market","kerry","guilty","guess","ground","gas","firing","eastern","early","dispute","dallas","communist","clintons","cites","chuck","check","blow","billionaire","baby","amnesty","address","\u201cif","wanted","thursday","third","talking","socialist","reutersipsos","rebels","policies","pact","officer","nothing","murdered","missing","many","maher","libyan","jailed","information","hope","harvey","gowdy","focus","efforts","donations","disturbing","crackdown","corporate","companies","comes","clash","center","avoid","asylum","word","whining","watchdog","waiting","unity","trey","threatening","thought","testify","teacher","taxpayers","surprise","supports","spanish","small","shooter","seeking","sales","religious","recount","quits","protester","position","person","parents","not","nation","long","lied","leaked","injured","hysterical","harassment","hammers","fix","facts","expert","expects","european","endorses","drop","draws","dialogue","december","deals","crimes","cover","ca","africas","across","25","winning","went","visa","veterans","up","treasury","total","threatened","suspect","suicide","spying","showing","seven","scam","relations","reach","prove","polls","phone","patrick","orlando","options","militant","marriage","leading","kenya","joins","jimmy","isn\u2019t","humiliated","hotel","hispanic","half","green","golf","gingrich","forward","failed","entire","elected","defending","cooperation","condemns","christians","bus","brazil","blocks","blast","australian","attend","attempt","apart","anyone","alleged","agrees","admit","30","worker","van","unreal","undercover","trumprussia","testimony","swedish","suspends","stupid","strategy","sets","sentence","san","rightwing","ridiculous","remain","regime","qatar","propaganda","popular","phony","past","overhaul","nazi","met","meddling","mays","manafort","low","loss","looms","living","legislation","lady","kkk","jones","irs","insurance","husband","hundreds","gov","gonna","fit","fires","feds","explain","exit","ethics","enough","electoral","done","delivers","continues","completely","charlottesville","campus","brings","body","blacklivesmatter","backing","attempts","agent","advisor","accept","abc","18","12","\u201cyou","\u201cracist\u201d","you\u2019ll","w","treatment","thug","this","station","snl","schumer","robert","riots","reuters","remove","reasons","pushes","problem","presidents","posts","plane","online","nyt","newt","newspaper","monday","ministry","militia","mainstream","longer","libya","lavrov","keeps","judges","joke","international","hypocrisy","hunt","hand","families","false","falls","expose","endorsement","drive","dossier","dies","delay","deep","decide","complete","committed","commerce","chelsea","charity","cannot","burn","brother","bizarre","bannon","attacking","assad","arizona","amazing","alert","airport","actions","9","7","17","\u201cthis","\u2018fake","weighs","warned","vegas","usa","tim","threaten","thanks","sweden","stephen","starts","stance","speaking","serious","rush","risk","relationship","reid","region","records","reaction","raqqa","pulls","proposes","probes","path","on","nuts","nightmare","nails","mosque","mi","lynch","leaks","leads","info","helping","happen","gorsuch","files","fail","expected","eric","diplomats","diplomatic","die","defeat","conflict","confident","chaos","broke","blacks","biggest","aren\u2019t","allowed","actress","abe","\u201ci\u2019m","\u201cit\u2019s","wisconsin","we\u2019re","visits","view","va","trumpcare","trash","torture","suggests","straight","southern","situation","sell","road","rice","reportedly","removed","relief","reforms","reality","ratings","process","priceless","point","plot","pledge","planning","outrageous","outrage","numbers","nearly","marco","links","killer","kasich","kansas","joint","instead","independent","idea","hospital","hear","hateful","harry","hands","halt","google","farright","fan","door","dirty","critics","criticizes","courts","controversial","concerns","commander","cold","chance","caucus","cash","cancels","building","breitbart","brazils","blocked","bills","becomes","banks","baghdad","audience","account","\u201che","whines","what","wage","todd","tests","tech","tears","teachers","taken","susan","store","somali","short","shoot","sharpton","sexist","rock","responsible","promote","primary","price","pathetic","oklahoma","movement","massacre","local","listen","joy","jewish","jersey","japans","investment","hopeful","holds","historic","happy","graft","georgia","fellow","feel","fears","famous","exactly","emergency","drone","disabled","critical","coup","corrupt","convicted","consider","compares","commission","camp","brussels","brags","berkeley","aim","africa","zone","zika","worked","welcomes","wednesday","viewers","unlikely","trudeau","there\u2019s","shreds","shoots","search","scotus","rise","rhetoric","respond","resolve","reject","rate","protrump","prosecutors","proposed","priebus","photos","officers","offered","nigeria","myanmars","mock","missiles","mic","memorial","lets","land","jeb","japanese","i\u2019m","islam","irans","holding","hiding","harvard","graham","giuliani","gains","form","flags","failure","everything","duterte","domestic","dhs","deadly","cuban","credit","countries","collusion","colbert","closed","classified","challenges","cause","brilliantly","briefing","appears","amendment","accidentally","24","14","11","\u201cnot","\u2018white","\u2014","within","wire","warrant","vatican","tx","tpp","tower","that\u2019s","surveillance","sudan","stealing","stands","someone","silent","silence","sen","ross","restaurant","regional","radio","putting","propose","problems","poor","politico","poland","park","owned","ousted","ordered","neil","needed","nc","nasty","morning","mn","miss","memo","martin","majority","limits","leftists","leaving","las","lahren","la","kurdistan","kimmel","it\u201d","islamist","influence","increase","impact","illegally","hypocrite","him","hezbollah","helped","haley","gold","god","gang","floor","flee","environmental","endorse","egypts","discussed","dinner","detains","david","coverage","course","cost","collapse","closer","claiming","citizen","ceasefire","canadian","blood","beijing","behavior","begins","beaten","based","banned","ballot","april","allowing","agreement","ago","\u2018i","you","witch","winner","whether","weeks","wealthy","wasn\u2019t","tomi","toll","steal","split","spent","slammed","sharia","shares","seat","scheme","scalia","safe","runs","risks","restrictions","rescue","regulations","refuse","president\u201d","playing","pennsylvania","peninsula","paying","opening","nominate","network","multiple","mnuchin","mitch","mind","losing","linked","light","lee","lebanons","lawyers","latino","late","kidding","kid","keith","june","israels","island","iraqs","interior","infrastructure","indonesian","indian","idlib","huckabee","happens","hacks","guard","gone","gender","funded","friends","flight","firms","explosion","explodes","explode","enforcement","dr","display","detention","deir","declaration","de","crash","coulter","considering","congresswoman","confronts","cheer","cases","career","capitol","buy","burns","build","blaming","becoming","beat","bar","babies","asian","alzor","21","\u201ca","\u2018trump","zuma","zealand","wounded","western","warming","vow","venezuelas","usbacked","unveils","understand","tusk","trust","true","truck","trolls","theory","thanksgiving","targeted","taps","tantrum","suspended","sues","sued","stunned","standing","soldier","socialism","side","sickening","seriously","selling","safety","rick","resolution","resignation","remember","regulation","raped","rand","proposal","project","prisoner","pledges","plays","pipeline","peaceful","pakistani","page","package","operation","openly","nsa","nominees","news\u201d","mitt","minutes","medical","maine","lower","lot","liar","laughing","knows","kicked","kenyan","keeping","july","jim","jay","jake","italian","invites","invited","interest","handed","gulf","garland","games","fuel","feud","fails","fact"],"legendgroup":"","marker":{"color":"#636efa","size":[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0],"sizemode":"area","sizeref":0.04,"symbol":"circle"},"mode":"markers","name":"","showlegend":false,"x":[-0.4691135883331299,0.5847158432006836,-0.020941009745001793,8.026899337768555,-1.7754594087600708,-4.764277935028076,1.5446876287460327,4.987839698791504,-2.081042766571045,5.360555648803711,-0.08251551538705826,1.5454167127609253,6.706007480621338,-0.8447036743164062,-0.8269278407096863,-0.42754092812538147,-1.1904819011688232,0.12696784734725952,-1.0668367147445679,-1.6624386310577393,-1.113332986831665,-2.108311176300049,-1.0229393243789673,3.7115097045898438,1.1059744358062744,3.2279887199401855,6.838225364685059,0.22877579927444458,-1.7267765998840332,-1.5193372964859009,-0.9864121675491333,0.9279096126556396,-1.757352590560913,5.79509162902832,-1.3858369588851929,5.9899444580078125,-0.21802698075771332,3.9269301891326904,-5.262462615966797,1.7538210153579712,-0.9707668423652649,-1.4881670475006104,2.2503912448883057,0.2806035876274109,-3.2287070751190186,-0.2531944215297699,6.170248985290527,1.9369354248046875,-0.3891203999519348,-2.472963333129883,-0.2693992257118225,0.8858639597892761,-0.7486787438392639,-0.7870708703994751,-1.1502094268798828,-4.717738151550293,1.6201729774475098,1.239298701286316,-0.7104752063751221,-0.8505614399909973,-0.18036584556102753,-4.585810661315918,-2.4321742057800293,-0.7325449585914612,-2.1512317657470703,-0.6820656061172485,0.21870186924934387,3.294032335281372,1.1068698167800903,-0.48947182297706604,0.03311007469892502,-0.2995074689388275,-0.6046457886695862,-0.9623205661773682,0.7370226979255676,-0.790068507194519,0.14868268370628357,-0.20846059918403625,-0.6103066205978394,-0.8619563579559326,0.5517481565475464,0.9976133704185486,-1.411074161529541,-2.4181766510009766,-1.9039818048477173,-0.033332303166389465,-4.2214531898498535,3.952726364135742,-0.28151580691337585,1.1687146425247192,-0.05672324076294899,-2.054715156555176,-2.301239490509033,-3.6600778102874756,-0.04276449233293533,1.14946711063385,3.373534917831421,-6.80802583694458,1.1900783777236938,2.4672858715057373,1.5661282539367676,1.7490589618682861,0.6899795532226562,-0.5452825427055359,3.1627004146575928,4.720695495605469,-3.09199857711792,1.0441352128982544,3.054651975631714,0.7856925129890442,-1.0406110286712646,-0.5496785640716553,-2.114392042160034,-0.2877035140991211,-0.1321822851896286,2.279679536819458,-0.1561591774225235,-0.564300537109375,-2.3262887001037598,0.2558537423610687,0.9721447825431824,2.1725494861602783,-1.2916290760040283,-0.39061278104782104,0.358924925327301,4.3903632164001465,-0.7203390002250671,-0.16235072910785675,-1.2114193439483643,2.5192995071411133,3.1074395179748535,-2.560053825378418,0.08722295612096786,1.6587239503860474,0.5228661298751831,-1.4799177646636963,-0.2541217505931854,-0.17606210708618164,-0.5404825806617737,-0.7192979454994202,-2.144554376602173,-4.745693206787109,-1.529349684715271,-2.125458002090454,2.0647871494293213,1.65910804271698,0.8973714709281921,-0.17391906678676605,-1.4386224746704102,0.8918949365615845,1.7411292791366577,4.086873531341553,-0.9138907194137573,-2.6409225463867188,0.6014586091041565,1.0029451847076416,-2.690380334854126,-0.7432724833488464,-0.2639424204826355,-0.2881351113319397,-0.6094056367874146,1.353559136390686,0.4189782738685608,-4.301692008972168,-0.0932617262005806,-0.4962909519672394,1.9069041013717651,-1.630438208580017,0.8091776967048645,-1.8224436044692993,-0.19249752163887024,0.5750226378440857,-1.8728677034378052,-0.6047037243843079,3.973926067352295,-1.2126975059509277,3.9652841091156006,-3.848367214202881,-0.607239842414856,0.0024761080276221037,1.9849886894226074,0.7831164598464966,-2.109008312225342,-1.9880903959274292,1.0619250535964966,0.9347904920578003,1.3957487344741821,2.405519485473633,-1.169811487197876,2.7771952152252197,-0.26653212308883667,2.4353108406066895,-0.20140832662582397,1.1592037677764893,1.5652896165847778,-2.004952907562256,-2.0593230724334717,2.4556663036346436,0.8591964840888977,0.9869518280029297,4.299213886260986,0.6451877951622009,-2.9322872161865234,-0.7565209865570068,-0.7514600157737732,-0.7212756872177124,2.2094948291778564,-0.9752489924430847,-0.8396998643875122,3.6476480960845947,0.7685773968696594,1.6555980443954468,-0.09016473591327667,-1.675855040550232,-0.39631789922714233,-1.9569346904754639,3.145148515701294,0.8667182326316833,-1.056756615638733,-1.5648432970046997,-0.2846677005290985,3.294581890106201,4.131256580352783,-0.6817464828491211,0.7114226818084717,0.2699694037437439,-0.5166541337966919,1.0789732933044434,0.028759479522705078,-3.5192110538482666,3.9646413326263428,-0.8162915706634521,-1.6413989067077637,0.6845084428787231,-1.2437840700149536,2.636428117752075,-0.08565042167901993,2.043884754180908,0.8146785497665405,3.530613660812378,-3.2459561824798584,0.8948906660079956,0.41207802295684814,-0.6311085820198059,1.1357380151748657,-1.6990057229995728,0.19950762391090393,0.5400466322898865,-0.9486523270606995,2.5242764949798584,-1.8331797122955322,-0.23461875319480896,-4.2797064781188965,0.7343474626541138,-1.273887276649475,0.6729852557182312,-1.4283771514892578,1.1100505590438843,-0.5769848823547363,-2.6102285385131836,-0.22066310048103333,3.360689878463745,0.6277694702148438,2.459697961807251,-0.0654338002204895,-1.3227880001068115,0.10882225632667542,1.5273849964141846,3.3212435245513916,-0.09619419276714325,2.6829047203063965,-1.3463116884231567,-0.8824695348739624,1.6400599479675293,0.4935096800327301,-1.0274051427841187,0.41787391901016235,-1.253616452217102,0.12085369974374771,-1.8105952739715576,-1.892623782157898,-0.4683646559715271,0.27040618658065796,3.742919683456421,1.6949903964996338,2.942578077316284,-0.5576404929161072,0.5981265902519226,2.3150644302368164,1.0467073917388916,-0.2418588399887085,-1.2440224885940552,-0.29211053252220154,-0.20828877389431,-0.2097863405942917,1.7860383987426758,1.3879033327102661,2.6334452629089355,2.905214309692383,-3.136233329772949,-1.8760865926742554,1.3728135824203491,-1.163605809211731,1.510273814201355,2.599621295928955,-0.7369925379753113,-0.5990557670593262,2.700531244277954,0.2790755033493042,-1.1116559505462646,3.2921230792999268,2.674013614654541,3.1212079524993896,-2.0747547149658203,-0.5532546043395996,-0.840513288974762,1.9299376010894775,0.8242504000663757,3.3829345703125,-1.2993555068969727,-1.1210553646087646,-1.4839885234832764,-2.0737462043762207,-0.1545993834733963,-2.3752777576446533,-1.061217188835144,-1.5831669569015503,3.8891217708587646,1.7178153991699219,0.6331208944320679,2.6684324741363525,-1.4846537113189697,0.9002934098243713,-4.554057598114014,-2.0657856464385986,-3.504951000213623,-1.0440716743469238,1.5905016660690308,-2.1530921459198,0.7373750805854797,-1.1054821014404297,3.660170078277588,-1.419804573059082,-1.5355643033981323,-1.6778918504714966,-1.4429808855056763,0.20034466683864594,2.81449818611145,-1.7819732427597046,-1.2037266492843628,1.5080257654190063,-0.8974232077598572,-3.0017240047454834,-1.4073460102081299,-1.816606044769287,-1.6952719688415527,0.2104048728942871,1.3953429460525513,0.09490348398685455,-0.1441185623407364,0.5433801412582397,0.31995514035224915,2.0243804454803467,-0.6516264081001282,2.2692103385925293,2.2209081649780273,-0.6674489378929138,1.679357886314392,2.0573720932006836,2.7485287189483643,1.222568392753601,1.3496524095535278,3.1219124794006348,-0.9877239465713501,-1.677866816520691,-1.490427017211914,3.5202903747558594,-0.5960882306098938,-0.4210500121116638,1.2735952138900757,0.604922354221344,-4.014449119567871,0.921754777431488,-0.5096150040626526,-2.7390859127044678,-0.5385192632675171,0.24078623950481415,-2.66743540763855,2.021901845932007,-0.49850621819496155,-4.197057723999023,-2.1965153217315674,0.27309319376945496,-1.8742870092391968,-0.009132087230682373,3.0008926391601562,-3.145246744155884,-2.452186107635498,-2.4187967777252197,-3.7860493659973145,0.45855459570884705,0.22468224167823792,-1.1299209594726562,0.6033181548118591,1.6315542459487915,1.9999839067459106,-1.1204310655593872,-1.7729473114013672,-0.9811452031135559,1.1532883644104004,-1.2045631408691406,1.5558923482894897,-4.39275598526001,-1.9789562225341797,2.8563613891601562,-3.651970148086548,0.6803877949714661,2.42175555229187,0.39008039236068726,-1.1631747484207153,0.8087846040725708,2.5780768394470215,0.9672000408172607,-3.158590793609619,2.8050217628479004,0.24441008269786835,0.8312793374061584,1.2004793882369995,3.147455930709839,0.9155469536781311,3.5171945095062256,1.6924550533294678,-3.525679111480713,-0.03491348400712013,-0.963532030582428,-3.596327066421509,2.401442766189575,0.7148069739341736,-0.6403368711471558,-0.009908103384077549,-0.9471800923347473,1.3872660398483276,-0.5443850159645081,-1.409570336341858,0.9607363343238831,1.7252542972564697,-2.0555379390716553,-1.9212669134140015,-1.2702616453170776,-0.28331589698791504,1.2027513980865479,2.376701593399048,-0.680377721786499,-0.782008945941925,-1.7229795455932617,1.6670987606048584,-1.9368762969970703,3.2921371459960938,1.2763780355453491,2.0649921894073486,1.6053917407989502,0.5105273723602295,1.498390793800354,1.3739559650421143,-1.745077133178711,-0.6519291996955872,2.1202847957611084,-1.1042461395263672,-0.23433071374893188,-2.008906364440918,0.09445785731077194,2.0945193767547607,-4.189716815948486,0.03584052994847298,-3.569533586502075,-0.1783176064491272,-0.49713751673698425,-0.7950845956802368,-1.280321717262268,-2.206249475479126,0.3625335097312927,2.0563600063323975,0.1299508959054947,-0.4292529821395874,2.889702558517456,1.5688631534576416,3.061324119567871,-0.9100607633590698,0.15249469876289368,2.5360987186431885,3.382983922958374,-3.0498335361480713,2.5398685932159424,-2.3423569202423096,2.790928363800049,0.1507585048675537,2.741161823272705,0.35733410716056824,2.4185409545898438,-1.49637770652771,-0.35414862632751465,-1.5880231857299805,2.9716005325317383,1.3452022075653076,-0.6659769415855408,0.1305716633796692,0.8925700783729553,-0.9351545572280884,2.6061768531799316,-1.8276259899139404,-1.192007303237915,2.428709030151367,-0.5766246318817139,1.1604772806167603,-0.505247950553894,-1.2782883644104004,0.737306535243988,3.5434634685516357,1.132545828819275,1.465679407119751,-2.248748302459717,-1.2474125623703003,-1.7399232387542725,0.9785021543502808,-1.5731945037841797,1.3515512943267822,-0.6823289394378662,1.5369898080825806,2.4955127239227295,3.122628927230835,0.8829468488693237,3.3947830200195312,0.5175493359565735,-3.785550832748413,-1.2051243782043457,2.207740545272827,-0.21278469264507294,-0.13212819397449493,1.682983160018921,0.3228314220905304,0.9791322946548462,1.2230119705200195,-2.387258529663086,3.1185812950134277,-0.3675675094127655,-3.348052740097046,2.6916022300720215,1.7244945764541626,1.7335971593856812,2.5412919521331787,0.7706657648086548,-0.11268903315067291,0.10094881802797318,2.8823554515838623,1.357511281967163,0.9239895343780518,1.2417960166931152,0.4399411380290985,-0.336119145154953,-2.2499663829803467,0.18656325340270996,2.6844770908355713,-0.3806159496307373,-0.5968202948570251,-0.38068774342536926,-1.2137750387191772,-1.7009698152542114,-0.36830925941467285,-1.8301615715026855,1.7705886363983154,-1.057598352432251,-2.8689043521881104,-0.31982818245887756,1.3392651081085205,1.118958830833435,-0.33646318316459656,0.5721537470817566,0.6202840805053711,2.5790321826934814,0.762078583240509,-1.020990252494812,-0.13587942719459534,-0.1481412798166275,-1.6158862113952637,-1.8785289525985718,-1.5088820457458496,0.8908095955848694,-0.9440094828605652,1.0864464044570923,0.9529932737350464,-0.7834638357162476,0.04273122176527977,2.2410361766815186,0.3472777307033539,-0.03528725728392601,-1.473315954208374,-0.7113890647888184,-0.31707242131233215,2.6318955421447754,-0.4735511541366577,1.035206913948059,2.806556463241577,-2.130157947540283,-0.9880368113517761,0.3871464431285858,-1.1331454515457153,1.8841657638549805,-0.1814095824956894,0.3981646001338959,1.4116365909576416,2.114793062210083,-0.07820157706737518,-0.7322837710380554,2.6766932010650635,-1.3509832620620728,-3.617926836013794,1.2468440532684326,1.592549443244934,-0.33635053038597107,-3.0785305500030518,2.580890655517578,-3.6493117809295654,1.8085979223251343,-2.0331547260284424,-2.2533998489379883,1.3494435548782349,0.19514672458171844,1.8689496517181396,1.6161257028579712,-3.1105284690856934,1.9160103797912598,2.5113770961761475,-1.8793585300445557,-1.9011361598968506,3.1278061866760254,1.7231979370117188,2.0842602252960205,-1.6771003007888794,2.5634515285491943,-2.010004997253418,1.3435484170913696,1.6685210466384888,-0.7112398743629456,-2.4185824394226074,-1.5615019798278809,-3.9067435264587402,1.6409862041473389,1.9151191711425781,-0.2664904296398163,-1.1120524406433105,-0.1310044676065445,1.1772396564483643,-0.21725206077098846,-1.2770795822143555,-1.4810980558395386,-1.0871931314468384,-2.6167941093444824,-0.572540283203125,2.831001043319702,-0.18046824634075165,-1.4641411304473877,1.4405725002288818,1.1074795722961426,-0.12314074486494064,-1.7869664430618286,0.7308820486068726,1.2546569108963013,-0.6752250790596008,-0.7562411427497864,2.273632049560547,1.5774362087249756,0.8294625878334045,-0.33187997341156006,0.8542193174362183,-3.7130794525146484,1.3561445474624634,0.17733262479305267,-3.8264105319976807,1.1044098138809204,-1.9218342304229736,-2.8730549812316895,-0.5803250074386597,2.4449477195739746,-0.10555663704872131,0.2725762128829956,-0.08213885128498077,-0.7809884548187256,2.586134672164917,1.4793286323547363,-2.018312454223633,1.4394819736480713,-0.5675216913223267,0.7447526454925537,0.5997393727302551,2.5301504135131836,3.392564535140991,-0.13463753461837769,-0.6753708124160767,-1.1041617393493652,1.2501403093338013,1.0667530298233032,0.2995063066482544,2.3174808025360107,-0.7352003455162048,2.2424516677856445,-1.4629848003387451,2.4918394088745117,-0.7399417757987976,-0.6546962857246399,-0.05430437624454498,2.65531063079834,2.0350539684295654,0.48271751403808594,2.8319902420043945,0.03440758213400841,-3.4206278324127197,1.0528990030288696,2.6009128093719482,-1.0653351545333862,2.3941009044647217,3.5451345443725586,0.4776809513568878,2.1510026454925537,2.9544990062713623,-0.18793083727359772,2.3507778644561768,-0.8030738234519958,-1.9962811470031738,-0.41652870178222656,0.6357576847076416,0.9495667815208435,1.8852534294128418,1.761083960533142,1.3336350917816162,1.471287488937378,2.7168281078338623,-0.22767643630504608,-1.8360626697540283,-0.2189924120903015,1.584306001663208,-1.0071494579315186,-2.1095707416534424,0.3664728105068207,0.3774181604385376,-2.1545355319976807,2.753204107284546,2.3477680683135986,-0.26550549268722534,3.0695652961730957,1.1063284873962402,-2.9647576808929443,-0.3841564953327179,1.9571073055267334,-0.053042806684970856,-0.5717299580574036,1.9987342357635498,1.0440962314605713,0.8898147344589233,-2.5628225803375244,-1.3373528718948364,0.7325129508972168,0.5443178415298462,0.5300877690315247,-1.1580718755722046,0.3429255187511444,-2.475670099258423,2.2725675106048584,0.7797618508338928,-1.5679303407669067,0.8309282064437866,1.0484743118286133,-0.7220790982246399,2.3908960819244385,-1.7441787719726562,-0.9892425537109375,1.753811240196228,1.306479811668396,0.7194107174873352,-1.8171337842941284,0.9848475456237793,-0.2125050276517868,-0.7868230938911438,-1.4445735216140747,1.2279702425003052,0.8289937376976013,-0.1947779506444931,-3.2865874767303467,2.1753599643707275,0.5115805268287659,1.5120998620986938,2.1506662368774414,-0.4634651243686676,-0.09248419851064682,0.019129738211631775,2.786794662475586,3.3036773204803467,0.250912606716156,0.1800837516784668,-2.0740973949432373,0.017903055995702744,-3.703942060470581,2.9769253730773926,2.254256010055542,0.01067332923412323,-1.6060162782669067,0.26772743463516235,-2.303853750228882,2.2688450813293457,-2.7484045028686523,0.2549618184566498,-0.6963419318199158,-0.898670494556427,-1.851844310760498,-0.5272735953330994,0.6149822473526001,-1.0157095193862915,0.9419995546340942,-2.5598912239074707,2.3218631744384766,-0.771600604057312,-0.13181376457214355,0.9681625962257385,-3.3156769275665283,1.8216688632965088,-1.423988938331604,1.3604096174240112,-0.15452341735363007,-0.020169902592897415,2.662388801574707,-1.2915714979171753,0.8827068209648132,-3.0658838748931885,-0.15969690680503845,1.2125219106674194,-1.2562892436981201,1.8209582567214966,-3.394695520401001,1.7425451278686523,0.3155147135257721,0.47005584836006165,-2.870225429534912,-0.751905620098114,-1.4719358682632446,-1.3934909105300903,1.2998195886611938,1.9833391904830933,0.4943610429763794,-1.7882134914398193,-2.428248643875122,-0.5167698860168457,0.673842191696167,0.09365391731262207,1.3644094467163086,1.6285135746002197,0.1355402022600174,-0.15809929370880127,2.303596258163452,-1.1946855783462524,-3.1184535026550293,1.7156697511672974,-1.634070873260498,-1.9804937839508057,1.2819161415100098,1.956230878829956,1.1876391172409058,-0.2607160210609436,-3.134387254714966,1.1169639825820923,1.8821561336517334,1.3019126653671265,-1.325788140296936,-3.192859411239624,-1.2163655757904053,0.9120975732803345,-0.6563420295715332,0.7759885787963867,-0.6399819850921631,-3.15212345123291,-0.9721903204917908,2.564324140548706,-1.0272899866104126,-2.836441993713379,2.3862240314483643,-1.0875684022903442,0.571000874042511,-0.2734960615634918,0.0037403141614049673,-0.7922734022140503,1.6131930351257324,-0.6000636219978333,1.8185982704162598,-0.20426924526691437,1.1230037212371826,1.4691804647445679,-0.07064124196767807,1.8427766561508179,0.600450873374939,-0.31026673316955566,0.4832603335380554,2.4848062992095947,-3.0818653106689453,2.592390537261963,-0.1590501368045807,2.1877498626708984,0.20644600689411163,0.7337900996208191,-3.410642623901367,-1.9325199127197266,0.39291054010391235,2.901071071624756,0.5289570689201355,-3.4998998641967773,2.0152556896209717,1.646404504776001,-2.941911458969116,-2.2690024375915527,0.2749005854129791,2.998962640762329,-1.4783588647842407,1.8051466941833496,-0.3401000201702118,-0.37663406133651733,0.10155957192182541,0.9895402789115906,0.2432076632976532,-0.35359832644462585,-2.3191850185394287,-0.8597133755683899,-2.1881000995635986,-3.025939464569092,-0.6457914710044861,0.11178497970104218,1.8417272567749023,-2.72509765625,0.4800626039505005,2.5533268451690674,1.1267553567886353,-1.1172664165496826,-0.05660920962691307,0.4502834975719452,-1.4034744501113892,-1.0668519735336304,1.6235264539718628,2.264404535293579,1.0762765407562256,2.0465786457061768,2.0644516944885254,-0.5770219564437866,-0.15002091228961945,0.23691542446613312,0.7617157697677612,2.055567741394043,2.040621757507324,-2.135554075241089,-2.222898244857788,0.8520523905754089,0.865132212638855,2.425337553024292,0.6845743656158447,-0.6105363965034485,-1.2795785665512085,0.21300362050533295,-0.6211288571357727,-0.7942588925361633,2.0285277366638184,-3.5462241172790527,0.25158944725990295,-3.2037177085876465,0.9636353850364685,-1.2117804288864136,0.0028259940445423126,0.9749710559844971,0.808950662612915,2.015866279602051,-3.325192451477051,1.2346622943878174,0.6776774525642395,1.3483970165252686,-3.4794604778289795,-3.8321192264556885,-0.3878471851348877,-1.2217559814453125,-0.9973582029342651,-1.62648606300354,1.5766754150390625,0.33827608823776245,0.3521287739276886,0.8134124279022217,-0.407582551240921,-1.7367968559265137,-1.691481113433838,1.9464545249938965,0.5387993454933167,-0.6871386766433716,1.7126010656356812,1.7436081171035767,-2.2277276515960693,-1.0018420219421387,-1.2073752880096436,2.2837066650390625,0.22787271440029144,1.448405385017395,-3.161709785461426,2.7664966583251953,-2.1521506309509277,2.101325750350952,1.0406800508499146,-1.4157251119613647,0.306670218706131,0.5774793028831482,0.5563384294509888,-0.3200712203979492,-0.6409603953361511,-1.469222068786621,0.8658795356750488,1.9849497079849243,-2.3907277584075928,1.7664520740509033,-0.06003482639789581,-0.7821134924888611,-0.5473549365997314,-1.5126243829727173,-1.0747653245925903,-1.583263635635376,-3.456873655319214,0.7928363680839539,1.8724236488342285,0.4224068820476532,0.9489332437515259,-0.7829077839851379,-0.9302012324333191,0.694342315196991,1.4184176921844482,2.6111724376678467,-0.025467796251177788,-0.03706194460391998,0.19784614443778992,-0.6280217170715332,-0.7165746092796326,-0.43236878514289856,-0.7481853365898132,-1.2078462839126587,-0.8811495304107666,1.694329857826233,-1.341519832611084,2.8355586528778076,-1.5900003910064697,-2.0309348106384277,-1.0774075984954834,-0.11584731936454773,2.200772285461426,0.8016698360443115,2.7799832820892334,-1.8094580173492432,-2.896005392074585,1.2638351917266846,-0.07080195844173431,0.2151658982038498,-1.122782826423645,-0.7703558802604675,0.16172540187835693,0.027181528508663177,0.4477669298648834,2.0780656337738037,-1.8358452320098877,-0.20226575434207916,2.49635648727417,0.11048422753810883,-0.6167132258415222,0.9132718443870544,-1.2276716232299805,-0.42014995217323303,0.5427612066268921,0.5818313956260681,-1.0268305540084839,-2.967892646789551,-3.1335184574127197,1.259940266609192,-0.5233659148216248,0.6714972257614136,0.5987609028816223,1.1350722312927246,-0.9246036410331726,1.9328551292419434,2.2745072841644287,-0.20096439123153687,0.34857484698295593,2.919677734375,1.7746431827545166,-0.12718093395233154,-0.575420618057251,0.13760922849178314,-0.7536014914512634,1.3656771183013916,1.0607935190200806,0.5722208619117737,-0.8559868931770325,-2.6463027000427246,-0.23100008070468903,2.2344963550567627,-2.395247220993042,-0.8122166991233826,-0.6996468901634216,-0.4711139500141144,-1.0168495178222656,-1.5363271236419678,0.63567715883255,0.9356048703193665,-0.8580793142318726,-1.660119891166687,-0.6075873374938965,-0.2486458271741867,-0.8665539622306824,1.5826071500778198,-0.40156713128089905,-1.2841838598251343,2.060335397720337,-2.9099955558776855,0.9219754338264465,-1.2846676111221313,0.66904217004776,1.5897483825683594,-1.9739264249801636,1.706731915473938,0.5248072147369385,-0.9624849557876587,-1.5530471801757812,-1.8059070110321045,-1.461153268814087,-0.18364067375659943,1.7934778928756714,-1.2938388586044312,-0.06596464663743973,-2.1685662269592285,-1.988201379776001,-2.200796365737915,-1.212232232093811,0.9213951826095581,0.04067784920334816,0.8223374485969543,2.7466464042663574,2.162977933883667,2.104015588760376,-3.565844774246216,-0.4112246334552765,-1.6830545663833618,2.116947889328003,0.5508521795272827,1.0832234621047974,-1.7454807758331299,-1.8536508083343506,2.191358804702759,1.6530239582061768,-0.718376636505127,0.13451777398586273,-2.523932695388794,-2.2559988498687744,0.8236302733421326,-1.4119242429733276,-1.7176008224487305,0.8671731948852539,0.3196762502193451,-1.5683717727661133,-1.3376332521438599,-1.4353249073028564,2.5937986373901367,0.35421550273895264,-1.6750587224960327,-1.5250558853149414,-2.0137267112731934,-1.4917683601379395,-3.0271522998809814,0.5908008813858032,1.6300349235534668,-2.1719279289245605,-2.2757933139801025,1.5956621170043945,0.13596591353416443,0.7471308708190918,-0.4274720847606659,2.167915105819702,0.4619843363761902,-1.2142727375030518,1.8502140045166016,1.0227420330047607,-3.2970354557037354,1.2099591493606567,1.8867918252944946,2.1619679927825928,-3.66041898727417,-1.5870076417922974,-2.528184413909912,-0.8566299080848694,1.1611744165420532,0.9382847547531128,1.1810370683670044,-1.3554599285125732,-1.097760558128357,1.653241753578186,-1.6387118101119995,-2.4386918544769287,-1.8434456586837769,-1.7312681674957275,-1.0195708274841309,2.024513006210327,-1.3287831544876099,-1.514237642288208,1.4377449750900269,2.1642725467681885,-3.050328016281128,-0.5429458022117615,0.4539778530597687,2.0186214447021484,-2.3786158561706543,-1.1525884866714478,0.1381586492061615,-1.0329447984695435,1.9959113597869873,2.0728049278259277,-1.608673095703125,1.588523030281067,-1.3445243835449219,2.007455348968506,1.2093467712402344,1.2638862133026123,-1.9543648958206177,1.542189121246338,1.6138397455215454,1.9562820196151733,-0.6844785213470459,-1.9935228824615479,-0.6397461295127869,0.9913520216941833,-0.9350655674934387,-0.06790802627801895,-0.8880918622016907,0.9308242797851562,-2.9211068153381348,1.5354959964752197,-0.5543023943901062,0.25507768988609314,0.6556329727172852,-1.4607092142105103,1.9560856819152832,-0.6593601107597351,2.9695873260498047,1.943355917930603,-1.7635315656661987,2.107788562774658,-0.5139192938804626,2.2108967304229736,-2.134213924407959,-0.03514796495437622,0.9710013270378113,-2.8093063831329346,-0.27217650413513184,-1.5933589935302734,-0.3733360171318054,-2.1283435821533203,-1.179597020149231,-3.1137633323669434,-0.5602788925170898,-3.1241564750671387,1.2510703802108765,2.068025827407837,-2.5996932983398438,0.3388230502605438,0.044406816363334656,1.2580474615097046,2.0124289989471436,-1.5012420415878296,0.4228786826133728,1.2809098958969116,-1.7237318754196167,0.8679957389831543,1.3714741468429565,-1.9082032442092896,-1.4946510791778564,-0.1889025717973709,1.7223832607269287,-3.060800313949585,1.408923864364624,-1.2102185487747192,-2.628770351409912,1.3845868110656738,0.10470877587795258,-0.3091387152671814,1.349611520767212,0.3864862322807312,-3.2658722400665283,-1.8484110832214355,-0.36399248242378235,-0.30772340297698975,-2.6782426834106445,-0.4358544647693634,1.4390662908554077,1.8598171472549438,2.223564386367798,-0.5426368713378906,0.5846884846687317,0.24337729811668396,-0.5931992530822754,0.10345436632633209,1.577647089958191,0.5132519602775574,0.6114527583122253,1.5367358922958374,1.0386326313018799,1.7716137170791626,-1.6334961652755737,-1.4780850410461426,1.504241704940796,0.08139027655124664,-2.800764799118042,-2.0311391353607178,-2.2543833255767822,-2.523169994354248,-1.1320643424987793,0.7558014988899231,1.3161941766738892,2.727524995803833,-0.46234777569770813,-0.6004546880722046,1.9834498167037964,0.3817373812198639,-1.2626432180404663,0.6089635491371155,2.36592698097229,1.8544169664382935,0.7969698309898376,-0.004603976849466562,-0.8046677708625793,-1.2352070808410645,1.9840905666351318,-1.5948572158813477,-1.2517396211624146,-1.2484904527664185,-0.9397464394569397,0.9545084834098816,2.026179075241089,-2.647209644317627,1.539271593093872,-2.398136615753174,0.8383315801620483,0.41271862387657166,1.7841883897781372,-0.7057740688323975,-1.9265347719192505,1.399632215499878,-0.1000102236866951,-1.3349806070327759,-2.565361976623535,-2.8328022956848145,0.554308295249939,0.38082021474838257,-2.86615252494812,-0.18008120357990265,-2.2909739017486572,1.538979411125183,2.155165910720825,-0.6850972175598145,0.909871518611908,-1.4436352252960205,-0.900764524936676,1.1335432529449463,2.553556442260742,1.669646143913269,1.4380452632904053,0.7563936710357666,1.7443467378616333,0.837049126625061,-1.3545632362365723,-0.6438177227973938,0.7772983908653259,0.030400576069951057,1.5197362899780273,0.6696580052375793,0.44455385208129883,1.9505921602249146,0.8729050159454346,1.8406046628952026,0.6099681854248047,-2.058682918548584,1.5904285907745361,-1.9097251892089844,1.8878132104873657,0.38714390993118286,1.7255957126617432,-0.5941662788391113,-1.0461487770080566,-0.5434054732322693,-1.1684167385101318,1.6099215745925903,1.8266830444335938,2.1517393589019775,0.4493069052696228,-0.6915308833122253,1.6398530006408691,0.6644647717475891,-0.5285324454307556,1.5952167510986328,-1.2950254678726196,1.1302717924118042,0.4361075460910797,-2.5781056880950928,0.9360635876655579,1.4650570154190063,-0.6138439774513245,0.6532789468765259,-1.397086262702942,0.9251040816307068,0.13788779079914093,-1.9289631843566895,-1.547783374786377,1.9623781442642212,-1.3828264474868774,-0.7247925996780396,-2.0058224201202393,0.650456428527832,1.460927128791809,0.1726798117160797,-1.9880276918411255,-1.0810972452163696,-0.5279291272163391,-1.636385202407837,0.8774183392524719,-0.10000904649496078,1.9156684875488281,0.645609974861145,0.5243322253227234,-1.370427131652832,0.9675706028938293,-1.5612298250198364,1.0037835836410522,-0.30159470438957214,-0.8110479712486267,0.5979997515678406,-0.5898971557617188,-2.5480902194976807,1.6633363962173462,0.23171651363372803,-0.9469319581985474,-0.7132382988929749,-0.8250020146369934,0.1996045559644699,0.7899229526519775,-0.15565304458141327,-0.4206556975841522,1.2813782691955566,0.9346883893013,-0.19545283913612366,0.649339497089386,-0.5137578845024109,-0.7415170669555664,1.891158103942871,1.7154077291488647,-1.7672529220581055,-1.4680548906326294,-0.04572862386703491,-0.2941438555717468,-0.9947359561920166,1.6708276271820068,2.088578939437866,-0.6655685901641846,1.1346523761749268,1.2566512823104858,1.2698453664779663,1.483048677444458,0.8421698808670044,1.4179824590682983,-0.43246397376060486,1.8877562284469604,-1.5079739093780518,-1.7244763374328613,1.3106757402420044,0.1342976987361908,-0.2673788070678711,-1.6443556547164917,-1.5088523626327515,1.1965407133102417,-2.5874290466308594,-0.342885822057724,0.1690528243780136,-2.8896727561950684,-0.3589324653148651,-1.1495383977890015,-2.929826259613037,-2.3138864040374756,0.4748811721801758,1.2257497310638428,2.4864630699157715,2.1808226108551025,-0.36020830273628235,2.100621461868286,0.1605185717344284,0.33584752678871155,-2.9414167404174805,0.8445644378662109,0.14099246263504028,0.7435774803161621,-0.13816359639167786,-0.0510525219142437,-0.3151744604110718,-2.059082269668579,1.3702913522720337,-0.6845459938049316,-0.9444738626480103,-0.18076716363430023,-0.7780250310897827,1.1934808492660522,-1.6474997997283936,0.5358994603157043,2.0395736694335938,1.3943192958831787,0.4486767053604126,1.9809268712997437,1.1895225048065186,0.6888068318367004,-2.5997023582458496,1.7372807264328003,1.2377489805221558,-0.30645084381103516,2.1523327827453613,-0.718231737613678,-1.3975797891616821,1.5729377269744873,2.2882039546966553,1.468977689743042,-1.2245298624038696,-0.2771824598312378,0.41113242506980896,-1.0936094522476196,-2.3725218772888184,-0.44456785917282104,-1.201350450515747,0.027465278282761574,2.282764196395874,0.05126436799764633,-0.002765894867479801,-2.9408159255981445,0.6601296067237854,-0.17430326342582703,-0.6345909833908081,1.5148049592971802,0.5382689237594604,0.4744456112384796,-0.6769800782203674,-0.4220663905143738,1.8876327276229858,0.862011730670929,1.6636264324188232,-1.5729990005493164,1.3943313360214233,-2.5358028411865234,1.1258209943771362,-0.6354827880859375,-0.4173393249511719,-1.080558180809021,1.5860676765441895,-0.11065927147865295,1.1079154014587402,-1.391475796699524,0.12733139097690582,0.9691084027290344,1.1807039976119995,0.10530564934015274,-2.6106770038604736,0.00593950692564249,-2.4784443378448486,1.0893213748931885,0.6355000734329224,2.0124318599700928,-0.8367364406585693,-1.1023461818695068,-1.5201220512390137,-0.12951642274856567,-1.1635115146636963,-1.2944093942642212,-0.8251286149024963,-0.5292079448699951,1.0500125885009766,0.08640910685062408,-0.5664594769477844,1.1883773803710938,0.5873064994812012,-3.242966890335083,1.2475420236587524,-0.14925554394721985,-0.4268360733985901,-0.5747450590133667,-2.549923896789551,-0.26703861355781555,-1.2004402875900269,1.6519031524658203,1.7445452213287354,1.8404804468154907,0.14039762318134308,0.5937995910644531,-1.9289121627807617,-2.4434292316436768,0.06627494841814041,0.7107032537460327,0.9763338565826416,1.5549900531768799,1.7596096992492676,0.16977334022521973,-1.8885828256607056,-0.39976590871810913,1.3478200435638428,1.0479557514190674,1.1364010572433472,1.5970782041549683,-0.5857762098312378,0.16182778775691986,-1.0196757316589355,1.9919989109039307,-0.30608293414115906,1.077301025390625,0.7645629048347473,-0.5288717746734619,1.574992299079895,0.8947911858558655,0.6143794655799866,-1.69906747341156,-3.0568668842315674,-1.0731353759765625,-1.6807188987731934,-0.3808143734931946,-0.43339017033576965,0.4493027627468109,-2.5921075344085693,0.46271759271621704,1.044454574584961,-0.5222622752189636,-0.34562090039253235,1.3164308071136475,1.9807560443878174,-0.9420590996742249,-0.9353954195976257,1.3596621751785278,-1.0733938217163086,-0.6056832671165466,1.7782434225082397,0.6999127864837646,-2.717430353164673,2.289386510848999,-1.5712296962738037,-0.6144251823425293,-1.872055172920227,2.147648811340332,1.0608848333358765,-0.8698014616966248,-0.8756405711174011,-0.6639513373374939,-0.3248562812805176,-0.2399638146162033,-1.0885423421859741,-2.6482088565826416,-0.25799256563186646,-1.3861490488052368,-0.8127859234809875,2.050889730453491,1.6532864570617676,1.261709213256836,-1.3525599241256714,1.8872742652893066,-2.0722007751464844,-1.4341899156570435,-0.23485009372234344,-2.075948476791382,-1.5295021533966064,-1.4037201404571533,0.5276739597320557,-2.300248861312866,-2.0179684162139893,0.5148985981941223,1.5857915878295898,0.6919757723808289,-0.4802531898021698,-2.913898468017578,-2.7981462478637695,0.051464956253767014,0.2003205567598343,1.4646238088607788,0.07663299888372421,0.9057610630989075,-0.007569916546344757,0.1640036553144455,-2.699347734451294,1.5627228021621704,0.13031361997127533,-2.6923434734344482,-1.1660634279251099,1.7275513410568237,0.42434847354888916,-1.0087312459945679,0.5252354145050049,-2.002959728240967,-0.925709068775177,0.39751189947128296,1.0441808700561523,1.2354748249053955,-2.53848934173584,0.829951286315918,1.9960839748382568,-2.419447183609009,-1.0299838781356812,-1.1317057609558105,-1.2788145542144775,0.1294967532157898,1.0941871404647827,-1.2109698057174683,0.6860437989234924,-1.2950433492660522,1.1184189319610596,1.8198754787445068,-0.6728019118309021,0.08610408008098602,1.32585608959198,1.9759069681167603,-2.285844326019287,0.958662748336792,-1.232311725616455,1.7683030366897583,1.756262183189392,2.1084561347961426,-2.2716927528381348,1.1843812465667725,0.4246048629283905,-1.8913229703903198,1.2142314910888672,-0.3238558769226074,-1.3270831108093262,1.9104386568069458,-1.9700766801834106,-2.5339694023132324,1.35774564743042,0.37152978777885437,1.2786977291107178,-0.12386476993560791,1.742300271987915,-0.8340317606925964,-1.6883926391601562,-0.12894155085086823,-2.1527390480041504,-1.2324934005737305,1.7069419622421265,-1.4859099388122559,-0.19709190726280212,1.5631345510482788,-2.272106647491455,-0.476672887802124,1.503786325454712,0.7170316576957703,-2.6250007152557373,0.41992872953414917,1.762211561203003,-1.0329195261001587,1.5497208833694458,-0.16116467118263245,1.3508776426315308,1.675706148147583,0.11862444132566452,-0.2914246618747711,-0.9527323842048645,-1.4158155918121338,-2.5960466861724854,1.6364115476608276,-0.7446051836013794,1.5221471786499023,1.291601538658142,1.88179349899292,-2.5204031467437744,0.49824947118759155,1.6892409324645996,-1.939300537109375,0.7871072888374329,-0.9890927076339722,-1.6893632411956787,-0.5345204472541809,1.6982704401016235,-0.3296400010585785,-2.007234811782837,0.8314989805221558,-0.09244124591350555,0.328370064496994,2.039970636367798,0.30132535099983215,0.6282758116722107,-1.3793450593948364,-1.6892406940460205,-1.6064523458480835,-2.8058583736419678,-2.1184017658233643,-1.4995492696762085,-1.3782202005386353,1.9189536571502686,-0.14977635443210602,0.15873685479164124,-0.7654390931129456,-0.4388478100299835,-1.7642629146575928,1.4771599769592285,0.10560958087444305,-1.6587566137313843,-0.7516942024230957,1.6514356136322021,-2.984501600265503,-1.1777578592300415,1.0231961011886597,1.756934404373169,-0.07483446598052979,0.5625980496406555,-0.802454948425293,-2.174882650375366,0.3483526408672333,-0.20341207087039948,1.0935629606246948,1.8194860219955444,0.03168848901987076,2.1764166355133057,0.08891138434410095,-1.1616343259811401,0.04582963511347771,0.0732429027557373,2.047478199005127,1.3270258903503418,-3.0269854068756104,1.2594748735427856,-1.4719176292419434,0.6945852637290955,0.08914574980735779,2.3646082878112793,-0.5156047940254211,-1.774415135383606,0.3558565676212311,0.6902698278427124,0.07795694470405579,-0.09587352722883224,-1.9910368919372559,-0.5851841568946838,-1.641692876815796,-1.614327073097229,-0.6263812780380249,1.85573410987854,1.0280017852783203,-0.9200363755226135,-0.7520089149475098,0.8958845734596252,0.09229738265275955,-3.196855306625366,0.6695030331611633,0.9201352000236511,-2.6927902698516846,0.8031585812568665,0.6407989263534546,0.9697473049163818,-0.9006804823875427,-1.1435385942459106,-0.7048884034156799,-2.364664077758789,-1.4756158590316772,0.2087658941745758,-2.300367832183838,2.15889573097229,1.672275185585022,1.5146620273590088,-1.3698234558105469,-2.7476398944854736,-1.1256128549575806,-2.8386707305908203,-1.30722975730896,-1.4704327583312988,-2.864926338195801,-1.765918254852295,-2.234635353088379,1.0755375623703003,0.14472337067127228,-0.48944738507270813,0.2745499908924103,1.9671332836151123,-0.0007568690343759954,1.9229686260223389,-0.01337474212050438,-1.129216194152832,-2.559051275253296,-0.977131187915802,1.2234152555465698,2.1034200191497803,-0.6933262348175049,1.3779762983322144,0.4861508309841156,-2.09233021736145,-2.243077516555786,-1.1469902992248535,1.1187227964401245,-0.22527000308036804,1.4779492616653442,-1.7599282264709473,0.28835344314575195,0.004035677760839462,0.9550051093101501,-0.7553961873054504,-0.41017231345176697,1.5109875202178955,-0.5719350576400757,1.694697380065918,-0.5274214148521423,1.4109797477722168,-0.016897669062018394,0.09910876303911209,-0.9080726504325867,2.2055540084838867,0.009932450018823147,-2.1411895751953125,-1.6949983835220337,1.7441080808639526,2.3845374584198,-1.9338947534561157,-2.6499814987182617,-1.0019477605819702,-1.0536137819290161,0.2270406186580658,-0.45370253920555115,-2.657301425933838,-2.63150691986084,-2.695544481277466,1.3795830011367798,-2.1520063877105713,-0.44926074147224426,0.1613226979970932,0.25955501198768616,0.6801518201828003,1.1166274547576904,0.6522380113601685,0.11962170153856277,-2.5708415508270264,1.5863465070724487,-0.12172658741474152,-0.6020715832710266,-0.3625606298446655,1.256956934928894,1.7322964668273926,-0.7969076633453369,2.090792179107666,0.7436695694923401,1.729678988456726,0.1383400857448578,1.1945664882659912,-0.9006137251853943,1.6909451484680176,-0.6235017776489258,-1.892280101776123,1.8758724927902222,-1.253258228302002,1.7604315280914307,0.966856837272644,-1.254814863204956,-0.2637898623943329,0.6907366514205933,-2.8270487785339355,-0.5137991309165955,-1.6795755624771118,-0.892179012298584,-2.8031609058380127,0.498908668756485,-2.2390992641448975,-0.22810791432857513,1.5292044878005981,0.09257836639881134,-1.003053903579712,1.0403584241867065,1.7771093845367432,0.23326437175273895,-1.2215551137924194,-0.2499510943889618,-2.1201839447021484,0.33344900608062744,1.3742717504501343,1.310054898262024,1.7306112051010132,1.6277698278427124,-2.263686418533325,-1.8880720138549805,-0.12904316186904907,0.274108350276947,0.24157710373401642,1.6937239170074463,-1.9346643686294556,-1.8081071376800537,1.4179909229278564,-1.5874744653701782,1.8474150896072388,-1.2354615926742554,-1.7858084440231323,-1.1618413925170898,-2.7324910163879395,-1.2649478912353516,-0.21193340420722961,1.331071138381958],"xaxis":"x","y":[-0.0037877128925174475,-0.034254465252161026,0.0069166189059615135,-0.05983343347907066,0.057142239063978195,0.07263773679733276,0.0478069931268692,-0.12789006531238556,0.06432900577783585,-0.027598721906542778,-0.05314316228032112,-0.060539405792951584,-0.09759906679391861,0.11624348163604736,-0.029909104108810425,-0.08943327516317368,0.02054153010249138,0.0166257806122303,0.0331452339887619,0.06343584507703781,0.12520967423915863,-0.0404345765709877,-0.0281162541359663,-0.02033558301627636,-0.06082909181714058,-0.06791817396879196,-0.014203733764588833,-0.00504468334838748,-0.001181425410322845,-0.04363955557346344,0.09321114420890808,-0.03349064663052559,0.009700813330709934,-0.0789995789527893,-0.004002554342150688,-0.05045834928750992,0.016355715692043304,-0.040538907051086426,-0.08656807243824005,0.03016827255487442,0.015190137550234795,0.038196925073862076,0.053412266075611115,0.07081656157970428,0.08408752083778381,-0.03192102909088135,0.056513793766498566,0.0880374163389206,-0.11159535497426987,-0.008505628444254398,0.02851288579404354,0.009742644615471363,0.1500285118818283,-0.049061402678489685,0.0031754302326589823,-0.01264811772853136,0.04938399791717529,0.040913112461566925,-0.02021847479045391,0.050201453268527985,0.06030363589525223,-0.07159881293773651,0.04670806601643562,0.008978302590548992,0.06408902257680893,-0.07725466787815094,-0.13078612089157104,-0.0827772244811058,0.06974197924137115,0.04073365032672882,-0.06557480990886688,-0.0015217355685308576,0.026849696412682533,-0.08235134929418564,0.007589705288410187,0.005396696738898754,-0.06474964320659637,-0.018446221947669983,-0.08919725567102432,0.012644656002521515,0.06152867525815964,0.0710231363773346,-0.020287107676267624,-0.022164927795529366,0.03439110890030861,0.02608742006123066,-0.029672730714082718,0.06436271220445633,0.011269811540842056,-0.14657171070575714,0.05122286081314087,-0.004223732743412256,0.0716436356306076,0.08778773993253708,0.059793442487716675,-0.11542289704084396,0.07367560267448425,-0.012343304231762886,-0.053450342267751694,-0.0758640468120575,-0.006184447556734085,-0.033611588180065155,0.04190738871693611,-0.03041287697851658,-0.14879627525806427,-0.007237154059112072,0.11392935365438461,0.01397672574967146,-0.0381193533539772,0.028336280956864357,0.08262529224157333,-0.06989623606204987,0.04872826486825943,-0.037010107189416885,0.0469343401491642,0.028050189837813377,0.0325467512011528,0.06575074046850204,0.08490520715713501,-0.07233882695436478,-0.050059836357831955,-0.10145852714776993,-0.03270265460014343,0.04005734249949455,0.03790462762117386,-0.06640474498271942,0.07684341818094254,-0.028445763513445854,0.025455817580223083,-0.08712703734636307,-0.022039499133825302,0.09712721407413483,0.01887989416718483,0.10611467808485031,0.060135453939437866,0.23113028705120087,0.13256962597370148,-0.0846862941980362,0.0532742403447628,-0.038196828216314316,0.06276925653219223,0.10719020664691925,0.02324005775153637,0.10890183597803116,-0.11122829467058182,0.03761681541800499,0.11443958431482315,-0.02513745240867138,-0.04329710081219673,0.054298464208841324,0.0774960145354271,-0.06738069653511047,0.01752689853310585,-0.0020145692396909,-0.0019219176610931754,-0.00983287114650011,0.10534338653087616,0.07950543612241745,0.04137863218784332,-0.0686875507235527,-0.11432277411222458,0.04130789265036583,0.032847970724105835,0.03297174721956253,-0.027062933892011642,-0.11969731748104095,0.06774221360683441,0.01800304837524891,-0.0801955834031105,-0.005311768036335707,-0.002064947970211506,-0.003297344082966447,-0.06533072888851166,0.04274550452828407,-0.018863825127482414,0.1776682436466217,-0.017906824126839638,0.07982375472784042,-0.05135049670934677,-0.033809248358011246,-0.0045103393495082855,0.05184987187385559,0.07384782284498215,-0.032771993428468704,-0.09745443612337112,0.04754676669836044,-0.06649608910083771,0.011279615573585033,-0.05923999473452568,0.10818891227245331,-0.0362921841442585,-0.08009965717792511,-0.07783430814743042,0.04362352564930916,-0.06623679399490356,0.04674331471323967,0.021538667380809784,0.018047498539090157,-0.06929431855678558,0.042007926851511,0.07398565113544464,-0.003283961210399866,0.061507437378168106,0.013144277967512608,-0.03180532157421112,0.013646669685840607,-0.03972867503762245,0.07240922003984451,-0.12458422034978867,-0.046490978449583054,-0.1331271380186081,-0.06461464613676071,-0.03570614382624626,-0.06014647334814072,0.08576376736164093,0.023449786007404327,-0.07480743527412415,-0.0005186502821743488,-0.018255574628710747,-0.010772375389933586,-0.09953390061855316,-0.05544614791870117,-0.010825661942362785,0.12000563740730286,-0.036756616085767746,0.009905579499900341,-0.03848602995276451,0.09930697083473206,0.07779964804649353,-0.018712535500526428,-0.019222620874643326,0.17730076611042023,0.04240228235721588,-0.13396653532981873,-0.11240079253911972,0.0546482615172863,0.05023782700300217,0.10540521144866943,0.11032537370920181,-0.08287254720926285,0.08299937844276428,0.1612236201763153,0.011692014522850513,0.02681209333240986,-0.08619450032711029,0.04297932982444763,0.08969505876302719,0.02478218637406826,-0.06542147696018219,-0.07405992597341537,-0.07176236808300018,0.06313100457191467,-0.029626993462443352,-0.08145745098590851,0.020251920446753502,-0.08674491941928864,0.02140074595808983,-0.11601290851831436,-0.022784000262618065,-0.02551965042948723,-0.05649314448237419,-0.11514522135257721,0.029927410185337067,0.008522890508174896,0.08325661718845367,-0.13728153705596924,0.013227177783846855,0.020946253091096878,0.004854721482843161,0.007443990092724562,-0.03942449018359184,0.11527244746685028,0.015925196930766106,0.09641752392053604,-0.048449404537677765,-0.139724999666214,0.037442319095134735,0.03678290173411369,0.10424354672431946,-0.038315560668706894,-0.04435110464692116,-0.061436574906110764,-0.06094677373766899,0.020023461431264877,-0.04300134256482124,0.011320849880576134,-0.06683936715126038,0.008859270252287388,-0.032983385026454926,-0.026944488286972046,-0.015532427467405796,-0.03717866539955139,0.04706777632236481,-0.0246557779610157,-0.015865033492445946,-0.01039816439151764,0.028297562152147293,0.01089184544980526,-0.040534257888793945,0.02218698151409626,0.003004126250743866,0.018289409577846527,-0.03031129203736782,0.05303278565406799,0.023760203272104263,-0.059801530092954636,-0.0347757488489151,0.11628327518701553,0.09163829684257507,-0.06736322492361069,-0.05254434421658516,0.0022955299355089664,-0.02904374897480011,0.06081918254494667,-0.005802156403660774,-0.02000543288886547,-0.0031506658997386694,-0.029267657548189163,-0.040521785616874695,0.01827949658036232,0.01216996181756258,-0.002525999676436186,-0.07486028969287872,0.030958866700530052,0.0547466054558754,0.05967353284358978,-0.020122703164815903,-0.07476861774921417,-0.11629052460193634,0.028215890750288963,-0.023502444848418236,-0.024434838443994522,0.0111978305503726,-0.03334619104862213,0.023241549730300903,0.010672366246581078,0.07440199702978134,0.018224062398076057,-0.09583266079425812,0.1739647388458252,0.08850056678056717,-0.0053534358739852905,-0.03982014209032059,0.13257741928100586,0.05130619928240776,-0.15323413908481598,0.026074346154928207,-0.11258748918771744,-0.07670637965202332,-0.026124294847249985,0.06256454437971115,-0.03738078102469444,0.013661732897162437,0.06865492463111877,0.00019836371939163655,-0.006311193108558655,-0.0033694456797093153,-0.057141732424497604,-0.09016375988721848,-0.0716436356306076,-0.06664233654737473,0.002481223316863179,0.007165326736867428,0.028912412002682686,0.04563451558351517,0.030723415315151215,0.05608374997973442,-0.0030843631830066442,0.013345862738788128,-0.0742853581905365,0.1364476978778839,-0.04817454516887665,-0.06419163197278976,-0.11176344752311707,0.09605163335800171,-0.042566027492284775,0.05328463390469551,0.03888227045536041,-0.021049680188298225,0.027414651587605476,0.021050507202744484,0.07069025188684464,-0.07888369262218475,0.009227583184838295,-0.05847446992993355,-0.007811808492988348,-0.04838342219591141,-0.005411829333752394,0.02583060972392559,-0.019328642636537552,0.01571747474372387,0.09103988856077194,-0.02722873166203499,0.02238226868212223,-0.11866818368434906,0.043342139571905136,0.03362322971224785,-0.08735982328653336,0.014101763255894184,0.04286026954650879,0.03692061826586723,-0.017575571313500404,-0.07506390661001205,-0.030531061813235283,-0.009081241674721241,-0.1872541457414627,0.046374984085559845,-0.03579777106642723,-0.07814415544271469,-0.09561971575021744,0.038339052349328995,0.16514526307582855,0.007466341368854046,-0.13686640560626984,-0.045719463378190994,-0.0177200585603714,-0.04595015197992325,0.06567146629095078,-0.01275099441409111,-0.057845983654260635,0.02290087379515171,0.09884161502122879,-0.011441830545663834,-0.026240399107336998,-0.004789205268025398,0.12100530415773392,-0.04975113272666931,-0.03990401700139046,-0.03604932129383087,-0.050063662230968475,-0.08916250616312027,-0.03208538889884949,-0.0350663848221302,-0.0372651070356369,-0.07447165250778198,0.08354542404413223,0.06473132222890854,-0.046022117137908936,0.001223264611326158,-0.13247793912887573,-0.04285053536295891,-0.003159652929753065,-0.02834080159664154,0.0466860830783844,-0.12127344310283661,0.08279284089803696,-0.018855800852179527,0.07737241685390472,-0.024966947734355927,-0.015988221392035484,0.01153713371604681,-0.01895744912326336,0.08173062652349472,0.09500513970851898,0.13033661246299744,0.09373104572296143,0.06700007617473602,-0.04636111855506897,-0.0045795501209795475,-0.03514226898550987,0.04298688843846321,0.046082448214292526,0.08908671140670776,0.016278570517897606,0.09656120091676712,0.021448563784360886,-0.07713370025157928,-0.07221820950508118,0.024178817868232727,0.08790746331214905,0.05079606547951698,-0.06928394734859467,0.002081207698211074,0.07278068363666534,0.02016260102391243,0.1073017418384552,-0.038603972643613815,-0.03644886240363121,0.017599672079086304,-0.08405415713787079,0.16397106647491455,0.012123076245188713,0.008317595347762108,-0.06073775514960289,0.02157108671963215,-0.09481111168861389,0.04543474689126015,-0.04952124506235123,-0.017592106014490128,-0.09888361394405365,-0.028765780851244926,-0.1082138791680336,-0.06793440133333206,-0.06710544228553772,0.02707059495151043,-0.0923924446105957,0.01801432855427265,-0.00648565124720335,0.04526570066809654,-0.05338621884584427,-0.04008971154689789,0.002824349096044898,0.04711298644542694,-0.06676498800516129,0.01712757535278797,0.017062796279788017,-0.025231512263417244,-0.09800775349140167,-0.032589733600616455,0.08471536636352539,-0.002156982896849513,0.003517718054354191,0.016499346122145653,-0.008268986828625202,-0.047833409160375595,0.09153936803340912,0.07291848212480545,-0.08877915143966675,0.02423086389899254,-0.05032925680279732,0.0012625762028619647,-0.05255696550011635,-0.01147371530532837,0.001433965633623302,-0.00551520986482501,0.026967886835336685,0.04611159861087799,-0.010035167448222637,-0.08334794640541077,-0.030839689075946808,-0.0062157269567251205,0.025354886427521706,0.0010430100373923779,0.0242741871625185,-0.04592365771532059,-0.030352642759680748,0.015632446855306625,0.003278958611190319,0.021406378597021103,0.05985552445054054,-0.019714364781975746,0.08969064801931381,0.025210266932845116,-0.0950060710310936,-0.03544372320175171,0.0027830591425299644,-0.06743425130844116,-0.10018011182546616,0.022882509976625443,0.18872128427028656,-0.04863015189766884,-0.03966856747865677,0.02031680755317211,-0.009196418337523937,0.001734457677230239,0.04033626616001129,0.041779931634664536,0.04738519713282585,0.0015675898175686598,0.09391432255506516,-0.08884643018245697,0.0497400276362896,0.05011054873466492,-0.0033335548359900713,0.07755869626998901,0.0636816918849945,-0.06580393016338348,0.05068422853946686,-0.0915345847606659,-0.011614611372351646,-0.04129180312156677,-0.073977991938591,0.041045039892196655,-0.09614492207765579,0.12169962376356125,0.0620260089635849,0.036012765020132065,0.018213588744401932,-0.06760339438915253,-0.03777115046977997,-0.004954126663506031,0.07287641614675522,-0.1066899448633194,-0.08541475236415863,0.03601733595132828,0.06571394205093384,0.05462777614593506,0.0010956295300275087,0.0021319244988262653,-0.0012447546469047666,0.06760120391845703,-0.027562757954001427,0.0824194848537445,-0.016958286985754967,-0.17654141783714294,-0.07241617143154144,-0.0816110372543335,-0.06184712052345276,0.005067229270935059,-0.10550688207149506,-0.16177554428577423,0.018048306927084923,-0.033893998712301254,0.05368085950613022,0.01034794095903635,-0.11681108176708221,-0.055561140179634094,0.049313150346279144,-0.010845587588846684,-0.054288581013679504,0.015029791742563248,-0.07580764591693878,0.06513963639736176,0.002982382196933031,-0.05360721796751022,0.021351808682084084,0.059096381068229675,-0.04443621635437012,0.09759952127933502,-0.019169848412275314,-0.037834614515304565,-0.02928621880710125,-0.01787436380982399,-0.04964073747396469,-0.03269754350185394,0.13159649074077606,-0.06507811695337296,-0.009310952387750149,-0.09567653387784958,-0.004811391234397888,-0.030182458460330963,0.023653727024793625,0.03714619576931,-0.030157631263136864,-0.09711982309818268,-0.07751978933811188,-0.056610479950904846,0.14740853011608124,-0.07959435880184174,-0.028239235281944275,-0.09581628441810608,0.1114516630768776,0.005452334880828857,-0.06795874238014221,-0.07665768265724182,0.09835173934698105,0.03374297171831131,0.01135962177067995,-0.05095285177230835,-0.027034126222133636,0.17684565484523773,0.03638497740030289,-0.049192413687705994,-0.048090267926454544,0.07721074670553207,-0.09655006229877472,0.045330408960580826,-0.010884122923016548,0.06709787994623184,0.04505395144224167,-0.009039707481861115,-0.047529324889183044,0.008018871769309044,-0.0016598216025158763,-0.05538155511021614,-0.007563285529613495,0.140081524848938,0.09934567660093307,-0.08184105157852173,-0.01716139353811741,0.04103398323059082,-0.05418790131807327,-0.10914312303066254,-0.010314748622477055,0.044857606291770935,0.08105123043060303,-0.035495974123477936,-0.011514581739902496,-0.01531638391315937,0.02591053582727909,-0.022492583841085434,-0.04458264261484146,-0.07791240513324738,0.047294870018959045,-0.006481866352260113,-0.019890548661351204,0.10046817362308502,0.03276403620839119,-0.00992244016379118,0.03155168145895004,-0.015902739018201828,-0.09537902474403381,0.04795249551534653,-0.0953940898180008,-3.7266156141413376e-05,0.02731204219162464,-0.030477292835712433,-0.0038928412832319736,-0.0769663080573082,-0.030129324644804,0.15674489736557007,0.1214827224612236,-0.04530330374836922,0.030199771746993065,-0.16299577057361603,-0.017867354676127434,-0.04529866203665733,-0.06084394082427025,-0.12978465855121613,-0.020757293328642845,-0.07869481295347214,-0.07598154246807098,-0.048425763845443726,-0.00740914698690176,0.08576055616140366,-0.003010210581123829,-0.08562709391117096,0.006488177925348282,-0.0344020277261734,-0.05176243558526039,-0.039310522377491,-0.022043377161026,-0.006734474096447229,0.049732744693756104,-0.1200004443526268,0.04105090722441673,0.002184666693210602,-0.005033391527831554,-0.023834440857172012,-0.04083678498864174,-0.025285204872488976,0.006012027617543936,-0.0009086162317544222,0.0015780478715896606,-0.02470340207219124,0.0625515878200531,0.15216602385044098,0.05664379149675369,-0.004041832406073809,-0.0614103227853775,-0.08715949207544327,0.11758438497781754,-0.06581961363554001,-0.09518905729055405,-0.050154153257608414,0.016759542748332024,-0.0033234816510230303,-0.09794851392507553,0.0027280335780233145,-0.080005943775177,0.07667382061481476,-0.08541189879179001,0.033329468220472336,-0.014680449850857258,-0.04754992201924324,-0.030613180249929428,0.13628050684928894,0.0025787553749978542,-0.01665392331779003,0.00604961859062314,-0.01711343228816986,-0.10238759964704514,0.0829392522573471,-0.13356271386146545,-0.001167065929621458,-0.04139820486307144,0.056887295097112656,-0.05539301410317421,0.019895724952220917,0.005491737276315689,-0.033589739352464676,-0.06607463210821152,0.0805569440126419,-0.017722219228744507,-0.002479758346453309,0.03309028223156929,0.007139168679714203,-0.0993046835064888,-0.006186995655298233,0.12669117748737335,-0.02126966416835785,-0.046813879162073135,-0.034153442829847336,0.020962480455636978,-0.1115984097123146,0.04602191597223282,-0.1698186695575714,0.027636412531137466,-0.1358662247657776,0.09210533648729324,0.06207680329680443,0.1547987163066864,0.03641071170568466,0.014711780473589897,-0.003956717438995838,0.05821996182203293,0.057713307440280914,0.05510440841317177,0.002532580867409706,0.07425960153341293,-0.015237013809382915,0.005788923241198063,-0.042361676692962646,0.011126946657896042,-0.09134092926979065,-0.04223917797207832,0.009631317108869553,0.027388833463191986,-0.07177022844552994,0.008159604854881763,0.03552646562457085,0.09699732065200806,0.0029394596349447966,-0.020862862467765808,0.000557968916837126,-0.10182713717222214,0.026546139270067215,0.03337745741009712,-0.04343754053115845,0.09530088305473328,0.0035361715126782656,-0.011203491128981113,0.09462151676416397,0.018559416756033897,-0.15050108730793,-0.02609236165881157,-0.0030824800487607718,0.06159153953194618,0.05506886541843414,-0.05215783789753914,0.05988515168428421,0.034109704196453094,-0.15360775589942932,0.02695191279053688,0.05102090910077095,-0.06125390902161598,-0.05510631576180458,0.027071064338088036,-0.1296301782131195,0.02531500533223152,-0.0032186610624194145,0.05306192860007286,0.017432134598493576,-0.06505247950553894,0.0036605556961148977,0.03379453346133232,0.014390013180673122,-0.0699935108423233,0.06738509237766266,-0.020480738952755928,-0.04627307504415512,-0.039656396955251694,0.030194593593478203,0.11302323639392853,0.07981443405151367,-0.0040887887589633465,0.040713775902986526,0.07584201544523239,-0.04039732739329338,0.040943846106529236,0.05256384238600731,-0.09092525392770767,-0.018047714605927467,-0.002795004053041339,-0.03414464369416237,0.001543046091683209,0.0050144195556640625,0.03865008428692818,0.011943376623094082,0.006699426099658012,-0.12575435638427734,0.022252274677157402,-0.11446774005889893,-0.003351061139255762,0.0719989761710167,-0.021315913647413254,-0.047665130347013474,0.04256123676896095,0.07415846735239029,0.049410201609134674,-0.06846944987773895,0.038229405879974365,0.0070910076610744,-0.04845196381211281,-0.037520863115787506,0.051262836903333664,-0.09088369458913803,-0.03486622869968414,-0.0069768973626196384,-0.09682480990886688,-0.11099021136760712,0.032166704535484314,-0.06197477504611015,0.05858875811100006,0.0992560163140297,-0.0027981786988675594,-0.0870671197772026,-0.005122033879160881,-0.056448351591825485,-0.001443074899725616,0.007218929473310709,-0.07854640483856201,-0.09671088308095932,-0.03955673426389694,0.002322981134057045,-0.03128211945295334,0.07500387728214264,0.04213353618979454,-0.19000349938869476,0.09477809816598892,0.11778128892183304,0.04613321274518967,-0.025870606303215027,0.003682081587612629,-0.02975650131702423,0.04596111923456192,-0.11426334828138351,0.019570987671613693,-0.018487177789211273,0.043583955615758896,-0.14030425250530243,0.08333255350589752,-0.04904540628194809,-0.03279174491763115,0.05763756111264229,0.020809819921851158,0.02908160164952278,0.04350324347615242,-0.015733379870653152,-0.03922083228826523,-0.061373766511678696,-0.09093085676431656,0.11651958525180817,-0.05665184557437897,0.005278885364532471,0.02297208271920681,-0.05903635919094086,-0.02182011306285858,0.07202808558940887,0.08939029276371002,-0.031471338123083115,0.029253896325826645,0.020641524344682693,0.021428072825074196,-0.0805431380867958,-0.08179599791765213,-0.039028942584991455,0.11156561970710754,-0.11630236357450485,0.06770027428865433,0.06076575443148613,0.1019834354519844,0.02121058478951454,0.1557307094335556,0.06125520542263985,0.035370081663131714,-0.018016500398516655,0.001154748722910881,-0.06355039775371552,-0.01632176898419857,0.06043996661901474,0.008034348487854004,-0.07788243144750595,-0.1291450560092926,-0.008810613304376602,-0.039792563766241074,0.05881049111485481,0.008045719936490059,-0.09432056546211243,0.08269161731004715,0.03471524640917778,0.06371967494487762,0.035261768847703934,0.026752276346087456,-0.011761030182242393,-0.018851669505238533,-0.04178290441632271,-0.02819364331662655,0.0016978695057332516,0.07811887562274933,0.046862952411174774,0.11878737062215805,-0.07251143455505371,0.09860488772392273,-0.08705861121416092,0.01071957778185606,-0.010624416172504425,-0.02198161371052265,-0.002777388319373131,0.13305263221263885,-0.035380665212869644,-0.06441837549209595,-0.08936507254838943,-0.051051441580057144,0.07856395840644836,0.03909583389759064,-0.02549230121076107,-0.10858915746212006,-0.06950763612985611,-0.05084221810102463,0.04376380890607834,-0.020929183810949326,-0.09463535249233246,0.0397702120244503,-0.0630330741405487,0.03498945012688637,-0.001714017242193222,0.02459734119474888,-0.08852874487638474,0.038396913558244705,-0.092405304312706,-0.028856204822659492,0.05601706728339195,-0.152348130941391,0.06460370123386383,-0.040205709636211395,-0.10316193848848343,-0.056336577981710434,0.06598974764347076,0.05596296489238739,-0.0347408652305603,-0.057370901107788086,-0.10813616216182709,-0.01725481078028679,0.04158404469490051,0.02485734410583973,0.007073846645653248,0.06618338078260422,0.07723680883646011,-0.06272372603416443,0.054677292704582214,-0.02057076431810856,0.10344266891479492,-0.021318208426237106,-0.0808640718460083,0.02272217534482479,-0.04733454808592796,0.0721760243177414,0.006909030489623547,-0.012769746594130993,-0.023347072303295135,0.0010944963432848454,-0.00992034561932087,0.019491072744131088,0.04306279867887497,-0.056067366153001785,0.05303715169429779,0.0010108252754434943,0.029967518523335457,-0.01778426021337509,0.05206599831581116,-0.003556851064786315,0.09256644546985626,-0.05136976018548012,0.052556585520505905,0.025633489713072777,0.09805107861757278,-0.0813499242067337,-0.005999886896461248,0.14494292438030243,0.07993830740451813,0.04498996213078499,0.03256924822926521,-0.08049973845481873,0.09375083446502686,0.02263461798429489,-0.0009874968091025949,-0.030886223539710045,0.008263347670435905,-0.043360110372304916,-0.01107100211083889,-0.05465935915708542,0.10888030380010605,0.04304889217019081,0.05937002971768379,-0.03163077309727669,0.011436947621405125,0.016681073233485222,0.04268571361899376,0.017358645796775818,-0.12072190642356873,-0.07112493366003036,-0.09875873476266861,0.06079069897532463,0.0415094755589962,-0.08252178132534027,-0.026081375777721405,0.04170132428407669,0.06826658546924591,0.0004947225679643452,-0.061873335391283035,0.11436304450035095,-0.07396450638771057,0.021327488124370575,0.02744816057384014,-0.020316503942012787,0.07717151939868927,-0.014553303830325603,-0.02925035171210766,0.005135840270668268,0.06813023239374161,-0.052938152104616165,0.02174178697168827,-0.0073647769168019295,-0.0952392965555191,0.017226094380021095,0.06285058706998825,-0.007566392887383699,0.10989730805158615,0.059888698160648346,0.03567870333790779,0.039596471935510635,0.11222133040428162,0.017772430554032326,0.14302703738212585,-0.11652720719575882,-0.07876425236463547,0.002500082366168499,0.04246976971626282,0.03585681691765785,0.01597372442483902,-0.0020507890731096268,-0.1418447494506836,0.05494306981563568,0.041760023683309555,-0.05698902904987335,-0.043428149074316025,0.05192049965262413,-0.0035658341366797686,-0.05925712361931801,0.07023842632770538,-0.05721459537744522,0.08410695940256119,-0.0652688518166542,0.01653360016644001,0.06171349808573723,-0.04092708230018616,0.10498751699924469,-0.05496754124760628,0.02862134762108326,-0.02194126322865486,0.06405068188905716,-0.13172577321529388,0.07799416035413742,0.09225153177976608,0.051523465663194656,-0.018785808235406876,-0.0867353156208992,-0.10769757628440857,0.10735046863555908,0.04865480959415436,0.01882905699312687,-0.03613380342721939,0.03677988424897194,-0.033605340868234634,-0.021402623504400253,-0.18005341291427612,-0.11791810393333435,0.013361613266170025,0.08577404916286469,-0.10307541489601135,0.10780222713947296,0.05872069299221039,-0.037661731243133545,0.012307723052799702,0.02686384692788124,0.0399218387901783,0.018972182646393776,-0.02074400894343853,-0.0599592849612236,0.05843500792980194,-0.025027833878993988,-0.05590210482478142,0.01334905531257391,0.03858502209186554,-0.05768784135580063,0.04144397750496864,-0.05099678784608841,0.04158135876059532,-0.03492719680070877,-0.003955174703150988,0.050129055976867676,-0.08508995920419693,-0.002916234778240323,0.09439056366682053,-0.07089166343212128,0.028157705441117287,-0.005101674702018499,0.001502170693129301,0.060251545161008835,0.09904288500547409,-0.06732165068387985,0.09355021268129349,0.009923460893332958,0.0264241024851799,0.009322237223386765,-0.089693583548069,0.13561443984508514,0.013612218201160431,-0.04143780842423439,0.029194606468081474,-0.09026023000478745,0.051718082278966904,0.14090470969676971,0.051010217517614365,-0.07693928480148315,-0.08131086081266403,0.037274137139320374,0.04245148226618767,0.026741739362478256,-0.08947134017944336,-0.03143467754125595,0.03182196617126465,-0.012957443483173847,-0.12699027359485626,0.0695149153470993,-0.06891348958015442,0.06719354540109634,0.026257650926709175,-0.09850220382213593,0.06691472977399826,0.10826603323221207,0.06645212322473526,-0.053159285336732864,0.031035179272294044,-0.13822676241397858,0.003185640787705779,-0.05781722441315651,-0.045868199318647385,-0.0053371042013168335,0.019989699125289917,0.054039932787418365,-0.07547895610332489,0.004457154776901007,0.04897970333695412,0.08353014290332794,0.013014494441449642,0.010120469145476818,-0.048302970826625824,-0.06391111761331558,-0.08877893537282944,0.0199603121727705,0.04188575968146324,-0.04028990492224693,0.010345079936087132,0.05548339709639549,0.04083692282438278,0.037561528384685516,-0.052438344806432724,0.08244963735342026,0.06262639909982681,0.08750685304403305,0.08943124115467072,0.11272464692592621,0.11068414151668549,-0.03199958801269531,0.011342876590788364,-0.031311117112636566,-0.07313574850559235,-0.0912105068564415,0.031957004219293594,-0.007119383197277784,-0.008836112916469574,0.1331396847963333,-0.022321874275803566,-0.026668038219213486,-0.011342291720211506,-0.07404446601867676,0.08403803408145905,-0.00039749775896780193,0.1798015832901001,-0.041665975004434586,-0.01767890155315399,-0.055946290493011475,0.031692009419202805,-0.03384406864643097,-0.02054234966635704,0.11559686809778214,0.07215815037488937,-0.03806360065937042,0.03918379545211792,0.04691694676876068,0.057251300662755966,-0.01800128072500229,0.0903395488858223,-0.07486501336097717,-0.038244158029556274,-0.03146173432469368,0.14624719321727753,0.03183569759130478,0.06900838762521744,0.007792344782501459,-0.05249151214957237,-0.03447234258055687,0.0450788289308548,0.10861930251121521,-0.01989138312637806,0.0344565324485302,-0.0033339783549308777,-0.050979044288396835,-0.004854253958910704,-0.018081851303577423,0.0494316928088665,-0.008832828141748905,0.09154567122459412,0.12977932393550873,0.020883027464151382,0.028460312634706497,-0.016021739691495895,0.17262719571590424,0.0785684585571289,-0.04459168761968613,-0.008826090954244137,0.01560116559267044,0.01905466429889202,-0.011858245357871056,-0.0869918167591095,-0.11188799887895584,0.005562020465731621,0.10164462774991989,0.009235193021595478,0.04953211545944214,0.1582432985305786,-0.02679619938135147,-0.17053988575935364,-0.022951629012823105,-0.012310815043747425,0.15023332834243774,-0.058201152831315994,0.05297518149018288,0.044670991599559784,-0.023736054077744484,0.04733063653111458,0.05034960061311722,-0.06514086574316025,0.09020188450813293,-0.02058674953877926,-0.061624594032764435,-0.018451709300279617,0.01586131379008293,-0.03735792264342308,-0.0034699146635830402,0.020098352804780006,-0.04709174111485481,-0.07034194469451904,0.0633692666888237,0.0999472588300705,0.03446291759610176,-0.04226148501038551,-0.049885403364896774,-0.027031028643250465,-0.051705874502658844,0.009273461997509003,0.09994909912347794,-0.03857961297035217,-0.057116229087114334,0.04504672810435295,0.07673812657594681,-0.017063569277524948,0.051955439150333405,0.005000431090593338,0.17638984322547913,0.034085363149642944,0.0733441635966301,0.06241978332400322,0.07329563051462173,0.014687098562717438,0.02630496770143509,-0.08405458927154541,0.047769252210855484,-0.011716140434145927,-0.06455382704734802,-0.008149509318172932,-0.021279988810420036,0.10850166529417038,-0.017650991678237915,-0.11231786012649536,-0.03359308838844299,0.007206534035503864,0.06809750199317932,0.021977802738547325,-0.012152805924415588,-0.020185906440019608,0.008355091325938702,-0.042237188667058945,0.010958724655210972,0.09582360088825226,-0.10777487605810165,-0.11291811615228653,-0.16399098932743073,-0.07405150681734085,-0.023266633972525597,0.016838524490594864,0.019148509949445724,0.022897599264979362,-0.08025163412094116,0.09577985852956772,0.030025582760572433,0.025715408846735954,0.031356338411569595,-0.019650211557745934,-0.05446653440594673,-0.01494359690696001,0.0041838097386062145,-0.00042476027738302946,-0.0077612996101379395,0.004702962469309568,-0.027800751850008965,0.09516522288322449,0.08042726665735245,-0.06662631034851074,-0.005593586713075638,0.03881240263581276,0.003410287667065859,-0.009139705449342728,-0.017758749425411224,-0.002812874736264348,-0.05723064765334129,-0.07536886632442474,0.03818925470113754,-0.08690406382083893,-0.11421874910593033,-0.050382159650325775,-0.08058357238769531,-0.13767090439796448,0.02974618785083294,-0.1091388463973999,-0.05262936279177666,-0.025843840092420578,-0.10667062550783157,-0.02127084881067276,0.01590828225016594,-0.014890165068209171,0.03773987293243408,-0.00466058449819684,0.022803816944360733,0.0001811372785596177,-0.0017265123315155506,0.05263683572411537,-0.017590472474694252,0.06172632798552513,0.026691798120737076,-0.04381267726421356,0.016964895650744438,0.1415524035692215,0.01811187155544758,0.026005178689956665,-0.029381144791841507,-0.01984969712793827,0.08053998649120331,0.09397264569997787,0.06821716576814651,-0.026542723178863525,-0.08177094906568527,0.005780558101832867,0.021763993427157402,-0.17196141183376312,-0.08528780937194824,-0.07025088369846344,-0.023437514901161194,-0.10019197314977646,-0.001562323304824531,-0.054256752133369446,0.013966645114123821,-0.01691441982984543,-0.09433221071958542,0.015863681212067604,-0.03670967370271683,0.03111536055803299,-0.1409148871898651,-0.10561572015285492,0.08023663610219955,0.026496972888708115,-0.06972123682498932,-0.01856723614037037,0.03532953932881355,-0.0722557008266449,0.06914307922124863,-0.026219289749860764,-0.08962422609329224,-0.019095169380307198,-0.0692947506904602,0.01182498224079609,-0.009251351468265057,0.10143197327852249,0.04547965154051781,-0.016154399141669273,0.040598560124635696,0.06407162547111511,-0.06904571503400803,-0.013820062391459942,-0.11095497012138367,0.0758974552154541,0.030006185173988342,-0.00740831671282649,0.020363643765449524,0.0718255266547203,0.0024637996684759855,0.10362492501735687,0.04205668717622757,-0.04466988146305084,-0.09217415750026703,-0.006316934246569872,-0.0640118345618248,0.04245322197675705,-0.01489232201129198,-0.0726155936717987,-0.032244935631752014,-0.061604805290699005,-0.05117685720324516,0.03191331773996353,0.011928432621061802,0.021503206342458725,0.06392430514097214,0.028485408052802086,0.003229778492823243,-0.0005379399517551064,-0.057434335350990295,0.054573655128479004,0.005095964297652245,-0.05675753578543663,0.02015378512442112,0.09324927628040314,0.06374739110469818,-0.05120239034295082,0.11730962991714478,0.06682434678077698,0.032796140760183334,-0.09026284515857697,-0.0851673036813736,0.0300014466047287,-0.11322557181119919,-0.025907279923558235,0.10098282992839813,0.034423213452100754,-0.0053036147728562355,-0.07120359688997269,-0.03287291154265404,0.024266112595796585,0.07234569638967514,0.16205504536628723,0.11753524094820023,-0.1246575266122818,-0.06660893559455872,-0.06378305703401566,-0.02746296115219593,-0.15488602221012115,0.01650012470781803,0.0475810244679451,0.0849960669875145,0.05527418106794357,-0.11407948285341263,0.09898409992456436,0.09231781959533691,0.013221830129623413,-0.03543673828244209,-0.0020076243672519922,-0.11086685210466385,0.050783053040504456,0.04316934570670128,0.09644480049610138,0.03762371838092804,0.10911348462104797,0.17150819301605225,-0.04600023105740547,0.06596244126558304,-0.048313744366168976,-0.0837697833776474,-0.02415449731051922,0.10664603114128113,0.0584566630423069,0.031715311110019684,-0.006574271246790886,-0.07478293031454086,0.12991206347942352,0.028875717893242836,0.004367994610220194,0.010722486302256584,-0.028314420953392982,0.0011510560289025307,0.08917876332998276,-0.007745124399662018,0.0144318463280797,0.04805498197674751,0.06663383543491364,-0.03686017170548439,0.1310933232307434,-0.14330607652664185,0.026421017944812775,0.08919377624988556,0.09292545914649963,-0.059482403099536896,-0.10463735461235046,-0.010988404043018818,-0.06196488440036774,0.037645373493433,-0.151127427816391,0.04412924125790596,-0.06370996683835983,-0.047754399478435516,0.04208195209503174,-0.01732255518436432,0.06838371604681015,0.03202816843986511,0.0018161323387175798,0.216002956032753,-0.031601231545209885,0.07007625699043274,0.04830300807952881,0.01636495068669319,0.055961109697818756,-0.017920807003974915,0.021615296602249146,0.064971424639225,0.07545283436775208,0.04678697511553764,-0.027722522616386414,-0.061807867139577866,0.019428472965955734,0.046211156994104385,-0.020746329799294472,0.0809420570731163,0.14269274473190308,0.07200752198696136,-0.024993030354380608,-0.10661812126636505,0.08113661408424377,0.05986085534095764,0.03524262085556984,0.10043755173683167,0.047028522938489914,-0.09927477687597275,-0.03461913764476776,-0.030919497832655907,0.08278422802686691,0.02291278727352619,0.027670415118336678,-0.024360889568924904,0.07458756119012833,0.02649674192070961,-0.0016894044820219278,0.10327914357185364,0.06844211369752884,0.03627735748887062,-0.11025125533342361,-0.053448621183633804,-0.02145666629076004,-0.0753089189529419,-0.0343175008893013,-0.06552769988775253,0.05522549897432327,0.002318728482350707,-0.010609889402985573,-0.0998350977897644,0.050243277102708817,0.011353611946105957,-0.012288474477827549,-0.013837692327797413,-0.004830078221857548,-0.04924708977341652,-0.08090736716985703,-0.09260474890470505,0.037769388407468796,0.10212650895118713,-0.012790394015610218,0.05703017860651016,-0.03921184688806534,0.016248976811766624,0.0851704478263855,0.017304234206676483,-0.036404214799404144,0.014531071297824383,0.1172696202993393,0.06420064717531204,0.018956825137138367,0.09491021186113358,-0.008518225513398647,0.07721524685621262,-0.013362091034650803,0.03585132211446762,0.02642158605158329,0.0700814425945282,-0.0092935711145401,-0.015307474881410599,0.04969174042344093,0.11263573914766312,0.039453327655792236,-0.06777818500995636,-0.0023092685732990503,-0.015459819696843624,0.06081349775195122,0.029211007058620453,-0.00600449601188302,-0.09270373731851578,0.08868581056594849,0.04299425333738327,0.07995343208312988,0.018724704161286354,0.046601224690675735,-0.009069795720279217,-0.09867161512374878,0.047225382179021835,-0.0583532489836216,-0.05540076643228531,0.07739360630512238,-0.16900193691253662,-0.05275372415781021,0.019075514748692513,-0.11463265866041183,0.00901625957340002,-0.024655116721987724,0.008383901789784431,0.018929561600089073,-0.056113552302122116,0.06355943530797958,-0.076370470225811,0.16682152450084686,0.013992320746183395,-0.05527128279209137,-0.03795900568366051,0.10864869505167007,0.01950356736779213,-0.0005733381258323789,0.042017992585897446,-0.03535161912441254,0.03329363465309143,-0.05813128873705864,0.08373023569583893,-0.09576930105686188,-0.10406481474637985,0.024845773354172707,0.02380049228668213,-0.0596979595720768,0.0911654606461525,0.05508217588067055,0.037188950926065445,0.022123003378510475,-0.039475202560424805,-0.03902420029044151,-0.01634102128446102,0.0022817214485257864,-0.06519848108291626,0.005550747271627188,-0.00839893240481615,0.023309487849473953,-0.1587381809949875,0.0381363146007061,0.07211242616176605,-0.01572507806122303,0.12223901599645615,0.08188141882419586,0.00024685569223947823,0.062053196132183075,-0.05753328278660774,0.053332697600126266,0.033940695226192474,0.06317745894193649,-0.07728232443332672,-0.028243061155080795,-0.11050325632095337,0.04243392124772072,0.07861614972352982,-0.009937104769051075,0.008027073927223682,0.16937004029750824,-0.007339630275964737,-0.016787294298410416,-0.05013367161154747,-0.05916823074221611,0.014423578977584839,0.004444634076207876,0.019887909293174744,0.004582008346915245,-0.036568496376276016,0.0268391240388155,0.03156602382659912,-0.11653696000576019,-0.1337907463312149,-0.0008984736632555723,-0.029280949383974075,-0.008768144994974136,-0.0676318034529686,-0.033338505774736404,-0.011704973876476288,0.10869104415178299,-0.022535573691129684,0.038630787283182144,0.006919119507074356,0.008436337113380432,0.012197860516607761,-0.05489741638302803,0.049126043915748596,0.018659746274352074,0.009924477897584438,-0.016889061778783798,-0.05755807086825371,0.003006832906976342,0.008291373960673809,0.0006014558020979166,-0.0031243625562638044,-0.04013857617974281,-0.07278116047382355,-0.0012803381541743875,0.010820594616234303,-0.09304093569517136,-0.08806103467941284,0.054139137268066406,-0.04678712040185928,-0.0169676523655653,-0.020019060000777245,0.012316984124481678,-0.044233519583940506,0.009083135984838009,0.07627307623624802,-0.035851430147886276,-0.1109507754445076,0.043156400322914124,0.006654043681919575,-0.0107383718714118,-0.04029006138443947,0.08506478369235992,0.016610795632004738,0.05868940427899361,-0.0652308538556099,0.022491950541734695,0.02148459479212761,-0.029393529519438744,-0.028712494298815727,-0.031155461445450783,-0.18934772908687592,0.01235823892056942,0.06041078269481659,-0.02198949083685875,-0.009329828433692455,0.05808860436081886,-0.13404203951358795,0.17360340058803558,-0.01646699570119381,-0.07065236568450928,0.07038845121860504,0.12904532253742218,0.041302621364593506,-0.04571688175201416,-0.0507986843585968,0.02713470533490181,0.00024042901350185275,-0.07289392501115799,-0.173280268907547,-0.012273513711988926,-0.00864748191088438,2.656757033037138e-06,0.043289415538311005,-0.024090642109513283,0.0017870290903374553,-0.03925500065088272,0.054327331483364105,0.1151726245880127,0.023097563534975052,-0.051789868623018265,0.15106192231178284,-0.030481405556201935,0.026679040864109993,0.04141698777675629,0.010807262733578682,-0.04285106435418129,-0.014705794863402843,-0.03929495811462402,-0.05199310556054115,0.01834983564913273,-0.058536749333143234,0.023227881640195847,-0.08309505134820938,-0.10792476683855057,-0.05341117084026337,0.035116132348775864,0.0022264623548835516,0.08126378059387207,0.03069831058382988,0.030172426253557205,0.04204818233847618,0.02930234931409359,-0.0930466577410698,0.06639686226844788,-0.018496999517083168,0.08475134521722794,-0.01929425448179245,-0.051339395344257355,0.05374976992607117,0.002849797485396266,-0.008028198033571243,0.010407807305455208,-0.03177130967378616,0.02919982373714447,-0.013802082277834415,0.027710871770977974,0.056956905871629715,-0.012158484198153019,0.022965513169765472,-0.04569977521896362,-0.0018706381088122725,0.05244802311062813,-0.06944038718938828,0.08282686769962311,0.0690106526017189,-0.09358042478561401,0.03690073639154434,-0.051036346703767776,-0.024121437221765518,0.05299682915210724,-0.00013491703430190682,0.03453942388296127,-0.078817218542099,-0.07619442790746689,0.012467815540730953,-0.03179115802049637,0.08149696886539459,0.05786837264895439,0.040229879319667816,-0.05403446406126022,-0.0023391209542751312,0.025436297059059143,-0.013956079259514809,-0.011486289091408253,0.05205686017870903,-0.06828557699918747,0.014056453481316566,-0.1169816181063652,0.010323499329388142,-0.03739900887012482,0.009072607383131981,-0.09056683629751205,-0.03878598287701607,-0.042915087193250656,-0.03442775830626488,0.14041437208652496,0.014492739923298359,-0.004738732241094112,0.015083023346960545,-0.0037841489538550377,-0.0010978634236380458,0.02147994190454483,-0.025649424642324448,0.06461654603481293,0.06833963096141815,0.005871088244020939,-0.02190684713423252,-0.020281100645661354,-0.10575291514396667,-0.11811915040016174,-0.1671329289674759,0.01538166869431734,0.02747979387640953,0.043450288474559784,-0.006893376354128122,-0.05967040732502937,0.1197802945971489,0.05405227467417717,0.012866324745118618,0.052978698164224625,0.12069012969732285,-0.027420829981565475,-0.005902363918721676,0.0969490259885788,-0.15116731822490692,-0.003022753167897463,-0.11818261444568634,-0.11718310415744781,-0.013192635960876942,-0.06574401259422302,0.0013949251733720303,-0.12662306427955627,0.0005047793383710086,0.01959069073200226,-0.11121194809675217,0.008199199102818966,0.10634399950504303,-0.07740482687950134,0.060842495411634445,0.008250905200839043,0.08051864057779312,0.04211447387933731,0.010593104176223278,0.0035717503633350134,0.03613891452550888,0.015585850924253464,0.12832605838775635,-0.08357854187488556,0.06496415287256241,-0.010301136411726475,0.027523482218384743,-0.013234904035925865,-0.0642690509557724,0.05067148059606552,-0.061520110815763474,-0.08556877076625824,0.06725437939167023,0.021803559735417366,0.06800884753465652,-0.007631339598447084,-0.007474226411432028],"yaxis":"y","type":"scattergl"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"x-component"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"y-component"}},"legend":{"tracegroupgap":0,"itemsizing":"constant"},"margin":{"t":60}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('1fab01ce-27d3-48b8-a5a3-68414ca80170');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                            </script>        </div>


</div>
</div>
<p>Some interesting patters: At the farthest left in our model, we see words such as Trump’s, Obama’s, and Hilary. It’s quite self-explanatory why these go together. In the region [-5, -3] x [-0.15, 0], we see multiple places, like Australia, Germanys, and Kenya, next to this rectangle, we see other tangential words like Australian, and kremlin. Overall, the model seems to classify things based off a clear pattern. It’s still kind of hard to see how it makes these decision, but you can get a good idea of it by visualizing the embeddings.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>