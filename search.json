[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "Introduction and Getting Started\nToday, we’re going to use python to construct an interesting data visualization of the Palmer Penguins dataset. You can read the data into python by running the following code:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nAdditionally, we’re going to want to import both seaborn and matplotlib to create our plots. To do this, run the following code.\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nAnalyzing the Dataset\nNow let’s take a look at the dataset. To get an idea of what it looks like, let’s inspect the first five rows of our penguins DataFrame, using the .head() function as seen below.\n\npenguins.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nAs we can see, the data holds recorded measurements of Culmen length and depth, flipper length, and body mass, with indentifiers such as the study name, sample number, species, region, island, stage, individual id, clutch completion, egg date, and sex. In all, there 17 columns for each entry.\n\n\nVisualizing the Correlations in the Dataset\nLet’s try and find a correlation between measurements. Take flipper length and body mass. An interesting hypothesis would be that as flipper length increases, so does body mass. Let’s visualize this using a scatterplot. We’ll use seaborn and matplotlib along with the “penguins” dataframe we made earlier. The code is as follows:\n\nsns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nWe take data from our penguins DataFrame, specifically from the “Flipper Length (mm)” and “Body Mass (g)” columns. We use seaborn as a shortcut to using matplotlib functions, like scatterplot as above. Pass penguins, “Flipper Length (mm),” and “Body Mass (g)” as arguments to sns.scatterplot. Choose a preferred title, and plot using plt.show. As a further modification, we can add a line of best fit by using sns.regplot, which autofits a line of regression. It’s produced as follows:\n\nsns.regplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", scatter_kws={\"color\":\"blue\"}, line_kws={\"color\":\"red\"})\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nHere, scatter_kws and line_kws are both dictionaries that allow us to change the colors of the datapoints and line of best fit. Body mass and flipper length seem to be directly proportional. This makes sense, since the larger the flipper, the heavier it is, thus increasing body mass."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Scraping Inception Actor Data from the Internet",
    "section": "",
    "text": "Introduction\nLet’s practice web scraping. We’re going to scrape information from the credits page of my favorite movie, Inception, from its page on the movie database website. Here’s a link to my project repository: https://github.com/Nkannan12/TMDB-scraper-scrapy. To start your own project, run the following command in your terminal:\nscrapy startproject TMDB_scraper\nThis will create a directory with a bunch of files, but the only ones you really need to worry about are settings.py and a new one I’m asking you to create now insdie the spiders subdirectory: tmdb_spider.py. This is our actual spider, and it’s where we’re going to do pretty much all of our work. Copy the following code into that file to start off the process:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis begins the definition of the TMDBSpider class, and creates a constructor that will allow us to enter a subdirectory into the terminal to scrape any movie’s page on TMDB!\n\n\nParsing Method #1\nWe define parsing methods to help the spider figure out what exactly to scrape. The first one we are going to look out is called parse(self, response). This method assumes we start out at the front page of a movie on TMDB. It is defined as follows:\ndef parse(self, response):\n        \"\"\"Yields a scrapy.Request to the full credits url, parsed using the parse_full_credits function.\n\n        Args:\n            response: Page to be parsed\n        \"\"\"\n        full_credits_url = f\"{self.start_urls[0]}cast\" #gets the url of the cast page\n        yield scrapy.Request(full_credits_url, callback=self.parse_full_credits)\nThis allows us to navigate to the full cast page of the movie. First, we find the actual url of the page. To do this we use the instance variable start_urls, which is a list, so we take its 0-index element (since there’s only one element in the list, the url itself). Then, we use the fact that every cast page on TMDB is the original movie url followed by /cast. Using python f-strings, we can write the full_credits_url. This is then yielded by a scrapy.Request, with the callback being our next parsing function, which is described in the next section.\n\n\nParsing Method #2\nOur next parsing method is parse_full_credits(self, response). This method seeks to pick out all actors credited in the movie. It is defined by the following code:\ndef parse_full_credits(self, response):\n    \"\"\"Yields a scrapy.Request to each actor's url, parsing using the parse_actor_page method\n\n    Args:\n        response: page to be scraped\n    \"\"\"\n    credits = response.css('ol.people.credits')[0] #all the actors are contained in the first ol.people.credits\n    actor_links = credits.css('div.info a::attr(href)').getall() #gets the links to actor pages\n    for actor in actor_links:\n        actor_url = response.urljoin(actor) #gets the full actor url\n        yield scrapy.Request(actor_url, self.parse_actor_page)\nTo write this, we must use the inspect element on the cast page to look for patterns in how the webpage is organized in html. You can do this by right-clicking on the page, going to developer tools, and clicking inspect. I noticed that the all credits are part of the ol.people.credits element. To get acting credits, we take the first element described. We use a css selector to do this, seen in the second line of code. We want to go further and retrieve the actual links to these actors’ pages. This is done in the second line, using inspect again to find a pattern. Links are contained in div.info a::attr(href). We want all links, so we use getall(). Finally, we use a for loop to get the full link for each actor, using urljoin(). For each actor, we yield a scrapy.Request with the actor’s url and the callback being our final parsing function, which is described in the next section.\n\n\nParsing Method #3 (This is the most important one!)\nOur final parsing method is parse_actor_page(self, response), which scrapes all the acting credits a specific actor has ever gotten. It is defined by the following code:\ndef parse_actor_page(self, response):\n    \"\"\"Yields a dictionary with two key-value pairs for each movie the actor has an acting credit for\n\n    Args:\n        response: Page to be scraped\n    \"\"\"\n    actor_name = response.css('div.title a::text').get() #gets the actor name\n    acting_index = 0 #index of which table.card.credits to return from div.credits\n    jobs = response.css('div.credits_list h3::text').getall() #acting, writing, producing, etc.\n    for i in range(len(jobs)):\n        if jobs[i] == 'Acting':\n            acting_index = i\n            break\n    acting_cred = response.css('div.credits_list table.card.credits')[acting_index]\n    movie_or_TV_names = acting_cred.css('bdi::text').getall() #movie name\n    for movie_or_TV_name in movie_or_TV_names:\n        yield {\n            \"actor\" : actor_name,\n            \"movie_or_TV_name\" : movie_or_TV_name\n        }\nAgain, we must use inspect. First we have to find the actor name. I noticed that this can be found in the div.title section. Using a::text allows us to get just the text that is the actor’s name (note in the previous method we used a::attr(href) to find the LINK). Next, we have to find the actor’s acting credits. To do this we must filter out all non-acting credits, like writing, prodcution, crew, etc. while gathering up everything else. To do this, we must scrape information from the corresponding header. These pages all have headers h3.zero, h3.one, h3.two and so on, that identify if a table of credits is for acting or not. These are all in the div.credits_list element, so I made a list of the headers. I can inspect the indices of this list to see which card correspond to which job (acting, writing, etc.), which is done by the for loop. Then, I can find acting credits using div.credits_list table.card.credits with the proper index for acting. Furthermore, we can find each movie name through bdi::text, since the names are in between bdi elemets. Finally, we yield a dictionary with two key-value pairs: the name of the actor, and the name of a movie, with one dictionary for each movie they are credited for.\n\n\nVisualizing our Scraped Data in Practice\nNow we can run the following terminal command to make a csv file from the data we scrape. Let’s look at the page for Inception:\nscrapy crawl tmdb_spider -o movies.csv -a subdir=27205-inception\nThis outputs a csv called movies.csv, and we can create a pandas DataFrame from this. This will help us visualize the scraped data further. Let’s go ahead and do that.\n\nimport pandas as pd\nimport seaborn as sns\n\nThe two things we’re going to need are pandas and seaborn to plot. We do this in the following code:\n\ndf = pd.read_csv(\"movies.csv\")\nmovie_actor_counts = df.groupby('movie_or_TV_name')['actor'].nunique().reset_index()\nmovie_actor_counts.columns = ['movie_or_TV_name', 'num_shared_actors']\nfiltered_movies = movie_actor_counts[movie_actor_counts['num_shared_actors']&gt;=4]\nfiltered_movies.drop(index=620, inplace=True)\n\n/tmp/ipykernel_35668/2527797669.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_movies.drop(index=620, inplace=True)\n\n\nThrough this, we have created a DataFrame with two column: movie_or_TV_name and num_shared_actors. I utlized .groupby() to create a new DataFrame from df that had a column that records the number of actors in df that are in each specific movie. Then I filtered this so that we only consider movies with at least 4 of the actors involved. Finally, I dropped the row for Inception, since it is an outlier in the data. This results in the following DataFrame:\n\nfiltered_movies\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nnum_shared_actors\n\n\n\n\n160\nBatman Begins\n4\n\n\n214\nBones\n4\n\n\n409\nDunkirk\n4\n\n\n430\nEnding the Knight\n4\n\n\n1257\nThe Dark Knight Rises\n7\n\n\n1280\nThe Fire Rises: The Creation and Impact of The...\n4\n\n\n1393\nThe Oscars\n4\n\n\n\n\n\n\n\nWe can further visualize this using a bargraph through seaborn, shown in the following code:\n\nsns.barplot(data=filtered_movies, x='num_shared_actors', y='movie_or_TV_name')\n\n\n\n\n\n\n\n\nThese are the movies with a significant number of actors crossing over from Inception. As we can see, The Dark Knight Rises truly stands out. This makes sense since it is a Christopher Nolan movie, the same director as Inception. In fact many of these are Christopher Nolan movies. So if you like Inception, watch some other Nolan movies, such as the ones in the bar plot!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Scraping Inception Actor Data from the Internet\n\n\n\n\n\n\nWeb Scraping\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 0\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\nNo matching items"
  }
]