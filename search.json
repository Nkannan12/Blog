[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Creating a Simple Message Bank Webapp",
    "section": "",
    "text": "Introduction\nToday we’re going to be making a simple webapp using the very useful python library Flask. To do this, we’ll need to do some work with html, python, and css. For reference, here’s the link to my github repository hosting the work involved in this project: https://github.com/Nkannan12/Message-Bank-Web-App.\n\n\nOur First Method\nNow let’s get into the meat of things by looking at how I implemented a flask application in python. This is seen in the app.py file on github. It starts with the following code, containing import statements and a line to get flask started:\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\napp = Flask(__name__)\nWe’re giong to use all of these in a bit. Firs up is sqlite3. Look at the first python function in app.py, reproduced below:\ndef get_message_db():\n    try:\n        return g.message_db #check to see if the database exists\n    except:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\") #create database if doesn't exist\n        cmd = \\\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT, \n            message TEXT\n        )\n        \"\"\"\n        with g.message_db:\n            cursor = g.message_db.cursor() \n            cursor.execute(cmd) #creates a table called messages with columns handle and message\n        return g.message_db\nThis function will be used to make (and refer to in the future) a database of the messages submitted through our webapp. It starts by checking whether or not such a database exists (since we don’t want to make a new database for each message, do we), hence the try-except statement. If the database doesn’t exist, it proceeds to the body of the except statement, which creates a new database in the first line. We would like to also record the name of people submitting through the website, so in the following command, we create a table in the database called messages that records the name (handle) and message content of a submission. Finally, we must execute this command, which is done in the body of the with statement. In the end, we return the database for future usage.\n\n\nThe Next Method\nThe next method in app.py allows us to actually record the messages.\ndef insert_message(request):\n    handle = request.form[\"name\"] #extract handle\n    message = request.form[\"message\"] #extract message\n\n    db = get_message_db() #work with database using method from aboe\n    cmd = \\\n    \"\"\"\n    INSERT INTO messages (handle, message) VALUES (?, ?)\n    \"\"\"\n    with db:\n        cursor = db.cursor()\n        cursor.execute(cmd, (handle, message)) #execute cmd, inserting message and handl into table (parametrized)\n        db.commit()\nFirst, we must extract the handle and message content from the request (remeber we imported this in the beginning). The exact sytax required here depends on our html template, which you’ll see later. Next, we initialize a database using the method we just wrote. Then we must make another command using SQL, where we insert parametrized values into the table messages in the database (the ?’s are replaced later with a tuple containing the info we actually want). Finally, we go through a with statement and let a cursor execute our command, subsituting relavent info in the form of a tuple (it should always be a tuple that we insert, even if there’s only one element). To make sure our changes are recorded, we use db.commit().\n\n\nA Method for Returning Random Messages\nWe need to make the webapp interactive, right. To do that, proceed to the next method:\ndef random_messages(n):\n    db = get_message_db()\n    cmd = \\\n    \"\"\"\n    SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\n    \"\"\"\n    with db:\n        cursor = db.cursor()\n        cursor.execute(cmd, (n,))\n        selected_messages = cursor.fetchall() #fethces all responses for future display\n        return selected_messages\nThis one is pretty similar in structure to the other two. Like the previous one, we need to initialize our database, if needed. Then we write an SQL command to select random messages from our database, using SQL’s ORDER BY RANDOM(). Finally, we execute another with statement, letting a cursor select a number of random messages (with their corresponding handle) which is returned to us for further use.\n\n\nHTML: A Brief Interlude\nBefore we dive into the final two python methods, we must first go over the idea of an html template. Navigate to the templates folder on github, and look at base.html, reproduced below:\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Message Bank&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Send Your Messages!&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View a Message&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nLooks like a ton of work, but it really isn’t all that much. In this base file, we just organize the layout of everything. As you can see, we’re planning on there being two links on the top of the page you can switch between. We further exted this html template with another one called submit.html, seen below:\n\n{% extends 'base.html' %}\n\n{% block header %}\n    &lt;h1&gt;{% block title %}Submit Your Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n    &lt;form method=\"post\"&gt;\n        &lt;label for=\"name\"&gt;What is your name/handle?&lt;/label&gt;\n        &lt;input name=\"name\" id=\"name\"&gt;\n        &lt;br&gt;\n        &lt;label for=\"message\"&gt;Write a message.&lt;/label&gt;\n        &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n        &lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit Message\"&gt;\n    &lt;/form&gt;\n    {% if thanks %}\n        Thanks for the message!\n    {% endif %}\n{% endblock %}\n\nThis one fills in stuff that was left blank in base.html, specifically block header, block title, and block content. This will be the front page of our site, where the user submits their message. I mentioned it before, but you have to be very careful of the syntax here – I’m letting the input of “name” (what the user types in as their handle) be called “name” with id “name”. Similarly, the name of “message” is “message” with id “message”. Then we create a submit button that says “Submit Message” to actually record info. Finally, we end with a nice thank you message that is triggered later on.\n\n\nThe Final Two Methods\nThe final two methods are used to render our html templates. The first renders the submit.html template (which we went over above):\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html', thanks=False)\n    else:\n        insert_message(request)\n        return render_template('submit.html', thanks=True) #makes sure to post response\nAbove the function defintion, @app.route('/submit', methods=['Post', 'Get']) helps define the url for the page submit.html is displayed on. It’s really important that this is right, or your page won’t load. First, we check if request.method == 'GET' to check if it’s an input or output scenario. If true, we just render the template using render_template(), imported above. Otherwise, we need to call the function inser_message(request) so we can actually record the submission. Finally, we trigger the aforementioned “thank you.” The next method renders the view.html template, which we didn’t go over, but is also in the github, in the templates folder:\n@app.route('/view/')\ndef view():\n    messages = random_messages(5) #caps the number of messages shown at 5\n    return render_template('view.html', messages=messages)\nIt’s very similar to def submit(). This is where we showcase the random messages bouncing around in our database, calling the random messages function (I used n=5 here to keep the number of messages popping reasonable), and returning the render template using the render_template() function. Finally,\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))\nlets us run the whole thing.\n\n\nWhat it Looks Like\nThis is what the submission page looks like. As you can see, it’s just like the submit.html template, with a place to write your name, a place to write your message, and a button to submit.  Here’s what the view message page looks like:  Again, it is all consistent with the template. You can see the message that I put in earlier. I went ahead an typed up four other random ones, and now they’re displayed for you to see."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "Introduction and Getting Started\nToday, we’re going to use python to construct an interesting data visualization of the Palmer Penguins dataset. You can read the data into python by running the following code:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nAdditionally, we’re going to want to import both seaborn and matplotlib to create our plots. To do this, run the following code.\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nAnalyzing the Dataset\nNow let’s take a look at the dataset. To get an idea of what it looks like, let’s inspect the first five rows of our penguins DataFrame, using the .head() function as seen below.\n\npenguins.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nAs we can see, the data holds recorded measurements of Culmen length and depth, flipper length, and body mass, with indentifiers such as the study name, sample number, species, region, island, stage, individual id, clutch completion, egg date, and sex. In all, there 17 columns for each entry.\n\n\nVisualizing the Correlations in the Dataset\nLet’s try and find a correlation between measurements. Take flipper length and body mass. An interesting hypothesis would be that as flipper length increases, so does body mass. Let’s visualize this using a scatterplot. We’ll use seaborn and matplotlib along with the “penguins” dataframe we made earlier. The code is as follows:\n\nsns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nWe take data from our penguins DataFrame, specifically from the “Flipper Length (mm)” and “Body Mass (g)” columns. We use seaborn as a shortcut to using matplotlib functions, like scatterplot as above. Pass penguins, “Flipper Length (mm),” and “Body Mass (g)” as arguments to sns.scatterplot. Choose a preferred title, and plot using plt.show. As a further modification, we can add a line of best fit by using sns.regplot, which autofits a line of regression. It’s produced as follows:\n\nsns.regplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", scatter_kws={\"color\":\"blue\"}, line_kws={\"color\":\"red\"})\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nHere, scatter_kws and line_kws are both dictionaries that allow us to change the colors of the datapoints and line of best fit. Body mass and flipper length seem to be directly proportional. This makes sense, since the larger the flipper, the heavier it is, thus increasing body mass."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Utilizing Machine Learning and Keras to Identify Fake News",
    "section": "",
    "text": "Introduction\nToday, we’re going to sift through loads of news data in an attempt to create a model that can accurately identify “fake news” from a host of articles. To do this, we’re going to use keras and tensorflow to create models for text classification. We’ll use most of the following imports below:\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 19.6 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nCollecting optree (from keras)\n  Downloading optree-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 286.8/286.8 kB 35.6 MB/s eta 0:00:00\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.10.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, optree, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.1.1 which is incompatible.\nSuccessfully installed keras-3.1.1 namex-0.0.7 optree-0.10.0\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport re\nimport string\nimport keras\nfrom keras import utils\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nDataset Prep\nWe’re going to take data from the below url, and create a function that creates a dataset from a dataframe made from the csv.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\n\ndf\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n22444\n10709\nALARMING: NSA Refuses to Release Clinton-Lynch...\nIf Clinton and Lynch just talked about grandki...\n1\n\n\n22445\n8731\nCan Pence's vow not to sling mud survive a Tru...\n() - In 1990, during a close and bitter congre...\n0\n\n\n22446\n4733\nWatch Trump Campaign Try To Spin Their Way Ou...\nA new ad by the Hillary Clinton SuperPac Prior...\n1\n\n\n22447\n3993\nTrump celebrates first 100 days as president, ...\nHARRISBURG, Pa.U.S. President Donald Trump hit...\n0\n\n\n22448\n12896\nTRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...\nMELBOURNE, FL is a town with a population of 7...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThe two things we want to do before creating the dataset are converting all letters in the dataframe columns “text” and “title” to lowercase and removing stopwords, such as “the,” “at,” and more. To remove stopwords, we need a database of stopwords to watch out for, which is found when importing the nltk library and downloading stopwords.\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nstop = stopwords.words('english')\n\nOur function starts by converting the aforementioned text to lowercase using the .lower() function (after .str() to make sure our values are strings). To remove the stopwords, we use the below lambda function in the .apply() function which only keeps words that aren’t in stop, our list of bad words. Finally, we create a tensorflow dataset out of a tuple of dictionaries named based off the column names in our dataframe. For ease of use, we implement the .batch(100) function so that our dataset unloads data 100 elements at a time, allowing for a streamlined training process with negligible accuracy costs.\n\ndef make_dataset(df):\n  df['title'] = df['title'].str.lower()\n  df['text'] = df['text'].str.lower()\n  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n  dataset = tf.data.Dataset.from_tensor_slices((\n      {'title' : df['title'], 'text' : df['text']}, {'fake' : df['fake']}\n  ))\n  dataset = dataset.shuffle(buffer_size = len(df), reshuffle_each_iteration=False)\n  dataset = dataset.batch(100)\n  return dataset\n\nWe make a dataset, and want to split it into training and validation. A good split would be 80% train, 20% validation. This process is done below.\n\ndataset = make_dataset(df)\n\n\ntrain_val_split = int(0.2*len(dataset))\ntrain_ds = dataset.skip(train_val_split)\nval_ds = dataset.take(train_val_split)\n\nAs you may recall, a baseline machine learning model in theory just guesses the majority class in the set. We can calculate the base_rate by counting the number of fake news citings in the training set, and dividing it by the total number of elements in the training set. It turns out fake news citings are the majority (barely). Thus, our base rate is approximately 52%.\n\nnum_fake = 0\n\nfor _, label in train_ds:\n    num_fake += np.sum(label['fake'])\n\nbase_rate = num_fake/(len(train_ds)*100) # number of fakes over total number of elements in the training set\n\nbase_rate\n\n0.5218888888888888\n\n\nThe next thing to do is prepare a vectorization layer that can be implemented into our models. This is made up of a standardization step, and multiple parameters outlining the total number of words to deal with, the output type, and sequence length. Below, we create a vectorization layer for the “title” aspect of each entry in our dataset. If we wanted to, we could create a vectorization layer for the “title” as well (which is shown in the comments below, it’s pretty much the same thing), but this would be redundant, as we would be vectorizing the same piece of data multiple times over, then embedding it twice as well.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# text_vectorize_layer = TextVectorization(\n    # standardize=standardization,\n    # max_tokens=size_vocabulary,\n    # output_mode='int',\n    # output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_ds.map(lambda x, y: x['title']))\n# text_vectorize_layer.adapt(train_ds.map(lambda x, y: x['text']))\n\nNext, we have to outline the inputs to our function. On one hand, we have the “title” input, everything seen in the title column of our dataset. On the other hand, we have the “text” input. Since they are both long strings, the semantics are the same.\n\n# inputs\ntitle_input = keras.Input(\n    shape = (1,),\n    name = \"title\",\n    dtype = \"string\"\n)\n\ntext_input = keras.Input(\n    shape = (1,),\n    name = \"text\",\n    dtype = \"string\"\n)\n\n\n\nModel Creation – Our First, “Title”-based, Model\nToday, we want to answer the question “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?” We’ll do this by creating three models. Our first model is only going to process features found in the “title” category of our input data. We use the pipeline seen below. The most important parts are the vectorization layer and the embedding layer, as these help us the most by clarifying what is what in the data. We end with an output layer that classifys what each training sample is.\n\n# layers for processing the title\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_title\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_output = layers.Dense(2, name='fake')(title_features)\n\nInstead of using the keras.Sequential API, we use the keras.Functional API to create these models. These consists of inputs which are passed through our features to an output. The hyperparameters and elements such as the optimizer and loss function as the same as always and held constant through our whole experimentation process.\n\nmodel1 = keras.Model(\n    inputs = title_input,\n    outputs = title_output\n)\nmodel1.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel1.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_title (Embedding)          │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_2           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │               8 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,008 (23.47 KB)\n\n\n\n Trainable params: 6,008 (23.47 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAs you can see, when just processing the title, we achieve an accuracy of around 90% at peak performance. This is pretty good! But I think we can do better.\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model1.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.5231 - loss: 0.6922 - val_accuracy: 0.5213 - val_loss: 0.6916\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5290 - loss: 0.6912 - val_accuracy: 0.5213 - val_loss: 0.6907\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5279 - loss: 0.6901 - val_accuracy: 0.5213 - val_loss: 0.6888\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5338 - loss: 0.6878 - val_accuracy: 0.5213 - val_loss: 0.6854\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5507 - loss: 0.6840 - val_accuracy: 0.6767 - val_loss: 0.6789\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5946 - loss: 0.6771 - val_accuracy: 0.6547 - val_loss: 0.6721\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6220 - loss: 0.6671 - val_accuracy: 0.6318 - val_loss: 0.6621\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6705 - loss: 0.6550 - val_accuracy: 0.7722 - val_loss: 0.6436\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6984 - loss: 0.6407 - val_accuracy: 0.7564 - val_loss: 0.6271\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7024 - loss: 0.6261 - val_accuracy: 0.7518 - val_loss: 0.6114\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.7110 - loss: 0.6103 - val_accuracy: 0.7778 - val_loss: 0.5949\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.7284 - loss: 0.5952 - val_accuracy: 0.7720 - val_loss: 0.5783\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7345 - loss: 0.5810 - val_accuracy: 0.7876 - val_loss: 0.5630\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7397 - loss: 0.5674 - val_accuracy: 0.7820 - val_loss: 0.5488\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7538 - loss: 0.5521 - val_accuracy: 0.7907 - val_loss: 0.5344\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7574 - loss: 0.5411 - val_accuracy: 0.7878 - val_loss: 0.5234\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7644 - loss: 0.5303 - val_accuracy: 0.7824 - val_loss: 0.5136\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7745 - loss: 0.5179 - val_accuracy: 0.7918 - val_loss: 0.5014\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7776 - loss: 0.5077 - val_accuracy: 0.7878 - val_loss: 0.4936\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7828 - loss: 0.4962 - val_accuracy: 0.7893 - val_loss: 0.4850\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7906 - loss: 0.4838 - val_accuracy: 0.7927 - val_loss: 0.4764\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.7975 - loss: 0.4777 - val_accuracy: 0.8031 - val_loss: 0.4634\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7978 - loss: 0.4693 - val_accuracy: 0.8049 - val_loss: 0.4564\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7994 - loss: 0.4636 - val_accuracy: 0.8011 - val_loss: 0.4527\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8065 - loss: 0.4548 - val_accuracy: 0.8196 - val_loss: 0.4375\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8115 - loss: 0.4460 - val_accuracy: 0.8231 - val_loss: 0.4286\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8164 - loss: 0.4376 - val_accuracy: 0.8267 - val_loss: 0.4217\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8241 - loss: 0.4274 - val_accuracy: 0.8311 - val_loss: 0.4131\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8287 - loss: 0.4173 - val_accuracy: 0.8442 - val_loss: 0.4012\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8283 - loss: 0.4150 - val_accuracy: 0.8362 - val_loss: 0.3993\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8314 - loss: 0.4080 - val_accuracy: 0.8320 - val_loss: 0.3983\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8340 - loss: 0.3995 - val_accuracy: 0.8564 - val_loss: 0.3777\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8420 - loss: 0.3911 - val_accuracy: 0.8567 - val_loss: 0.3721\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8422 - loss: 0.3848 - val_accuracy: 0.8467 - val_loss: 0.3734\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8444 - loss: 0.3812 - val_accuracy: 0.8596 - val_loss: 0.3600\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8523 - loss: 0.3731 - val_accuracy: 0.8487 - val_loss: 0.3653\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8501 - loss: 0.3685 - val_accuracy: 0.8678 - val_loss: 0.3440\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8575 - loss: 0.3570 - val_accuracy: 0.8716 - val_loss: 0.3359\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8596 - loss: 0.3509 - val_accuracy: 0.8673 - val_loss: 0.3387\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8591 - loss: 0.3523 - val_accuracy: 0.8787 - val_loss: 0.3232\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.8606 - loss: 0.3448 - val_accuracy: 0.8787 - val_loss: 0.3194\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8646 - loss: 0.3400 - val_accuracy: 0.8747 - val_loss: 0.3220\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8718 - loss: 0.3323 - val_accuracy: 0.8916 - val_loss: 0.3025\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8728 - loss: 0.3291 - val_accuracy: 0.8924 - val_loss: 0.2978\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8768 - loss: 0.3207 - val_accuracy: 0.8896 - val_loss: 0.2971\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8769 - loss: 0.3183 - val_accuracy: 0.8938 - val_loss: 0.2907\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8751 - loss: 0.3146 - val_accuracy: 0.8816 - val_loss: 0.2992\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8794 - loss: 0.3100 - val_accuracy: 0.9004 - val_loss: 0.2788\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8798 - loss: 0.3059 - val_accuracy: 0.9020 - val_loss: 0.2752\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8830 - loss: 0.2998 - val_accuracy: 0.8960 - val_loss: 0.2780\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our First Model\")\n\nText(0.5, 1.0, 'Visualizing Our First Model')\n\n\n\n\n\n\n\n\n\nBefore we jump into the next model, here’s a streamlined look at the current one. It’s really not that complex, but it more than gets the job done.\n\nfrom keras import utils\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nA Model That Only Looks at Text Data\nNow we’ll tackle the second idea posed in the question – how good is a model that only processes the text of an article. I hypothesize that this will be a little bit better than just looking at the title. The body of the article is a lot longer, and gives us a better idea of what’s going on. We use the same model structure for this model, only changing the input and embedding layer.\n\n# layers for processing the text\ntext_features = title_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_text\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_output = layers.Dense(2, name='fake')(text_features)\n\n\nmodel2 = keras.Model(\n    inputs = text_input,\n    outputs = text_output\n)\nmodel2.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel2.summary()\n\nModel: \"functional_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_text (Embedding)           │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_3           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_8 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │               8 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,008 (23.47 KB)\n\n\n\n Trainable params: 6,008 (23.47 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAt its peak, this model reaches almost 97% accuracy, which is sizably better than our original one. So my hypothesis was correct! Still, I think we can do even better.\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model2.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.5324 - loss: 0.6859 - val_accuracy: 0.5964 - val_loss: 0.6583\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.6767 - loss: 0.6444 - val_accuracy: 0.8749 - val_loss: 0.5896\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8646 - loss: 0.5723 - val_accuracy: 0.8696 - val_loss: 0.5095\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.8918 - loss: 0.4958 - val_accuracy: 0.8829 - val_loss: 0.4389\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9108 - loss: 0.4303 - val_accuracy: 0.8991 - val_loss: 0.3827\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9168 - loss: 0.3804 - val_accuracy: 0.9000 - val_loss: 0.3433\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9187 - loss: 0.3443 - val_accuracy: 0.9133 - val_loss: 0.3099\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9314 - loss: 0.3146 - val_accuracy: 0.9229 - val_loss: 0.2842\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.9319 - loss: 0.2906 - val_accuracy: 0.9260 - val_loss: 0.2644\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9330 - loss: 0.2736 - val_accuracy: 0.9293 - val_loss: 0.2482\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9361 - loss: 0.2564 - val_accuracy: 0.9296 - val_loss: 0.2352\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9389 - loss: 0.2437 - val_accuracy: 0.9316 - val_loss: 0.2235\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9418 - loss: 0.2326 - val_accuracy: 0.9324 - val_loss: 0.2144\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9438 - loss: 0.2249 - val_accuracy: 0.9520 - val_loss: 0.2037\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9442 - loss: 0.2139 - val_accuracy: 0.9531 - val_loss: 0.1980\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9461 - loss: 0.2072 - val_accuracy: 0.9544 - val_loss: 0.1909\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9461 - loss: 0.2016 - val_accuracy: 0.9556 - val_loss: 0.1840\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9479 - loss: 0.1952 - val_accuracy: 0.9564 - val_loss: 0.1799\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.9486 - loss: 0.1920 - val_accuracy: 0.9560 - val_loss: 0.1752\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9519 - loss: 0.1877 - val_accuracy: 0.9571 - val_loss: 0.1704\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9516 - loss: 0.1807 - val_accuracy: 0.9576 - val_loss: 0.1671\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9521 - loss: 0.1745 - val_accuracy: 0.9576 - val_loss: 0.1627\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9528 - loss: 0.1721 - val_accuracy: 0.9571 - val_loss: 0.1591\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9538 - loss: 0.1678 - val_accuracy: 0.9589 - val_loss: 0.1562\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9536 - loss: 0.1649 - val_accuracy: 0.9593 - val_loss: 0.1535\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9539 - loss: 0.1618 - val_accuracy: 0.9602 - val_loss: 0.1529\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9562 - loss: 0.1602 - val_accuracy: 0.9602 - val_loss: 0.1485\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9541 - loss: 0.1556 - val_accuracy: 0.9607 - val_loss: 0.1467\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9555 - loss: 0.1519 - val_accuracy: 0.9609 - val_loss: 0.1444\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9566 - loss: 0.1537 - val_accuracy: 0.9622 - val_loss: 0.1419\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9586 - loss: 0.1483 - val_accuracy: 0.9618 - val_loss: 0.1402\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9572 - loss: 0.1470 - val_accuracy: 0.9620 - val_loss: 0.1383\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9608 - loss: 0.1431 - val_accuracy: 0.9631 - val_loss: 0.1368\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9593 - loss: 0.1414 - val_accuracy: 0.9642 - val_loss: 0.1356\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9602 - loss: 0.1415 - val_accuracy: 0.9633 - val_loss: 0.1336\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9621 - loss: 0.1365 - val_accuracy: 0.9638 - val_loss: 0.1325\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9596 - loss: 0.1371 - val_accuracy: 0.9642 - val_loss: 0.1316\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9593 - loss: 0.1361 - val_accuracy: 0.9640 - val_loss: 0.1299\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9638 - loss: 0.1308 - val_accuracy: 0.9651 - val_loss: 0.1290\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9602 - loss: 0.1330 - val_accuracy: 0.9653 - val_loss: 0.1279\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9610 - loss: 0.1288 - val_accuracy: 0.9649 - val_loss: 0.1266\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9597 - loss: 0.1282 - val_accuracy: 0.9662 - val_loss: 0.1263\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9639 - loss: 0.1269 - val_accuracy: 0.9656 - val_loss: 0.1247\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9638 - loss: 0.1236 - val_accuracy: 0.9660 - val_loss: 0.1247\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9627 - loss: 0.1230 - val_accuracy: 0.9667 - val_loss: 0.1227\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9622 - loss: 0.1242 - val_accuracy: 0.9667 - val_loss: 0.1216\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9656 - loss: 0.1192 - val_accuracy: 0.9673 - val_loss: 0.1214\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9643 - loss: 0.1178 - val_accuracy: 0.9669 - val_loss: 0.1206\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9668 - loss: 0.1131 - val_accuracy: 0.9678 - val_loss: 0.1207\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9645 - loss: 0.1169 - val_accuracy: 0.9678 - val_loss: 0.1208\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our Second Model\")\n\nText(0.5, 1.0, 'Visualizing Our Second Model')\n\n\n\n\n\n\n\n\n\nAs I said before, the backbone of this model is the same as the model processing the titles.\n\nfrom keras import utils\nutils.plot_model(model2, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nAn All-Encompassing Model\nOur final model should be the best. It takes into account both the title data and text data, and should provide us with the fullest picture of what’s happening in each article. In this model, we concatenate the features of both other models in order to process the title and text data in tandem before finally feeding to a dense layer for classification.\n\n# processing both the title and text together\nmain = layers.concatenate([title_features, text_features], axis = 1)\noutput = layers.Dense(2, name = 'fake')(main)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\nmodel3.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel3.summary()\n\nModel: \"functional_9\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_title           │ (None, 500, 3)         │          6,000 │ text_vectorization[2]… │\n│ (Embedding)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_text            │ (None, 500, 3)         │          6,000 │ text_vectorization[3]… │\n│ (Embedding)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (Dropout)       │ (None, 500, 3)         │              0 │ embedding_title[0][0]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (Dropout)       │ (None, 500, 3)         │              0 │ embedding_text[0][0]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_5[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_7[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 6)              │              0 │ dropout_6[0][0],       │\n│                           │                        │                │ dropout_8[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 2)              │             14 │ concatenate[0][0]      │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 12,014 (46.93 KB)\n\n\n\n Trainable params: 12,014 (46.93 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThis model is consistently above 97%, and almost reaches 98% validation accuracy. It is truly the culmination of all the work we’ve done so far. It is almost scarily accuracte. It could even be better than you or me!\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model3.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.2764 - loss: 0.8166 - val_accuracy: 0.5262 - val_loss: 0.6887\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.6336 - loss: 0.6587 - val_accuracy: 0.9573 - val_loss: 0.5649\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.8558 - loss: 0.5459 - val_accuracy: 0.9656 - val_loss: 0.4711\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9362 - loss: 0.4611 - val_accuracy: 0.9618 - val_loss: 0.3979\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9524 - loss: 0.3926 - val_accuracy: 0.9669 - val_loss: 0.3385\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9576 - loss: 0.3379 - val_accuracy: 0.9691 - val_loss: 0.2927\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9632 - loss: 0.3018 - val_accuracy: 0.9707 - val_loss: 0.2577\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9658 - loss: 0.2657 - val_accuracy: 0.9707 - val_loss: 0.2299\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9649 - loss: 0.2397 - val_accuracy: 0.9700 - val_loss: 0.2079\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9656 - loss: 0.2181 - val_accuracy: 0.9707 - val_loss: 0.1905\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9647 - loss: 0.2020 - val_accuracy: 0.9700 - val_loss: 0.1769\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9659 - loss: 0.1881 - val_accuracy: 0.9716 - val_loss: 0.1639\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9682 - loss: 0.1755 - val_accuracy: 0.9720 - val_loss: 0.1533\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9671 - loss: 0.1640 - val_accuracy: 0.9713 - val_loss: 0.1441\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.9669 - loss: 0.1541 - val_accuracy: 0.9709 - val_loss: 0.1384\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9683 - loss: 0.1457 - val_accuracy: 0.9720 - val_loss: 0.1307\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9706 - loss: 0.1410 - val_accuracy: 0.9729 - val_loss: 0.1243\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9716 - loss: 0.1349 - val_accuracy: 0.9724 - val_loss: 0.1202\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9700 - loss: 0.1287 - val_accuracy: 0.9722 - val_loss: 0.1160\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9720 - loss: 0.1222 - val_accuracy: 0.9736 - val_loss: 0.1107\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9708 - loss: 0.1192 - val_accuracy: 0.9720 - val_loss: 0.1088\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9724 - loss: 0.1138 - val_accuracy: 0.9731 - val_loss: 0.1041\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9727 - loss: 0.1105 - val_accuracy: 0.9731 - val_loss: 0.1010\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9712 - loss: 0.1078 - val_accuracy: 0.9731 - val_loss: 0.0986\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9720 - loss: 0.1054 - val_accuracy: 0.9744 - val_loss: 0.0955\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9737 - loss: 0.1037 - val_accuracy: 0.9740 - val_loss: 0.0944\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9741 - loss: 0.0979 - val_accuracy: 0.9744 - val_loss: 0.0942\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9719 - loss: 0.0966 - val_accuracy: 0.9751 - val_loss: 0.0913\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9754 - loss: 0.0926 - val_accuracy: 0.9747 - val_loss: 0.0877\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9742 - loss: 0.0886 - val_accuracy: 0.9756 - val_loss: 0.0854\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9741 - loss: 0.0910 - val_accuracy: 0.9753 - val_loss: 0.0855\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9750 - loss: 0.0869 - val_accuracy: 0.9756 - val_loss: 0.0834\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9751 - loss: 0.0863 - val_accuracy: 0.9771 - val_loss: 0.0817\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9757 - loss: 0.0829 - val_accuracy: 0.9769 - val_loss: 0.0798\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9753 - loss: 0.0808 - val_accuracy: 0.9769 - val_loss: 0.0808\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9765 - loss: 0.0803 - val_accuracy: 0.9764 - val_loss: 0.0788\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9764 - loss: 0.0800 - val_accuracy: 0.9780 - val_loss: 0.0760\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9776 - loss: 0.0775 - val_accuracy: 0.9767 - val_loss: 0.0753\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9762 - loss: 0.0774 - val_accuracy: 0.9767 - val_loss: 0.0780\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9763 - loss: 0.0761 - val_accuracy: 0.9778 - val_loss: 0.0751\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9773 - loss: 0.0756 - val_accuracy: 0.9771 - val_loss: 0.0748\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9782 - loss: 0.0717 - val_accuracy: 0.9771 - val_loss: 0.0748\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9773 - loss: 0.0728 - val_accuracy: 0.9780 - val_loss: 0.0725\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9790 - loss: 0.0686 - val_accuracy: 0.9787 - val_loss: 0.0705\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9793 - loss: 0.0658 - val_accuracy: 0.9784 - val_loss: 0.0697\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9789 - loss: 0.0674 - val_accuracy: 0.9796 - val_loss: 0.0686\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9796 - loss: 0.0665 - val_accuracy: 0.9789 - val_loss: 0.0686\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9787 - loss: 0.0662 - val_accuracy: 0.9787 - val_loss: 0.0684\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9791 - loss: 0.0644 - val_accuracy: 0.9796 - val_loss: 0.0693\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9793 - loss: 0.0653 - val_accuracy: 0.9771 - val_loss: 0.0721\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our Third Model\")\n\nText(0.5, 1.0, 'Visualizing Our Third Model')\n\n\n\n\n\n\n\n\n\nThis model is more complex that both its predecessors because it’s a combination of them. It has about two times as many parameters and two times as many layers, with two separate embeddings for both title and text.\n\nfrom keras import utils\nutils.plot_model(model2, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nTesting\nOf course, we want to see how our model works in a test run. Consider the following dataframe full of test data. Using the function we defined earlier, we can create a test dataset to evaluate our final model on.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\n\n\ntest_ds = make_dataset(test_df)\n\nIt achieves around 97.5% validation accuracy. Pretty neat!\n\n_, accuracy = model3.evaluate(test_ds)\nprint(f'Accuracy on test set: {accuracy}')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9746 - loss: 0.0792\nAccuracy on test set: 0.974341869354248\n\n\n\n\nConsidering the Embeddings\nNow, lets consider how the model actualyl decides how to embed information. Below, we look at the embeddings of the title data.\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nweights = model3.get_layer('embedding_title').get_weights()[0] # get the weights from the embedding layer (only title here)\nvocab = title_vectorize_layer.get_vocabulary() # retreieve all the words so visualization can be interactive\npca = PCA(n_components=2) # reducing to 2 dimensions for easy visualization\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x-component'   : weights[:,0],\n    'y-component'   : weights[:,1]\n})\n\n\nimport plotly.express as px\n# make scatter plot of embeddings\nfig = px.scatter(embedding_df,\n                 x = \"x-component\",\n                 y = \"y-component\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nimage.png\n\n\nSome interesting patters: At the farthest left in our model, we see words such as Trump’s, Obama’s, and Hilary. It’s quite self-explanatory why these go together. In the region [-5, -3] x [-0.15, 0], we see multiple places, like Australia, Germanys, and Kenya, next to this rectangle, we see other tangential words like Australian, and kremlin. Overall, the model seems to classify things based off a clear pattern. It’s still kind of hard to see how it makes these decision, but you can get a good idea of it by visualizing the embeddings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Utilizing Machine Learning and Keras to Identify Fake News\n\n\n\n\n\n\nMachine Learning\n\n\nHW\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Simple Message Bank Webapp\n\n\n\n\n\n\nWeb Development\n\n\nSQL\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 0\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\nNo matching items"
  }
]