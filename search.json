[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/FINAL/index.html#i.-recipes",
    "href": "posts/FINAL/index.html#i.-recipes",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Recipes",
    "text": "I. Recipes\nOne of the first steps for this project is scraping a a dataset of recipes in order to recommend recipes to the user of the web app. We chose to scrape the recipes from AllRecipes.com because it specifically has a cuisines page that lists 49 different cuisines, and each recipe page has a detailed list of ingredients. One downfall of scraping this page that we noticed later on was that some common cuisines are missing from the main page, such as Mexican cuisine. However, we chose to keep the database to only these recipes in order to simplify the recommendation system since the other pages do not order the recipes by cuisine which we wanted to be one of the determining factors for recipe recommendation.\nFurthermore, choosing an appropriate webscraper was also important for this task. We used BeautifulSoup which is a Python library that is specifically designed for parsing HTML and XML documents, making it ideal for web scraping. Additionally, it provides simple methods and code for navigating, searching, and modifying a parse tree. BeautifulSoup also allows for direct scraping within a notebook rather than needing to define separate Python file, like with Proxy. Working within a notebook simplifies debugging and makes it easier to run the code.\nThis is a simple overview of how the web scraper function scrape_allREcipes_cuisines() scrapes the page: - Step 1: Scrape through the main cuisine page and make a dataframe of all of the cuisines and their URLs. - Step 2: Loop through the list and scrape the cuisine URLs for recipe URLs and make a new dataframe with the cuisine type, recipe name, and recipe page URL. - Step 3: Loop through the new list and scrape the recipe pages for the ingredient lists and add them to the list defined in the previous step.\n\n\ndef scrape_allRecipes_cuisines():\n  \"\"\"\n  Scrapes recipe data from Allrecipes.com based on different cuisines.\n\n  Returns:\n  pandas.DataFrame: DataFrame containing the scraped recipe information including Name, URL, Cuisine, and Ingredients.\n  \"\"\"\n\n  # URL of the page listing all cuisines on Allrecipes.com\n  url = 'https://www.allrecipes.com/cuisine-a-z-6740455'\n  result = requests.get(url)\n  doc = BeautifulSoup(result.text, \"html.parser\")\n  cuisines = doc.select('ul.loc.mntl-link-list a')\n\n  # Send a GET request to the page listing all cuisines\n  cuisine_dict = {}\n  for link in cuisines:\n      cuisine = link.get_text(strip=True)\n      url = link['href']\n      cuisine_dict[cuisine] = url\n\n  # Parse the HTML content of the page\n  df = pd.DataFrame(list(cuisine_dict.items()), columns=['Cuisine', 'URL'])\n\n  # Create an empty list to store recipe information\n  recipes_data = []\n\n  # Iterate over rows in the cuisine DataFrame\n  for index, row in df.iterrows():\n      cuisine_url = row['URL']\n      cuisine = df['Cuisine'][index]\n      result = requests.get(cuisine_url)\n      doc = BeautifulSoup(result.text, 'html.parser')\n\n      # {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'}\n      # Extract information for each recipe\n      recipe_info1 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card card--no-image'})\n      recipe_info2 = doc.find_all('a', {'class': 'comp mntl-card-list-items mntl-document-card mntl-card card--image-top card card--no-image'})\n      recipe_info = recipe_info1 + recipe_info2\n\n      # Iterate over each recipe and extract relevant information\n      for recipe_card in recipe_info:\n          name = recipe_card.find('span', {'class': 'card__title-text'}).text.strip()\n          url = recipe_card['href']\n\n          # Extract information from individual recipe URLs\n          if not pd.isna(url):\n              result2 = requests.get(url)\n              doc2 = BeautifulSoup(result2.text, 'html.parser')\n\n              # Create a list to store ingredients\n              ingredients_list = []\n              ingredients_container = doc2.find('div', {'class': 'mntl-lrs-ingredients'})\n\n              # Check if the container is found\n              if ingredients_container:\n                  # Find the list of ingredients\n                  ingredients_list_element = ingredients_container.find('ul', {'class': 'mntl-structured-ingredients__list'})\n\n                  # Check if the list of ingredients is found\n                  if ingredients_list_element:\n                      # Extract and append each ingredient to the list\n                      for ingredient_item in ingredients_list_element.find_all('li', {'class': 'mntl-structured-ingredients__list-item'}):\n                          ingredient = ingredient_item.find('span', {'data-ingredient-name': 'true'})\n                          quantity = ingredient_item.find('span', {'data-ingredient-quantity': 'true'})\n                          unit = ingredient_item.find('span', {'data-ingredient-unit': 'true'})\n\n                          if ingredient and quantity and unit:\n                              ingredient_text = f\"{quantity.text.strip()} {unit.text.strip()} {ingredient.text.strip()}\"\n                              ingredients_list.append(ingredient_text)\n\n              # Append recipe information to the list\n              recipes_data.append({\n                  'Name': name,\n                  'URL': url,\n                  'Cuisine': cuisine,\n                  'Ingredients': ingredients_list,\n              })\n\n  # Create a DataFrame from the list of recipes\n  recipes_df = pd.DataFrame(recipes_data)\n  return recipes_df\n\n\n\nThe function scrape_allRecipes_cuisines() is designed to gather recipe data from Allrecipes.com cuisine page. It operates by first accessing a URL listing various cuisines on the website. It then extracts the names and corresponding URLs of these cuisines. Subsequently, for each cuisine, the function accesses its specific URL to retrieve recipe information. It identifies recipe details such as name and URL by parsing the HTML content. Upon obtaining a recipe’s URL, it accesses the individual recipe page to extract the ingredients list. The function then compiles all gathered data into a structured format, returning a DataFrame containing recipe names, URLs, associated cuisines, and ingredients lists.\nIt moves through the pages as shown in the order bellow:\n\nAll Cuisines Page:\n\n\n\n\n\n\n\n\n\n\n\nSpecific Cuisine Page\n\n\n\n\n\n\n\n\n\n\n\nRecipe Page:\n\n\n\n\n\n\n\n\n\n\nBy running this function we produce a dataset like this:\n\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n['1 large head cabbage, cored and finely shred...\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n['8 large eggs', '2 (15 ounce) cans whole pick...\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n['2 cups uncooked elbow macaroni', '3 large ha...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n['1 (.25 ounce) package active dry yeast', '¼ ...\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n['7 quarts plain popped popcorn', '2 cups dry ...\n\n\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n['1 pound pork belly, cubed', '1 fresh red ch...\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n['4 skinless cooked chicken breasts, shredded...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n['2 ounces rice vermicelli', '8 rice wrappers...\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n['4 pounds beef soup bones (shank and knee)', ...\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n['2 tablespoons canola oil', '2 tablespoons fi...\n\n\n\n\n2321 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nUnfortunately the ingrediants lists are not organized in a good way, so we need to isolate key ingrediants that are common in many recipes. For the machine learning model we are trying to concentrate on produce, so this list will contain more produce than anything else. Additionally, to simplify the machine learning model we will not be including the measurements of the ingrediants because it is a lot harder to train a model to also recognize how much of an ingredient is in an image.\nFirst, we need to remove any punctuation from the string and then seacrch for the key words. To do all of this we will be using the functions remove_punctuation(text) and find_key_ingredients(df, key_ingredients). find_key_ingredients compares the each item from the key_ingredients list to the string to find if each word is in the string. For some of the ingredients there can be errors, such as ‘corn starch’ gets recognized as only ‘corn,’ so some if statements are implemented to try and prevent this mistake.\n\n\ndef remove_punctuation(text):\n  '''\n  - Removes the punctuation from the string of recipe ingredients\n  - Input: string of ingredients\n  - Output: the cleaned string\n  '''\n  translator = str.maketrans('', '', string.punctuation)\n  return text.translate(translator)\n\ndef find_key_ingredients(df, key_ingredients):\n  '''\n  - Compares the lists of ingredients and creates a new column in the dataframe\n    with a list of key ingredients for each recipe\n  - Input: dataframe of recipes, list of key ingredients\n  - Output: dataframe with new column of key ingredients\n  '''\n\n  # Function to check if any key ingredient is found in a list of ingredients\n  def find_word_in_string(text):\n    '''\n    - Finds key ingredients in ingredient string\n    - Input: text from ingredient column of df\n    - Output: List of key ingredients found in ingredient column\n    '''\n    R = []\n    for i in key_ingredients:\n      if i in text:\n        # a lot of recipes use corn starch and corn flour\n        if i == 'corn' and ('corn starch' in text or 'corn flour' in text):\n          break;\n        R.append(i)\n        # Sometimes only need one egg\n        if i == 'eggs' and 'egg' in text and 'eggs' not in R:\n          R.append(i)\n    return R\n\n  # Remove punctuation\n  df['Ingredients'] = df['Ingredients'].apply(remove_punctuation)\n\n  # Find key ingredients in each row\n  df['key_ingredients'] = df['Ingredients'].apply(find_word_in_string)\n  return df\n\n\n\nAfter running these function on the recipe dataframe we get a new dataframe with a column for key ingredients:\n\ncleaned_df\n\n\n  \n    \n\n\n\n\n\n\nName\nURL\nCuisine\nIngredients\nkey_ingredients\n\n\n\n\n0\nBest Vinegar Coleslaw\nhttps://www.allrecipes.com/recipe/59318/amish-...\nAmish and Mennonite\n1 large head cabbage cored and finely shredded...\n['cabbage', 'celery', 'onion', 'sugar']\n\n\n1\nPennsylvania Dutch Pickled Beets and Eggs\nhttps://www.allrecipes.com/recipe/13743/pennsy...\nAmish and Mennonite\n8 large eggs 2 15 ounce cans whole pickled bee...\n['onion', 'beet', 'eggs', 'sugar']\n\n\n2\nAmish Macaroni Salad\nhttps://www.allrecipes.com/recipe/74915/amish-...\nAmish and Mennonite\n2 cups uncooked elbow macaroni 3 large hardcoo...\n['celery', 'onion', 'bell pepper', 'eggs', 'su...\n\n\n3\nAmish Friendship Bread Starter\nhttps://www.allrecipes.com/recipe/7063/amish-f...\nAmish and Mennonite\n1 25 ounce package active dry yeast ¼ cup warm...\n['milk', 'flour', 'sugar']\n\n\n4\nMy Amish Friend's Caramel Corn\nhttps://www.allrecipes.com/recipe/74950/my-ami...\nAmish and Mennonite\n7 quarts plain popped popcorn 2 cups dry roast...\n['corn', 'sugar']\n\n\n...\n...\n...\n...\n...\n...\n\n\n2316\nVietnamese Grilled Pork Skewers\nhttps://www.allrecipes.com/recipe/261122/vietn...\nVietnamese\n1 pound pork belly cubed 1 fresh red chile pe...\n['garlic', 'pork', 'sugar']\n\n\n2317\nGoi Ga (Vietnamese Chicken and Cabbage Salad)\nhttps://www.allrecipes.com/recipe/271155/goi-g...\nVietnamese\n4 skinless cooked chicken breasts shredded 1 ...\n['cabbage', 'cilantro', 'onion', 'carrot', 'ch...\n\n\n2318\nVietnamese Fresh Spring Rolls\nhttps://www.allrecipes.com/recipe/24239/vietna...\nVietnamese\n2 ounces rice vermicelli 8 rice wrappers 85 i...\n['cilantro', 'lettuce', 'rice']\n\n\n2319\nPho (Vietnamese Noodle Soup)\nhttps://www.allrecipes.com/recipe/228443/authe...\nVietnamese\n4 pounds beef soup bones shank and knee 1 medi...\n['cilantro', 'garlic', 'onion', 'rice', 'beef']\n\n\n2320\nVietnamese Grilled Lemongrass Chicken\nhttps://www.allrecipes.com/recipe/241607/vietn...\nVietnamese\n2 tablespoons canola oil 2 tablespoons finely ...\n['garlic', 'chicken', 'sugar']\n\n\n\n\n2321 rows × 5 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThis is our final dataset that we will be using to compare with the results of the model and to recommend recipes. The functions used in this section can be found in recipe_manip.py."
  },
  {
    "objectID": "posts/FINAL/index.html#ii.-images-for-ingredients",
    "href": "posts/FINAL/index.html#ii.-images-for-ingredients",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II. Images for Ingredients",
    "text": "II. Images for Ingredients\nThe web scraping model I created to gather images of ingredients from Google Images uses both BeautifulSoup and Selenium, which are used for parsing HTML and automating web interactions. By using Selenium, and web driver like geckodriver for FireFox of chromedriver for Google Chrome, the script is able to load pages by scrolling to the end, ensuring that all images are loaded before the HTML is parsed by BeautifulSoup. Additionally, Selenium allows you to interact with elements on the page, which I used to clicking on each thumbnail to access the URLs of the corresponding images.\nThe main function initializes a web driver object and prompts the user for inputs including a list of keywords, the desired number of images to download for each keyword, and the name of the file to save the downloaded images. It also creates a new file if the specified one does not already exist in the directory. Subsequently, for each keyword provided by the user, the script generates a Google Images URL and calls the scrape_images function to extract the URLs of the images before proceeding to download them using the download_image function.\nThe scrape_images function is responsible for extracting image URLs from the provided Google Images URL. It utilizes Selenium to scroll through the webpage, ensuring all images are loaded, and then parses the HTML using BeautifulSoup. Thumbnails of images are identified and clicked to access the full-sized image URLs. These URLs are collected and returned as a set.\nThe download_image function handles the downloading and saving of images. It makes a request to the provided image URL and retrieves the image content. The content is then processed, ensuring it is in a usable format. The image is then saved as a JPEG file in the specified download path with the provided file name. This ensures that the images are successfully downloaded and stored in files designated to the ingredient classes for classification with the model later on.\nTo be able to run the code in the terminal or in a notebook, a web driver exicutable file must be stored within the same directory as the py file. This is also where the ingredient files will be downloaded.\n\n\ndef main():\n    wd = webdriver.Chrome()\n    data = input('Enter your search keyword: ')\n    data_list = data.split(\", \")\n    \n    num_images = int(input('Enter the number of images you want: '))\n    file_name = input('Enter file location: ')\n    \n    if not os.path.exists(file_name):\n        os.mkdir(file_name)\n    \n    count = 0\n    for i in data_list:\n        search_url = Google_Image + 'q=' + i #'q=' because its a query\n        print(search_url)\n        urls = scrape_images(search_url, wd, 1, num_images)\n        print(f\"Found {len(urls)}\")\n    \n        for j, url in enumerate(urls):\n            download_image(file_name + \"/\", url, str(count) + \".jpg\")\n            count += 1\n        \n        print(\"Success!\")\n            \n    print(\"Complete!\")\n    wd.quit()\n\ndef scrape_images(initial_url, wd, delay, max_images):\n    url = initial_url\n    wd.get(url)\n    \n    # scroll height of webpage\n    last_height = wd.execute_script(\"return document.body.scrollHeight\") \n    while True:\n        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")    # The driver scrolls down the webpage here.\n        time.sleep(8)\n        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n        if new_height == last_height:   # Breaks here if the driver reached the bottom of the webpage (when it cannot scroll down anymore).\n            break\n        last_height = new_height        # If the driver failed to scroll to the bottom of the webpage, the current scroll height is recorded. Following, it is compared to the scroll height in the next iteration of the loop to decide if the driver reached the bottom of the webpage or not.\n    wd.execute_script(\"window.scrollTo(0, 0);\")\n    \n    page_html = wd.page_source  # The HTML of the webpage is obtained by the driver.\n    pageSoup = bs4.BeautifulSoup(page_html, 'html.parser')  # Using Beautiful Soup to parse the HTML of the webpage.\n    thumbnails = wd.find_elements(By.CLASS_NAME, \"Q4LuWd\")  # This class name is obtained from the thumbnail of each image in Google Image Search.\n    time.sleep(3)\n    \n    len_thumbnails = len(thumbnails)    # The number of images found is recorded and printed.\n    print(\"Found %s image candidates\"%(len_thumbnails))\n    \n    image_urls = set()\n        \n    for img in thumbnails[len(image_urls): max_images]: # Loops through the images of the webpage to obtain and store the number of image URLs requested.\n        try: \n            img.click()\n            time.sleep(0.5)\n\n        except:\n            continue\n\n        images = wd.find_elements(By.CLASS_NAME, \"iPVvYb\") # This class name is obtained from the actual image and not its thumbnail in Google Image Search.\n        for image in images:\n            if image.get_attribute('src') in image_urls: # Prevents an image URL from being stored if it is already there.\n                max_images += 1 # Accounts for a duplicate image URL by ensuring that the function still returns the number of image URLs requested.\n                break\n\n            if image.get_attribute('src') and 'http' in image.get_attribute('src'): # Checks if an image URL is usable.\n                    image_urls.add(image.get_attribute('src')) # Stores an image URL if it is usable.\n                    #print(f\"Found {len(image_urls)}\")\n        \n    return image_urls # Returns all of the usable image URLs.\n\ndef download_image(download_path, url, file_name):\n\n    try:\n        image_content = requests.get(url).content\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to download image from {url} - {e}\")\n        return  # Return without attempting to process the image further\n\n    try:\n        image_file = io.BytesIO(image_content)\n        image = Image.open(image_file)\n    except (IOError, OSError) as e:\n        print(f\"Failed to open image file - {e}\")\n        return  # Return without attempting to process the image further\n\n    # Convert image to RGB mode\n    image = image.convert('RGB')\n\n    file_path = os.path.join(download_path, file_name)\n    try:\n        with open(file_path, \"wb\") as f:\n            image.save(f, \"JPEG\")\n        print(f\"Downloaded: {file_name}\")\n    except Exception as e:\n        print(f\"Failed to save image - {e}\")\n\n\n\nUnfortunately, despite Selenium allowing us to scroll to the bottom of the page, the Google Images page only loads around 400 images for a given search word. Therefore, to get a decent amount of images for training we need to input multiple key words for the same ingredient.\nFor example, for the ingredient onion, there are many different types of onions so we can make a list of these and input them when prompted: yellow onion, white onion, sweet onion, red onion, shallots, onion raw. Additionally, for certain produce or ingredients often when you look them up they come up cooked, to prevent this with many of the broader ingredients you should add ‘raw’ to the key frase.\n\n\n\n\n\n\n\n\n\nAlternatively, name is defined in the py file so you can run this in the terminal by calling “python image_scraper.py”\nBelow is what shows up during each step of the web scraping of the image URLs: 1. Scrolling to the end of the page and parsing HTML:\n\n\n\n\n\n\n\n\n\n\nClicking on each thumbnail and parsing HTML for image URLs:\n\n\n\n\n\n\n\n\n\n\nIn total, we were able to web scrape around 1000 images for 20 ingredients: Tomatoes, Potatoes, Zucchini, Broccoli, Atrichoke, Beans, Onion, Asparagus, Green Onion, Cabbage, Lentils, Garlic, Bell Peppers, Chili Peppers, Corn, Carrots, Mushrooms, Cherries, Apples, Rice.\nWe were not able to scrape more than this due to a few different issues. In order to fully load each page we had to allow time for it to pause between clicking and scrolling, so it took a long time to download a good amount of images for each ingredient. Additionally, the webscraper would often stop scraping randomly, most likely due to connection issues with the web driver and sometimes it would find zero items on the page."
  },
  {
    "objectID": "posts/FINAL/index.html#i.-implementation-of-the-model",
    "href": "posts/FINAL/index.html#i.-implementation-of-the-model",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Implementation of the Model",
    "text": "I. Implementation of the Model\nThe machine learning aspect of our project entailed the implementation of a CNN in pytorch. After researching many different models, and weighing the pros and cons of each architecture, we settled on using a Resnet to carry out image classification. While we had the option to utilize a pretrained model taken from the torchvision database, we felt that it would be better to implement and train our own rendition of the network. This allowed for minute adjustments to layer functions, such as changing the filter size in a convolution, or removing negligible operations to counteract overfitting. The code itself is quite complex. Let’s walk through the actual Resnet class:\nclass ResNet(nn.Module):\n    def __init__(self, block, num_block, num_classes=100,num_channel=3):\n        \"\"\"Initializes the ResNet model.\n\n        Args:\n            block: Basic block or bottleneck block.\n            num_block (list): List containing the number of blocks for each layer.\n            num_classes (int): Number of output classes.\n            num_channel (int): Number of input channels, default is 3.\n        \"\"\"\n        super().__init__()\n        self.in_channels = DIM\n        self.conv1 = nn.Sequential(\n          nn.Conv2d(num_channel, DIM, kernel_size=3, padding=1, bias=False),\n          nn.BatchNorm2d(DIM),\n          nn.ReLU(inplace=True))\n        self.conv2_x = self._make_layer(block, DIM, num_block[0], 1)\n        self.conv3_x = self._make_layer(block, DIM*2, num_block[1], 2)\n        self.conv4_x = self._make_layer(block, DIM*4, num_block[2], 2)\n        self.conv5_x = self._make_layer(block, DIM*8, num_block[3], 2)\n        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(DIM*8 * block.expansion, num_classes)\n        self.sigmoid = nn.Sigmoid()\nThis is just a snippet of the code that goes into creating a Resnet. The model starts out with a simple 2D convolution followed by a batch normalization and ReLU activation. Here, num_channels refers to the number of color channels in the input image (three for all our data) and DIM the number of output channels after the convolution is processed (DIM is variable, but we find a value of 64 works best). Then, we proceed to the meat of the architecture, utilizing a complex function called _make_layer() (definition not pictured) that is used to create a model “layer” consisting of a variable number of convolutions, batch normalizations, max pools, and activations, all depending on the type and number of “blocks” specified. Currently, we don’t need to worry about the explicit definition of a block – it’s quite involved, so it suffices to understand that they’re used as the framework of a layer in the model. After this constructor, model operations are put into motion using a feed-forward function:\ndef forward(self, x):\n  \"\"\"Forward pass of the ResNet model.\n\n    Args:\n      x (torch.Tensor): Input tensor.\n\n    Returns:\n      torch.Tensor: Output tensor.\n  \"\"\"\n  output = self.conv1(x)\n  output = self.conv2_x(output)\n  output = self.conv3_x(output)\n  output = self.conv4_x(output)\n  output = self.conv5_x(output)\n  output = self.avg_pool(output)\n  output = output.view(output.size(0), -1)\n  output = self.fc(output)\n\n  return self.sigmoid(output)\nData is fed through each layer (as outlined above) and its class probability distribution is returned using the sigmoid activation function. We use a Resnet18:\ndef resnet18(num_classes,num_channel):\n  \"\"\" returns a ResNet 18 object\n  \"\"\"\n  return ResNet(BasicBlock, [2, 2, 2, 2],num_classes,num_channel)\nas our main model, but by adjusting the block type and number, we can also possibly return a Resnet34, or Resnet50, all the way up to a Resnet152 (here, the number associated with a Resnet can be thought of as the number of layers in the model). This concludes the construction of our model – now we’re ready to create our dataset!"
  },
  {
    "objectID": "posts/FINAL/index.html#ii-dataset-creation",
    "href": "posts/FINAL/index.html#ii-dataset-creation",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II: Dataset Creation",
    "text": "II: Dataset Creation\nIn order to train, we must first create a dataset from which our model can learn from. Depending on how you store your training images, you can write a funciton to retrieve data in the form of numpy arrays. Our data was stored in a directory containing subdirectories of a given class to partition the set of different tyes of ingredients. To create numpy arrays of the images and their corresponding labels, we utilized the following:\ndef load_images_and_labels(parent_folder, target_size=(150,150)):\n  \"\"\"Returns image data in form of a numpy array\n    \n  args:\n    parent_folder: folder containing classes of images to move through sequentially\n    target_size: size of resulting images in image array\n  \"\"\"\n  images = []\n  labels = []\n  class_mapping = {\"beans\" : 0, \"bell_pepper\" : 1, \"potato\" : 2, \"tomato\" : 3}\n\n  for class_folder in os.listdir(parent_folder):\n    class_path = os.path.join(parent_folder, class_folder)\n    if os.path.isdir(class_path) and class_folder in class_mapping:\n      label = class_mapping[class_folder]\n      for image_file in os.listdir(class_path):\n        image_path = os.path.join(class_path, image_file)\n        try:\n          image = PIL.Image.open(image_path)\n          if image is not None:\n            if image.mode != 'RGB':\n              image = image.convert('RGB')\n            image = image.resize(target_size)\n            images.append(image)\n            labels.append(label)\n            except Exception as e:\n              print(f'Error reading image {image_path}: {e}')\n\n  return np.array(images), np.array(labels)\nThis function iterates through each file and adds the images to python using PIL, then appends each image to a list called images, as well as its label (corresponding to the stated mapping) to a list called labels. At the end, a numpy array version of both these lists is returned. Using these, we can then create a pytorch dataset: ```python class Ingredients(Dataset): “““Class creating pytorch dataset from images and their labels.\nArgs:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nAttributes:\n    images (list): List of images.\n    labels (list): List of corresponding labels.\n    transform (callable, optional): Augmentation of the images.\n\nMethods:\n    __len__(self): Returns the number of samples in the dataset.\n    __getitem__(self, idx): Retrieves the item at the given index.\n\n\"\"\"\ndef __init__(self, images, labels, transform=None):\n    \"\"\"Initializes the Ingredients dataset.\n\n    Args:\n        images (list): List of images.\n        labels (list): List of corresponding labels.\n        transform (callable, optional): Augmentation of the images.\n    \"\"\"\n    self.images = images\n    self.labels = labels\n    self.transform = transform\n\ndef __len__(self):\n    \"\"\"Returns the number of samples in the dataset.\n\n    Returns:\n        int: Number of samples in the dataset.\n    \"\"\"\n    return len(self.images)\n\ndef __getitem__(self, idx):\n    \"\"\"Retrieves the item at the given index.\n\n    Args:\n        idx (int): Index of the item to retrieve.\n\n    Returns:\n        tuple: A tuple containing the image and its corresponding label.\n    \"\"\"\n    image = self.images[idx]\n    image = PIL.Image.fromarray(image)\n    label = self.labels[idx]\n    if self.transform:\n        image = self.transform(image)\n    return image, label\nEvery pytorch dataset is a class inheriting from the dataset class (which you'll have to import through ```torchvision```) and needs to define the following three functions: a constructor (```__init__```), ```__len__()```, and ```__getitem__()```. The constructor records the data, ```__len__``` returns the length of the dataset, and ```__getitem__``` returns an image-label combo at an index of the set. Once we've created our dataset, there are multiple steps we would like to take to process our data. For one, we'd like to change the pixel values of our images to live in the range [-1,1] from the standard [0,255], done through the following:\n```python\ndef preprocess(images_arr):\n  \"\"\"Preprocess an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images to be preprocessed.\n\n    Returns:\n        numpy.ndarray: Preprocessed array of images, pixel values between -1 and 1.\n  \"\"\"\n  images_arr = images_arr.astype(np.float32)\n  images_arr = images_arr/255.0\n  images_arr = (images_arr * 2.0) - 1.0\n\n  return images_arr\nIn this function, we convert each image to a 32-bit float and then dilate the channels to fit within the desired interval. Additionally, we have written a function to find the mean and standard deviation across each channel:\ndef normalization_vals(images_arr):\n  \"\"\"Calculate mean and standard deviation values for normalizing an array of images.\n\n    Args:\n        images_arr (numpy.ndarray): Array of images for which to calculate normalization values.\n\n    Returns:\n        tuple: A tuple containing the mean and standard deviation values for each channel.\n  \"\"\"\n  means = np.mean(images_arr, axis=(0,1,2))\n  stds = np.std(images_arr, axis=(0,1,2))\n  means = tuple(means)\n  stds = tuple(stds)\n  return means, stds\nThese values will be used to normalize the pixel values of our training set. With all this, we are now ready to begin training."
  },
  {
    "objectID": "posts/FINAL/index.html#iii.-training-the-model",
    "href": "posts/FINAL/index.html#iii.-training-the-model",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "III. Training the Model",
    "text": "III. Training the Model\nThis part is relatively simple. The whole process is contained in one file, main.py, which imports useful functions as defined in other files. Here is main:\nimport torch\nimport torch.optim as optim\n# from torchinfo import summary\n\nfrom logger import Logger\nfrom train import run_epoch\nfrom resnet import resnet14, resnet18, resnet34, resnet50\nfrom data_loader import get_data\nfrom test import run_test\n\ntorch.manual_seed(0)\n\nEPOCHS = 200\nEARLY_STOP = 5\nLR = 0.1\nMOMENTUM = 0.9\nWEIGHT_DECAY = 0.0005 # 0.0005 test\nVAL_SPLIT = 0.1\n\nmodel_save_path = f'./model_logs/Ingredients1'\n\nlog = Logger()\nlog.set_logger(f'{model_save_path}.log')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntrain_loader, val_loader, test_loader, num_classes, num_channels = get_data(VAL_SPLIT)\nmodel = resnet18(num_classes, num_channels)\noptimizer = optim.SGD(model.parameters(), LR, MOMENTUM, weight_decay=WEIGHT_DECAY)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n\nrun_epoch(model, train_loader, val_loader, optimizer, scheduler, EPOCHS, EARLY_STOP, f'{model_save_path}.pth', device, log)\naccuracy = run_test(model, test_loader, device, f\"{model_save_path}.pth\")\nlog.logger.info(\"Accuracy: {:.10f}\".format(accuracy))\nOf note are get_data(), run_epoch(), and run_test(). The former is defined in another file, data_loader.py, and is as follows:\ndef get_data(val_split=0.5):\n    \"\"\"Prepares DataLoader instances for training, validation, and testing splits of an image dataset.\n    \n    Parameters:\n    - val_split (float, optional): Proportion of the training set to use for validation. Default is 0.5.\n\n    Returns:\n    - Tuple[DataLoader, DataLoader, DataLoader, int, int]: A tuple containing DataLoader instances for the\n      training, validation, and test datasets, the number of unique labels in the dataset, and the number 3,\n      whose specific meaning may depend on context (e.g., number of color channels).\n    \"\"\"\n    parent_folder = \"/content/drive/MyDrive/proj_files/ingredients/ingredients\"\n    images, labels = load_images_and_labels(parent_folder)\n    preprocess(images)\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomResizedCrop(32),\n        transforms.ToTensor(),\n        transforms.Normalize((0.38046584, 0.10854615, -0.13485776), (0.5249659, 0.59474176, 0.6634378))\n    ])\n\n    dataset = Ingredients(images, labels, transform)\n        \n    batch_size = 64 # change to appropriate value for dataset in use\n\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    train_test_split = int(np.floor(0.8 * dataset_size))\n    train_val_split = int(np.floor(val_split * dataset_size))\n\n    np.random.shuffle(indices)\n\n    train_idx, test_idx = indices[train_test_split:], indices[:train_test_split]\n    val_idx = train_idx[:train_val_split]\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_size,\n                              sampler=SubsetRandomSampler(train_idx),\n                              num_workers=4)\n    \n    test_loader = DataLoader(dataset,\n                             batch_size=batch_size,\n                             sampler=SubsetRandomSampler(test_idx),\n                             num_workers=4)\n\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_size,\n                            sampler=SubsetRandomSampler(val_idx),\n                            num_workers=4)\n    \n    return train_loader, val_loader, test_loader, len(np.unique(labels)), 3\nIt begins by identifying the parent directory in which all our data is located (you will have to change this to depending on where your images are located). Then it utilizes the preprocessing functions we defined above. Next, we define a dataset based on the preprocessed image data, and split this dataset into a training set, validation set, and test set. We then create an iterator of sorts called a DataLoader that allows us to effectively load data in batches to train the model. We create three separate data loaders, one for our training set, one for our valiadation set, and one for our test set. These, in addition to the number of classes and color channels in the data, are returned and further used in our main function. Thus, the data pipeline portion of the process has been completed. Now let’s define the actual training algorithm. The function run_epoch() is as follows:\ndef run_epoch(model:torch.nn.Module, train_loader:torch.utils.data.DataLoader, val_loader:torch.utils.data.DataLoader, optimizer, scheduler, epochs:int, early_stop:int, model_save_path:str, device:torch.device, log, loading=False):\n    \"\"\"Run training and validation for the given number of epochs.\n\n    Args:\n        model (torch.nn.Module): The model.\n        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        optimizer: The optimizer to update the model's parameters.\n        scheduler: Learning rate scheduler.\n        epochs (int): Number of epochs for training.\n        early_stop (int): Number of epochs to wait before early stopping if validation loss doesn't improve.\n        model_save_path (str): Path to save the best model.\n        device (torch.device): Device to be used for training (cuda or cpu).\n        log: Logger object for logging.\n        loading (bool): Flag indicating whether to load a pre-trained model. Default is False.\n\n    Returns:\n        None\n    \"\"\"\n    train_losses = []\n    val_losses = []\n    if loading==True:\n        model.load_state_dict(torch.load(model_save_path))\n        log.logger.info(\"-------------Model Loaded------------\")\n        \n    best_loss=0\n    #curr_early_stop = early_stop\n    model.to(device)\n    for epoch in range(epochs):\n        train_loss = train(model, train_loader, optimizer, device)\n        val_loss = val(model, val_loader, device)\n        scheduler.step()\n       \n        log.logger.info((f\"Epoch: {epoch+1} - loss: {train_loss:.10f} - test_loss: {val_loss:.10f}\"))\n\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if epoch == 0:\n            best_loss=val_loss\n        if val_loss&lt;=best_loss:\n            torch.save(model.state_dict(), model_save_path)    \n            best_loss=val_loss\n            log.logger.info(\"-------- Save Best Model! --------\")\nThis function utilizes both our training and validation data loaders. We defined functions for training and validation (train and val, respectively) that implement the classic backpropogation process. Additionally, we include a learning rate scheduler to help boost training results. Logger is used to create files that streamline the access and view of our results. Using validation loss as our metric, we save every model that is an improvement over the current best model. If needed, one could easily implement an early stopping system (in fact, our own early stop code is commented out of the original file on github), but based off our research, which involved looking into the processes outlined in the original Resnet research paper, it seemed best to evaluate the model of a full period of 200 epochs. Lastly, we look at the run_test() function, which is again, relatively simple:\ndef run_test(model, test_loader, device, model_save_path):\n    \"\"\"Run testing on the test data using the trained model.\n\n    Args:\n        model (torch.nn.Module): The trained model.\n        test_loader (torch.utils.data.DataLoader): DataLoader containing the test data.\n        device (torch.device): The device to be used for testing (cuda or cpu).\n        model_save_path (str): Path to the saved model.\n\n    Returns:\n        float: Accuracy achieved by the model on the test data.\n    \"\"\"\n    model.load_state_dict(torch.load(model_save_path))\n    model.to(device)\n    model.eval()\n\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predictions = torch.max(outputs, 1)\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predictions.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_predictions)\n\n    return accuracy\nHere, we use the test_loader to run through the dataset one more time, evaluating it using our final, fully trained model and calculating accuracy using scikit-learn’s accuray algorithm. We trained our model on a small subset of ingredients, which resulted in an accuracy score of around 70%. This score should improve immensley when a large number and variety of images is used as training data, unfortunately, we were unable to achieve this premier model due to a lack of resources. Our dataset creation function is quite itensive. Thus, when trying to load our set, both google colab and jupyter lab would run out of GPU RAM. Given the proper resources, it would be more than possible to produce a fully trained model on an expansive dataset. Still, we produced a more than adequate network using only a small subset. It’s training results are visualized below:\n\n\n\nimage.png\n\n\nAs we proceed through the cycle of epochs, the loss continues to decrease. There is not much overfitting present, thought the values start to stagnate near the end of the training process. Here, we achieved a 68% accuracy, but tuning hyperparameters further should increase this. Additionally, as mentioned before, a larger, more diverse dataset would do wonders for performance. Unfortunately, that’s just not possible with our currently available resources. Now with this, the training process is done. We proceed to put everything together in the form of a webapp made in Flask."
  },
  {
    "objectID": "posts/FINAL/index.html#i.-home-page",
    "href": "posts/FINAL/index.html#i.-home-page",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "I. Home Page",
    "text": "I. Home Page\nWhile creating the home page itself simple involved design we also first had to establish the HTML code to set up the Web APP. To start off we created a a base.html and a main.html. Main pourpose of the base.html is to provides a structured layout with a navigation bar and placeholders for dynamic content. The navigation bar ks able to naviage between the three pages”:\n\nHome page: Links to the main page of the application. The url_for(‘main’) function dynamically generates the URL to the view function named main.\nGenerate a Recipe: Links to the page where users can generate recipes. The url_for(‘generate’) generates the URL to the generate view function.\nRecipe List: Links to a page displaying a list of recipes. The url_for(‘receipe_page’) (note the typo in ‘recipe’) generates the URL to the receipe_page view function.\n\n\n\n\nimage.png\n\n\nBelow the main.html sets up the structure of the mian page. we used img src to present three images and imporve the appreace of our model but most importantly we added a button called “Generate a Receipe”. The style of the button is added at style.css. The picture below depics how our home page looks like. As you can see you can navigate to the generate a recipe page both through the button in the middle or through the navigation bar. Below the image we will dive into how we formulated the generate function.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/FINAL/index.html#ii.-list-of-recipes",
    "href": "posts/FINAL/index.html#ii.-list-of-recipes",
    "title": "Final Project: Recipe Generator Web App - Eat Your Veggies!",
    "section": "II. List of recipes",
    "text": "II. List of recipes\nAnother component of the website is the recipe list page. On this page, participants are simply able to explore the different cuisine and the recipes within them regardless of the ingredients they have.\nThe code from the app.py file below will present how the receipe list page work and how we navigate to the correct url of the recipe\nThe def filter_ recipe takes in the user selected cuisine and filter through the data farme to find the matching receipes. Lets break down the decorator and the function.\nfirst the @app.route(‘/filter-recipes’, methods=[‘POST’]) decorator tells Flask to use the filter_recipes function as the handler for requests to the URL /filter-recipes when the HTTP method is POST.\nWhen the create the function which runs when the POST request is made for filter-recipes. The “cuisine = request.form.get(‘cuisine’)” code extracts the value of cuisine from the form data submitted with the POST request. If cuisine is not present in the form data, None is returned instead. The next line “ingredients = None: Initializes ingredients with None” is added so when we are generating receipes we can also incldue ingredients howevere in this case users only select a cusine.\nThe function then calls the function gen_recipe with cuisine and ingredients as arguments. This function isreturns a collection of recommended recipes based on the provided cuisine (and potentially ingredients, if implemented).\nWe then convert the recommended_recipes into a list of dictionaries. Each dictionary represents a recipe, and the orient=‘records’ parameter means each row in the DataFrame is converted to a dictionary, with column names as keys.\nLastly we render the receipe result page.\n\n@app.route('/filter-recipes', methods=['POST'])\ndef filter_recipes():\n    \"\"\"Filter recipes based on selected cuisine and ingredients.\n\n    Retrieves the selected cuisine and ingredients from the request form, then generates\n    recommended recipes using the 'gen_recipe' function. Converts the recommended recipes\n    into a dictionary format and renders the 'recipe_results.html' template, passing the\n    recipes data.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data.\n    \"\"\"\n    cuisine = request.form.get('cuisine')\n    ingredients = None\n\n    recommended_recipes = gen_recipe(cuisine, ingredients)\n    recipes_data = recommended_recipes.to_dict(orient='records')\n\n    # Render the recipe_results.html template, passing the recipes data\n    return render_template('recipe_results.html', recipes=recipes_data)\n\n\nNow that we are at the recipe results page lets examine the recipe_resulst function:\nWhen a user accesses the /recipe_results/ URL, the application can respond to both GET and POST requests. For GET requests, it’s expected that the user has passed information through the URL’s query parameters, specifying both a type of cuisine and ingredients they’re interested in. The application checks if both pieces of information are provided; if not, it displays a message asking for both the cuisine and ingredients. If the information is present, it uses a function gen_recipe from earlier to fetch recommended recipes matching the specified criteria, converts this data into a format that can be easily used in the HTML template, and then displays it to the user on the recipe_results.html page. For POST requests, which are not explicitly handled with form data in this code, it simply displays a message encouraging the user to select a cuisine to view recipes, suggesting that the main interaction mode expected is through GET requests\n\n@app.route('/recipe_results/', methods=['GET', 'POST'])\ndef recipe_results():\n    \"\"\"Display recipe results based on selected cuisine and ingredients.\n\n    If the request method is GET, retrieves the selected cuisine and ingredients from the request arguments,\n    generates recommended recipes using the 'gen_recipe' function, converts the recommended recipes into a\n    dictionary format, and renders the 'recipe_results.html' template, passing the recipes data. If either\n    cuisine or ingredients is not provided, displays an error message.\n\n    If the request method is not GET, displays a message prompting the user to select a cuisine to view recipes.\n\n    Returns:\n        Response: Rendered template 'recipe_results.html' with recipes data or an error message.\n    \"\"\"\n    if request.method == 'GET':\n        # fetch cuisine and ingredients passed form generate\n        cuisine = request.args.get('cuisine')\n        ingredients = request.args.get('ingredients')\n        if cuisine is None or ingredients is None:\n            message = \"Provide both a cuisine and ingredients.\"\n            return render_template('recipe_results.html', message=message)\n        # print(cuisine + ' ' + ingredients)\n        recommended_recipes = gen_recipe(cuisine, ingredients)\n        # print(recommended_recipes)\n        recipes_data = recommended_recipes.to_dict(orient='records')\n        print(recipes_data)  # Debugging line to check data\n        return render_template('recipe_results.html', recipes=recipes_data)\n    else:\n        message = \"Please select a cuisine to view recipes.\"\n        return render_template('recipe_results.html', message=message)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\nThe code below is from the receipe_results.html, it dynamically display a list of recipes on a webpage. If recipes are available, each one is presented with its name (as a clickable link to the recipe’s page) and its key ingredients. If no recipes are available, a predefined message is displayed instead.\n\n\nLets break down how it works and what each part does:\nThe h1 simply displays the header of the page.\nThe div id=“recipes”&gt;…&lt;/div” encloses the entire section of the code that deals with displaying the recipes or a message in case there are no recipes to display. It is important to note that the id=“recipes” attribute is used to uniquely identify this division in the document, which can be useful for styling or scripting purposes.\nNext up % if recipes % is a Jinja2 conditional statement that checks if there is a recipes variable available and it has a truthy value (e.g., a non-empty list). If recipes is truthy, the block of code inside this condition is executed.\nIf the recipes variable exists and contains items than this lines % for recipe in recipes % , loop iterates over each item in recipes, note each item in the loop is referred to as recipe.\nInside the loop, a div class=“recipe”&gt;…&lt;/div is defined for each recipe. Within this divsion we displays the recipe’s name and links to the recipe’s URL. The recipe.Name and recipe.URL are placeholders that get replaced with the actual name and URL of the recipe, respectively, as provided by the recipes variable. A paragraph (p) with the class “key-ingredients” that displays the key ingredients of the recipe. The recipe.key_ingredients placeholder is replaced with the actual key ingredients data. Lastly , else is executed if recipes is falsy (e.g., an empty list or None). It renders a paragraph (p) displaying a message, which is provided by the message variable. To end the if-else conditional look we use endif\n\n\n\n\n\n\n\n\n\nHere is what the recipe list page looks like. Once users submit preferences they will be redirected to the same page as the recipe page from above. Again when clicking a recipe they will be redirected to the recipe website.\n\n\n\nfilter.png\n\n\nFinally, the recommended recipes are rendered on the recipe result page for the user to explore. This page allows users to explore the recipes from different cuisines without needing to upload any images, in the case that they do not have any ingredients."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "Introduction and Getting Started\nToday, we’re going to use python to construct an interesting data visualization of the Palmer Penguins dataset. You can read the data into python by running the following code:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nAdditionally, we’re going to want to import both seaborn and matplotlib to create our plots. To do this, run the following code.\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nAnalyzing the Dataset\nNow let’s take a look at the dataset. To get an idea of what it looks like, let’s inspect the first five rows of our penguins DataFrame, using the .head() function as seen below.\n\npenguins.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nAs we can see, the data holds recorded measurements of Culmen length and depth, flipper length, and body mass, with indentifiers such as the study name, sample number, species, region, island, stage, individual id, clutch completion, egg date, and sex. In all, there 17 columns for each entry.\n\n\nVisualizing the Correlations in the Dataset\nLet’s try and find a correlation between measurements. Take flipper length and body mass. An interesting hypothesis would be that as flipper length increases, so does body mass. Let’s visualize this using a scatterplot. We’ll use seaborn and matplotlib along with the “penguins” dataframe we made earlier. The code is as follows:\n\nsns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nWe take data from our penguins DataFrame, specifically from the “Flipper Length (mm)” and “Body Mass (g)” columns. We use seaborn as a shortcut to using matplotlib functions, like scatterplot as above. Pass penguins, “Flipper Length (mm),” and “Body Mass (g)” as arguments to sns.scatterplot. Choose a preferred title, and plot using plt.show. As a further modification, we can add a line of best fit by using sns.regplot, which autofits a line of regression. It’s produced as follows:\n\nsns.regplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", scatter_kws={\"color\":\"blue\"}, line_kws={\"color\":\"red\"})\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nHere, scatter_kws and line_kws are both dictionaries that allow us to change the colors of the datapoints and line of best fit. Body mass and flipper length seem to be directly proportional. This makes sense, since the larger the flipper, the heavier it is, thus increasing body mass."
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Creating a Simple Message Bank Webapp",
    "section": "",
    "text": "Introduction\nToday we’re going to be making a simple webapp using the very useful python library Flask. To do this, we’ll need to do some work with html, python, and css. For reference, here’s the link to my github repository hosting the work involved in this project: https://github.com/Nkannan12/Message-Bank-Web-App.\n\n\nOur First Method\nNow let’s get into the meat of things by looking at how I implemented a flask application in python. This is seen in the app.py file on github. It starts with the following code, containing import statements and a line to get flask started:\nfrom flask import Flask, g, render_template, request\nimport sqlite3\n\napp = Flask(__name__)\nWe’re giong to use all of these in a bit. Firs up is sqlite3. Look at the first python function in app.py, reproduced below:\ndef get_message_db():\n    try:\n        return g.message_db #check to see if the database exists\n    except:\n        g.message_db = sqlite3.connect(\"message_db.sqlite\") #create database if doesn't exist\n        cmd = \\\n        \"\"\"\n        CREATE TABLE IF NOT EXISTS messages (\n            handle TEXT, \n            message TEXT\n        )\n        \"\"\"\n        with g.message_db:\n            cursor = g.message_db.cursor() \n            cursor.execute(cmd) #creates a table called messages with columns handle and message\n        return g.message_db\nThis function will be used to make (and refer to in the future) a database of the messages submitted through our webapp. It starts by checking whether or not such a database exists (since we don’t want to make a new database for each message, do we), hence the try-except statement. If the database doesn’t exist, it proceeds to the body of the except statement, which creates a new database in the first line. We would like to also record the name of people submitting through the website, so in the following command, we create a table in the database called messages that records the name (handle) and message content of a submission. Finally, we must execute this command, which is done in the body of the with statement. In the end, we return the database for future usage.\n\n\nThe Next Method\nThe next method in app.py allows us to actually record the messages.\ndef insert_message(request):\n    handle = request.form[\"name\"] #extract handle\n    message = request.form[\"message\"] #extract message\n\n    db = get_message_db() #work with database using method from aboe\n    cmd = \\\n    \"\"\"\n    INSERT INTO messages (handle, message) VALUES (?, ?)\n    \"\"\"\n    with db:\n        cursor = db.cursor()\n        cursor.execute(cmd, (handle, message)) #execute cmd, inserting message and handl into table (parametrized)\n        db.commit()\nFirst, we must extract the handle and message content from the request (remeber we imported this in the beginning). The exact sytax required here depends on our html template, which you’ll see later. Next, we initialize a database using the method we just wrote. Then we must make another command using SQL, where we insert parametrized values into the table messages in the database (the ?’s are replaced later with a tuple containing the info we actually want). Finally, we go through a with statement and let a cursor execute our command, subsituting relavent info in the form of a tuple (it should always be a tuple that we insert, even if there’s only one element). To make sure our changes are recorded, we use db.commit().\n\n\nA Method for Returning Random Messages\nWe need to make the webapp interactive, right. To do that, proceed to the next method:\ndef random_messages(n):\n    db = get_message_db()\n    cmd = \\\n    \"\"\"\n    SELECT * FROM messages ORDER BY RANDOM() LIMIT ?\n    \"\"\"\n    with db:\n        cursor = db.cursor()\n        cursor.execute(cmd, (n,))\n        selected_messages = cursor.fetchall() #fethces all responses for future display\n        return selected_messages\nThis one is pretty similar in structure to the other two. Like the previous one, we need to initialize our database, if needed. Then we write an SQL command to select random messages from our database, using SQL’s ORDER BY RANDOM(). Finally, we execute another with statement, letting a cursor select a number of random messages (with their corresponding handle) which is returned to us for further use.\n\n\nHTML: A Brief Interlude\nBefore we dive into the final two python methods, we must first go over the idea of an html template. Navigate to the templates folder on github, and look at base.html, reproduced below:\n\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} - Message Bank&lt;/title&gt;\n&lt;nav&gt;\n  &lt;h1&gt;Send Your Messages!&lt;/h1&gt;\n  &lt;!-- &lt;b&gt;Navigation:&lt;/b&gt; --&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View a Message&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\n\nLooks like a ton of work, but it really isn’t all that much. In this base file, we just organize the layout of everything. As you can see, we’re planning on there being two links on the top of the page you can switch between. We further exted this html template with another one called submit.html, seen below:\n\n{% extends 'base.html' %}\n\n{% block header %}\n    &lt;h1&gt;{% block title %}Submit Your Message{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n    &lt;form method=\"post\"&gt;\n        &lt;label for=\"name\"&gt;What is your name/handle?&lt;/label&gt;\n        &lt;input name=\"name\" id=\"name\"&gt;\n        &lt;br&gt;\n        &lt;label for=\"message\"&gt;Write a message.&lt;/label&gt;\n        &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n        &lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit Message\"&gt;\n    &lt;/form&gt;\n    {% if thanks %}\n        Thanks for the message!\n    {% endif %}\n{% endblock %}\n\nThis one fills in stuff that was left blank in base.html, specifically block header, block title, and block content. This will be the front page of our site, where the user submits their message. I mentioned it before, but you have to be very careful of the syntax here – I’m letting the input of “name” (what the user types in as their handle) be called “name” with id “name”. Similarly, the name of “message” is “message” with id “message”. Then we create a submit button that says “Submit Message” to actually record info. Finally, we end with a nice thank you message that is triggered later on.\n\n\nThe Final Two Methods\nThe final two methods are used to render our html templates. The first renders the submit.html template (which we went over above):\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html', thanks=False)\n    else:\n        insert_message(request)\n        return render_template('submit.html', thanks=True) #makes sure to post response\nAbove the function defintion, @app.route('/submit', methods=['Post', 'Get']) helps define the url for the page submit.html is displayed on. It’s really important that this is right, or your page won’t load. First, we check if request.method == 'GET' to check if it’s an input or output scenario. If true, we just render the template using render_template(), imported above. Otherwise, we need to call the function inser_message(request) so we can actually record the submission. Finally, we trigger the aforementioned “thank you.” The next method renders the view.html template, which we didn’t go over, but is also in the github, in the templates folder:\n@app.route('/view/')\ndef view():\n    messages = random_messages(5) #caps the number of messages shown at 5\n    return render_template('view.html', messages=messages)\nIt’s very similar to def submit(). This is where we showcase the random messages bouncing around in our database, calling the random messages function (I used n=5 here to keep the number of messages popping reasonable), and returning the render template using the render_template() function. Finally,\nif __name__ == \"__main__\":\n    app.run(debug=True, host=\"0.0.0.0\", port=int(os.environ.get(\"PORT\", 8080)))\nlets us run the whole thing.\n\n\nWhat it Looks Like\nThis is what the submission page looks like. As you can see, it’s just like the submit.html template, with a place to write your name, a place to write your message, and a button to submit.  Here’s what the view message page looks like:  Again, it is all consistent with the template. You can see the message that I put in earlier. I went ahead an typed up four other random ones, and now they’re displayed for you to see."
  },
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Utilizing Machine Learning and Keras to Identify Fake News",
    "section": "",
    "text": "Introduction\nToday, we’re going to sift through loads of news data in an attempt to create a model that can accurately identify “fake news” from a host of articles. To do this, we’re going to use keras and tensorflow to create models for text classification. We’ll use most of the following imports below:\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 19.6 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nCollecting optree (from keras)\n  Downloading optree-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 286.8/286.8 kB 35.6 MB/s eta 0:00:00\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.10/dist-packages (from optree-&gt;keras) (4.10.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, optree, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.1.1 which is incompatible.\nSuccessfully installed keras-3.1.1 namex-0.0.7 optree-0.10.0\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport re\nimport string\nimport keras\nfrom keras import utils\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nDataset Prep\nWe’re going to take data from the below url, and create a function that creates a dataset from a dataframe made from the csv.\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndf = pd.read_csv(train_url)\n\n\ndf\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n22444\n10709\nALARMING: NSA Refuses to Release Clinton-Lynch...\nIf Clinton and Lynch just talked about grandki...\n1\n\n\n22445\n8731\nCan Pence's vow not to sling mud survive a Tru...\n() - In 1990, during a close and bitter congre...\n0\n\n\n22446\n4733\nWatch Trump Campaign Try To Spin Their Way Ou...\nA new ad by the Hillary Clinton SuperPac Prior...\n1\n\n\n22447\n3993\nTrump celebrates first 100 days as president, ...\nHARRISBURG, Pa.U.S. President Donald Trump hit...\n0\n\n\n22448\n12896\nTRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...\nMELBOURNE, FL is a town with a population of 7...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nThe two things we want to do before creating the dataset are converting all letters in the dataframe columns “text” and “title” to lowercase and removing stopwords, such as “the,” “at,” and more. To remove stopwords, we need a database of stopwords to watch out for, which is found when importing the nltk library and downloading stopwords.\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nstop = stopwords.words('english')\n\nOur function starts by converting the aforementioned text to lowercase using the .lower() function (after .str() to make sure our values are strings). To remove the stopwords, we use the below lambda function in the .apply() function which only keeps words that aren’t in stop, our list of bad words. Finally, we create a tensorflow dataset out of a tuple of dictionaries named based off the column names in our dataframe. For ease of use, we implement the .batch(100) function so that our dataset unloads data 100 elements at a time, allowing for a streamlined training process with negligible accuracy costs.\n\ndef make_dataset(df):\n  df['title'] = df['title'].str.lower()\n  df['text'] = df['text'].str.lower()\n  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n  dataset = tf.data.Dataset.from_tensor_slices((\n      {'title' : df['title'], 'text' : df['text']}, {'fake' : df['fake']}\n  ))\n  dataset = dataset.shuffle(buffer_size = len(df), reshuffle_each_iteration=False)\n  dataset = dataset.batch(100)\n  return dataset\n\nWe make a dataset, and want to split it into training and validation. A good split would be 80% train, 20% validation. This process is done below.\n\ndataset = make_dataset(df)\n\n\ntrain_val_split = int(0.2*len(dataset))\ntrain_ds = dataset.skip(train_val_split)\nval_ds = dataset.take(train_val_split)\n\nAs you may recall, a baseline machine learning model in theory just guesses the majority class in the set. We can calculate the base_rate by counting the number of fake news citings in the training set, and dividing it by the total number of elements in the training set. It turns out fake news citings are the majority (barely). Thus, our base rate is approximately 52%.\n\nnum_fake = 0\n\nfor _, label in train_ds:\n    num_fake += np.sum(label['fake'])\n\nbase_rate = num_fake/(len(train_ds)*100) # number of fakes over total number of elements in the training set\n\nbase_rate\n\n0.5218888888888888\n\n\nThe next thing to do is prepare a vectorization layer that can be implemented into our models. This is made up of a standardization step, and multiple parameters outlining the total number of words to deal with, the output type, and sequence length. Below, we create a vectorization layer for the “title” aspect of each entry in our dataset. If we wanted to, we could create a vectorization layer for the “title” as well (which is shown in the comments below, it’s pretty much the same thing), but this would be redundant, as we would be vectorizing the same piece of data multiple times over, then embedding it twice as well.\n\n#preparing a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\n# text_vectorize_layer = TextVectorization(\n    # standardize=standardization,\n    # max_tokens=size_vocabulary,\n    # output_mode='int',\n    # output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train_ds.map(lambda x, y: x['title']))\n# text_vectorize_layer.adapt(train_ds.map(lambda x, y: x['text']))\n\nNext, we have to outline the inputs to our function. On one hand, we have the “title” input, everything seen in the title column of our dataset. On the other hand, we have the “text” input. Since they are both long strings, the semantics are the same.\n\n# inputs\ntitle_input = keras.Input(\n    shape = (1,),\n    name = \"title\",\n    dtype = \"string\"\n)\n\ntext_input = keras.Input(\n    shape = (1,),\n    name = \"text\",\n    dtype = \"string\"\n)\n\n\n\nModel Creation – Our First, “Title”-based, Model\nToday, we want to answer the question “When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?” We’ll do this by creating three models. Our first model is only going to process features found in the “title” category of our input data. We use the pipeline seen below. The most important parts are the vectorization layer and the embedding layer, as these help us the most by clarifying what is what in the data. We end with an output layer that classifys what each training sample is.\n\n# layers for processing the title\ntitle_features = title_vectorize_layer(title_input)\ntitle_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_title\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_output = layers.Dense(2, name='fake')(title_features)\n\nInstead of using the keras.Sequential API, we use the keras.Functional API to create these models. These consists of inputs which are passed through our features to an output. The hyperparameters and elements such as the optimizer and loss function as the same as always and held constant through our whole experimentation process.\n\nmodel1 = keras.Model(\n    inputs = title_input,\n    outputs = title_output\n)\nmodel1.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel1.summary()\n\nModel: \"functional_5\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)                   │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_title (Embedding)          │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_2           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_6 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │               8 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,008 (23.47 KB)\n\n\n\n Trainable params: 6,008 (23.47 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAs you can see, when just processing the title, we achieve an accuracy of around 90% at peak performance. This is pretty good! But I think we can do better.\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model1.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.5231 - loss: 0.6922 - val_accuracy: 0.5213 - val_loss: 0.6916\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5290 - loss: 0.6912 - val_accuracy: 0.5213 - val_loss: 0.6907\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5279 - loss: 0.6901 - val_accuracy: 0.5213 - val_loss: 0.6888\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5338 - loss: 0.6878 - val_accuracy: 0.5213 - val_loss: 0.6854\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5507 - loss: 0.6840 - val_accuracy: 0.6767 - val_loss: 0.6789\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5946 - loss: 0.6771 - val_accuracy: 0.6547 - val_loss: 0.6721\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6220 - loss: 0.6671 - val_accuracy: 0.6318 - val_loss: 0.6621\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6705 - loss: 0.6550 - val_accuracy: 0.7722 - val_loss: 0.6436\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.6984 - loss: 0.6407 - val_accuracy: 0.7564 - val_loss: 0.6271\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7024 - loss: 0.6261 - val_accuracy: 0.7518 - val_loss: 0.6114\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.7110 - loss: 0.6103 - val_accuracy: 0.7778 - val_loss: 0.5949\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.7284 - loss: 0.5952 - val_accuracy: 0.7720 - val_loss: 0.5783\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7345 - loss: 0.5810 - val_accuracy: 0.7876 - val_loss: 0.5630\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7397 - loss: 0.5674 - val_accuracy: 0.7820 - val_loss: 0.5488\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7538 - loss: 0.5521 - val_accuracy: 0.7907 - val_loss: 0.5344\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7574 - loss: 0.5411 - val_accuracy: 0.7878 - val_loss: 0.5234\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7644 - loss: 0.5303 - val_accuracy: 0.7824 - val_loss: 0.5136\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.7745 - loss: 0.5179 - val_accuracy: 0.7918 - val_loss: 0.5014\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7776 - loss: 0.5077 - val_accuracy: 0.7878 - val_loss: 0.4936\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7828 - loss: 0.4962 - val_accuracy: 0.7893 - val_loss: 0.4850\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.7906 - loss: 0.4838 - val_accuracy: 0.7927 - val_loss: 0.4764\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.7975 - loss: 0.4777 - val_accuracy: 0.8031 - val_loss: 0.4634\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7978 - loss: 0.4693 - val_accuracy: 0.8049 - val_loss: 0.4564\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.7994 - loss: 0.4636 - val_accuracy: 0.8011 - val_loss: 0.4527\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8065 - loss: 0.4548 - val_accuracy: 0.8196 - val_loss: 0.4375\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8115 - loss: 0.4460 - val_accuracy: 0.8231 - val_loss: 0.4286\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8164 - loss: 0.4376 - val_accuracy: 0.8267 - val_loss: 0.4217\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8241 - loss: 0.4274 - val_accuracy: 0.8311 - val_loss: 0.4131\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8287 - loss: 0.4173 - val_accuracy: 0.8442 - val_loss: 0.4012\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8283 - loss: 0.4150 - val_accuracy: 0.8362 - val_loss: 0.3993\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8314 - loss: 0.4080 - val_accuracy: 0.8320 - val_loss: 0.3983\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step - accuracy: 0.8340 - loss: 0.3995 - val_accuracy: 0.8564 - val_loss: 0.3777\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8420 - loss: 0.3911 - val_accuracy: 0.8567 - val_loss: 0.3721\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8422 - loss: 0.3848 - val_accuracy: 0.8467 - val_loss: 0.3734\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8444 - loss: 0.3812 - val_accuracy: 0.8596 - val_loss: 0.3600\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8523 - loss: 0.3731 - val_accuracy: 0.8487 - val_loss: 0.3653\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8501 - loss: 0.3685 - val_accuracy: 0.8678 - val_loss: 0.3440\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8575 - loss: 0.3570 - val_accuracy: 0.8716 - val_loss: 0.3359\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8596 - loss: 0.3509 - val_accuracy: 0.8673 - val_loss: 0.3387\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8591 - loss: 0.3523 - val_accuracy: 0.8787 - val_loss: 0.3232\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.8606 - loss: 0.3448 - val_accuracy: 0.8787 - val_loss: 0.3194\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8646 - loss: 0.3400 - val_accuracy: 0.8747 - val_loss: 0.3220\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8718 - loss: 0.3323 - val_accuracy: 0.8916 - val_loss: 0.3025\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8728 - loss: 0.3291 - val_accuracy: 0.8924 - val_loss: 0.2978\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8768 - loss: 0.3207 - val_accuracy: 0.8896 - val_loss: 0.2971\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8769 - loss: 0.3183 - val_accuracy: 0.8938 - val_loss: 0.2907\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8751 - loss: 0.3146 - val_accuracy: 0.8816 - val_loss: 0.2992\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8794 - loss: 0.3100 - val_accuracy: 0.9004 - val_loss: 0.2788\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8798 - loss: 0.3059 - val_accuracy: 0.9020 - val_loss: 0.2752\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.8830 - loss: 0.2998 - val_accuracy: 0.8960 - val_loss: 0.2780\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our First Model\")\n\nText(0.5, 1.0, 'Visualizing Our First Model')\n\n\n\n\n\n\n\n\n\nBefore we jump into the next model, here’s a streamlined look at the current one. It’s really not that complex, but it more than gets the job done.\n\nfrom keras import utils\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nA Model That Only Looks at Text Data\nNow we’ll tackle the second idea posed in the question – how good is a model that only processes the text of an article. I hypothesize that this will be a little bit better than just looking at the title. The body of the article is a lot longer, and gives us a better idea of what’s going on. We use the same model structure for this model, only changing the input and embedding layer.\n\n# layers for processing the text\ntext_features = title_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, 3, name = \"embedding_text\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_output = layers.Dense(2, name='fake')(text_features)\n\n\nmodel2 = keras.Model(\n    inputs = text_input,\n    outputs = text_output\n)\nmodel2.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel2.summary()\n\nModel: \"functional_7\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ text (InputLayer)                    │ (None, 1)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ text_vectorization                   │ (None, 500)                 │               0 │\n│ (TextVectorization)                  │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ embedding_text (Embedding)           │ (None, 500, 3)              │           6,000 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_7 (Dropout)                  │ (None, 500, 3)              │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_average_pooling1d_3           │ (None, 3)                   │               0 │\n│ (GlobalAveragePooling1D)             │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_8 (Dropout)                  │ (None, 3)                   │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ fake (Dense)                         │ (None, 2)                   │               8 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 6,008 (23.47 KB)\n\n\n\n Trainable params: 6,008 (23.47 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAt its peak, this model reaches almost 97% accuracy, which is sizably better than our original one. So my hypothesis was correct! Still, I think we can do even better.\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model2.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.5324 - loss: 0.6859 - val_accuracy: 0.5964 - val_loss: 0.6583\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.6767 - loss: 0.6444 - val_accuracy: 0.8749 - val_loss: 0.5896\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8646 - loss: 0.5723 - val_accuracy: 0.8696 - val_loss: 0.5095\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.8918 - loss: 0.4958 - val_accuracy: 0.8829 - val_loss: 0.4389\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9108 - loss: 0.4303 - val_accuracy: 0.8991 - val_loss: 0.3827\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9168 - loss: 0.3804 - val_accuracy: 0.9000 - val_loss: 0.3433\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9187 - loss: 0.3443 - val_accuracy: 0.9133 - val_loss: 0.3099\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9314 - loss: 0.3146 - val_accuracy: 0.9229 - val_loss: 0.2842\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.9319 - loss: 0.2906 - val_accuracy: 0.9260 - val_loss: 0.2644\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9330 - loss: 0.2736 - val_accuracy: 0.9293 - val_loss: 0.2482\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9361 - loss: 0.2564 - val_accuracy: 0.9296 - val_loss: 0.2352\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9389 - loss: 0.2437 - val_accuracy: 0.9316 - val_loss: 0.2235\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9418 - loss: 0.2326 - val_accuracy: 0.9324 - val_loss: 0.2144\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9438 - loss: 0.2249 - val_accuracy: 0.9520 - val_loss: 0.2037\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9442 - loss: 0.2139 - val_accuracy: 0.9531 - val_loss: 0.1980\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9461 - loss: 0.2072 - val_accuracy: 0.9544 - val_loss: 0.1909\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9461 - loss: 0.2016 - val_accuracy: 0.9556 - val_loss: 0.1840\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9479 - loss: 0.1952 - val_accuracy: 0.9564 - val_loss: 0.1799\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 10ms/step - accuracy: 0.9486 - loss: 0.1920 - val_accuracy: 0.9560 - val_loss: 0.1752\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9519 - loss: 0.1877 - val_accuracy: 0.9571 - val_loss: 0.1704\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9516 - loss: 0.1807 - val_accuracy: 0.9576 - val_loss: 0.1671\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9521 - loss: 0.1745 - val_accuracy: 0.9576 - val_loss: 0.1627\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9528 - loss: 0.1721 - val_accuracy: 0.9571 - val_loss: 0.1591\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9538 - loss: 0.1678 - val_accuracy: 0.9589 - val_loss: 0.1562\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9536 - loss: 0.1649 - val_accuracy: 0.9593 - val_loss: 0.1535\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9539 - loss: 0.1618 - val_accuracy: 0.9602 - val_loss: 0.1529\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9562 - loss: 0.1602 - val_accuracy: 0.9602 - val_loss: 0.1485\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9541 - loss: 0.1556 - val_accuracy: 0.9607 - val_loss: 0.1467\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9555 - loss: 0.1519 - val_accuracy: 0.9609 - val_loss: 0.1444\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9566 - loss: 0.1537 - val_accuracy: 0.9622 - val_loss: 0.1419\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9586 - loss: 0.1483 - val_accuracy: 0.9618 - val_loss: 0.1402\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9572 - loss: 0.1470 - val_accuracy: 0.9620 - val_loss: 0.1383\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 16ms/step - accuracy: 0.9608 - loss: 0.1431 - val_accuracy: 0.9631 - val_loss: 0.1368\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 11ms/step - accuracy: 0.9593 - loss: 0.1414 - val_accuracy: 0.9642 - val_loss: 0.1356\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9602 - loss: 0.1415 - val_accuracy: 0.9633 - val_loss: 0.1336\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9621 - loss: 0.1365 - val_accuracy: 0.9638 - val_loss: 0.1325\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9596 - loss: 0.1371 - val_accuracy: 0.9642 - val_loss: 0.1316\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9593 - loss: 0.1361 - val_accuracy: 0.9640 - val_loss: 0.1299\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9638 - loss: 0.1308 - val_accuracy: 0.9651 - val_loss: 0.1290\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9602 - loss: 0.1330 - val_accuracy: 0.9653 - val_loss: 0.1279\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9610 - loss: 0.1288 - val_accuracy: 0.9649 - val_loss: 0.1266\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9597 - loss: 0.1282 - val_accuracy: 0.9662 - val_loss: 0.1263\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9639 - loss: 0.1269 - val_accuracy: 0.9656 - val_loss: 0.1247\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9638 - loss: 0.1236 - val_accuracy: 0.9660 - val_loss: 0.1247\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9627 - loss: 0.1230 - val_accuracy: 0.9667 - val_loss: 0.1227\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9622 - loss: 0.1242 - val_accuracy: 0.9667 - val_loss: 0.1216\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9656 - loss: 0.1192 - val_accuracy: 0.9673 - val_loss: 0.1214\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9643 - loss: 0.1178 - val_accuracy: 0.9669 - val_loss: 0.1206\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9668 - loss: 0.1131 - val_accuracy: 0.9678 - val_loss: 0.1207\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9645 - loss: 0.1169 - val_accuracy: 0.9678 - val_loss: 0.1208\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our Second Model\")\n\nText(0.5, 1.0, 'Visualizing Our Second Model')\n\n\n\n\n\n\n\n\n\nAs I said before, the backbone of this model is the same as the model processing the titles.\n\nfrom keras import utils\nutils.plot_model(model2, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nAn All-Encompassing Model\nOur final model should be the best. It takes into account both the title data and text data, and should provide us with the fullest picture of what’s happening in each article. In this model, we concatenate the features of both other models in order to process the title and text data in tandem before finally feeding to a dense layer for classification.\n\n# processing both the title and text together\nmain = layers.concatenate([title_features, text_features], axis = 1)\noutput = layers.Dense(2, name = 'fake')(main)\n\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = output\n)\nmodel3.compile(optimizer='adam',\n              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel3.summary()\n\nModel: \"functional_9\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)              ┃ Output Shape           ┃        Param # ┃ Connected to           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ title (InputLayer)        │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text (InputLayer)         │ (None, 1)              │              0 │ -                      │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ text_vectorization        │ (None, 500)            │              0 │ title[0][0],           │\n│ (TextVectorization)       │                        │                │ text[0][0]             │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_title           │ (None, 500, 3)         │          6,000 │ text_vectorization[2]… │\n│ (Embedding)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ embedding_text            │ (None, 500, 3)         │          6,000 │ text_vectorization[3]… │\n│ (Embedding)               │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_5 (Dropout)       │ (None, 500, 3)         │              0 │ embedding_title[0][0]  │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_7 (Dropout)       │ (None, 500, 3)         │              0 │ embedding_text[0][0]   │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_5[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ global_average_pooling1d… │ (None, 3)              │              0 │ dropout_7[0][0]        │\n│ (GlobalAveragePooling1D)  │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_6 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dropout_8 (Dropout)       │ (None, 3)              │              0 │ global_average_poolin… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ concatenate (Concatenate) │ (None, 6)              │              0 │ dropout_6[0][0],       │\n│                           │                        │                │ dropout_8[0][0]        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ fake (Dense)              │ (None, 2)              │             14 │ concatenate[0][0]      │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n\n\n\n Total params: 12,014 (46.93 KB)\n\n\n\n Trainable params: 12,014 (46.93 KB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nThis model is consistently above 97%, and almost reaches 98% validation accuracy. It is truly the culmination of all the work we’ve done so far. It is almost scarily accuracte. It could even be better than you or me!\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\nhistory = model3.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs = 50,\n                    callbacks=[callback],\n                    verbose = True)\n\nEpoch 1/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.2764 - loss: 0.8166 - val_accuracy: 0.5262 - val_loss: 0.6887\nEpoch 2/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.6336 - loss: 0.6587 - val_accuracy: 0.9573 - val_loss: 0.5649\nEpoch 3/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.8558 - loss: 0.5459 - val_accuracy: 0.9656 - val_loss: 0.4711\nEpoch 4/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9362 - loss: 0.4611 - val_accuracy: 0.9618 - val_loss: 0.3979\nEpoch 5/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9524 - loss: 0.3926 - val_accuracy: 0.9669 - val_loss: 0.3385\nEpoch 6/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9576 - loss: 0.3379 - val_accuracy: 0.9691 - val_loss: 0.2927\nEpoch 7/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9632 - loss: 0.3018 - val_accuracy: 0.9707 - val_loss: 0.2577\nEpoch 8/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9658 - loss: 0.2657 - val_accuracy: 0.9707 - val_loss: 0.2299\nEpoch 9/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9649 - loss: 0.2397 - val_accuracy: 0.9700 - val_loss: 0.2079\nEpoch 10/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9656 - loss: 0.2181 - val_accuracy: 0.9707 - val_loss: 0.1905\nEpoch 11/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9647 - loss: 0.2020 - val_accuracy: 0.9700 - val_loss: 0.1769\nEpoch 12/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9659 - loss: 0.1881 - val_accuracy: 0.9716 - val_loss: 0.1639\nEpoch 13/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.9682 - loss: 0.1755 - val_accuracy: 0.9720 - val_loss: 0.1533\nEpoch 14/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9671 - loss: 0.1640 - val_accuracy: 0.9713 - val_loss: 0.1441\nEpoch 15/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 13ms/step - accuracy: 0.9669 - loss: 0.1541 - val_accuracy: 0.9709 - val_loss: 0.1384\nEpoch 16/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9683 - loss: 0.1457 - val_accuracy: 0.9720 - val_loss: 0.1307\nEpoch 17/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9706 - loss: 0.1410 - val_accuracy: 0.9729 - val_loss: 0.1243\nEpoch 18/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 20ms/step - accuracy: 0.9716 - loss: 0.1349 - val_accuracy: 0.9724 - val_loss: 0.1202\nEpoch 19/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9700 - loss: 0.1287 - val_accuracy: 0.9722 - val_loss: 0.1160\nEpoch 20/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9720 - loss: 0.1222 - val_accuracy: 0.9736 - val_loss: 0.1107\nEpoch 21/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9708 - loss: 0.1192 - val_accuracy: 0.9720 - val_loss: 0.1088\nEpoch 22/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9724 - loss: 0.1138 - val_accuracy: 0.9731 - val_loss: 0.1041\nEpoch 23/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9727 - loss: 0.1105 - val_accuracy: 0.9731 - val_loss: 0.1010\nEpoch 24/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9712 - loss: 0.1078 - val_accuracy: 0.9731 - val_loss: 0.0986\nEpoch 25/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9720 - loss: 0.1054 - val_accuracy: 0.9744 - val_loss: 0.0955\nEpoch 26/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9737 - loss: 0.1037 - val_accuracy: 0.9740 - val_loss: 0.0944\nEpoch 27/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 17ms/step - accuracy: 0.9741 - loss: 0.0979 - val_accuracy: 0.9744 - val_loss: 0.0942\nEpoch 28/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9719 - loss: 0.0966 - val_accuracy: 0.9751 - val_loss: 0.0913\nEpoch 29/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9754 - loss: 0.0926 - val_accuracy: 0.9747 - val_loss: 0.0877\nEpoch 30/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9742 - loss: 0.0886 - val_accuracy: 0.9756 - val_loss: 0.0854\nEpoch 31/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9741 - loss: 0.0910 - val_accuracy: 0.9753 - val_loss: 0.0855\nEpoch 32/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 12ms/step - accuracy: 0.9750 - loss: 0.0869 - val_accuracy: 0.9756 - val_loss: 0.0834\nEpoch 33/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.9751 - loss: 0.0863 - val_accuracy: 0.9771 - val_loss: 0.0817\nEpoch 34/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9757 - loss: 0.0829 - val_accuracy: 0.9769 - val_loss: 0.0798\nEpoch 35/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9753 - loss: 0.0808 - val_accuracy: 0.9769 - val_loss: 0.0808\nEpoch 36/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9765 - loss: 0.0803 - val_accuracy: 0.9764 - val_loss: 0.0788\nEpoch 37/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9764 - loss: 0.0800 - val_accuracy: 0.9780 - val_loss: 0.0760\nEpoch 38/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9776 - loss: 0.0775 - val_accuracy: 0.9767 - val_loss: 0.0753\nEpoch 39/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9762 - loss: 0.0774 - val_accuracy: 0.9767 - val_loss: 0.0780\nEpoch 40/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9763 - loss: 0.0761 - val_accuracy: 0.9778 - val_loss: 0.0751\nEpoch 41/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9773 - loss: 0.0756 - val_accuracy: 0.9771 - val_loss: 0.0748\nEpoch 42/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9782 - loss: 0.0717 - val_accuracy: 0.9771 - val_loss: 0.0748\nEpoch 43/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9773 - loss: 0.0728 - val_accuracy: 0.9780 - val_loss: 0.0725\nEpoch 44/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 14ms/step - accuracy: 0.9790 - loss: 0.0686 - val_accuracy: 0.9787 - val_loss: 0.0705\nEpoch 45/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 12ms/step - accuracy: 0.9793 - loss: 0.0658 - val_accuracy: 0.9784 - val_loss: 0.0697\nEpoch 46/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9789 - loss: 0.0674 - val_accuracy: 0.9796 - val_loss: 0.0686\nEpoch 47/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9796 - loss: 0.0665 - val_accuracy: 0.9789 - val_loss: 0.0686\nEpoch 48/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.9787 - loss: 0.0662 - val_accuracy: 0.9787 - val_loss: 0.0684\nEpoch 49/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 18ms/step - accuracy: 0.9791 - loss: 0.0644 - val_accuracy: 0.9796 - val_loss: 0.0693\nEpoch 50/50\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.9793 - loss: 0.0653 - val_accuracy: 0.9771 - val_loss: 0.0721\n\n\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\nplt.title(\"Visualizing Our Third Model\")\n\nText(0.5, 1.0, 'Visualizing Our Third Model')\n\n\n\n\n\n\n\n\n\nThis model is more complex that both its predecessors because it’s a combination of them. It has about two times as many parameters and two times as many layers, with two separate embeddings for both title and text.\n\nfrom keras import utils\nutils.plot_model(model2, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\n\nTesting\nOf course, we want to see how our model works in a test run. Consider the following dataframe full of test data. Using the function we defined earlier, we can create a test dataset to evaluate our final model on.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_df = pd.read_csv(test_url)\n\n\ntest_ds = make_dataset(test_df)\n\nIt achieves around 97.5% validation accuracy. Pretty neat!\n\n_, accuracy = model3.evaluate(test_ds)\nprint(f'Accuracy on test set: {accuracy}')\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.9746 - loss: 0.0792\nAccuracy on test set: 0.974341869354248\n\n\n\n\nConsidering the Embeddings\nNow, lets consider how the model actualyl decides how to embed information. Below, we look at the embeddings of the title data.\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nweights = model3.get_layer('embedding_title').get_weights()[0] # get the weights from the embedding layer (only title here)\nvocab = title_vectorize_layer.get_vocabulary() # retreieve all the words so visualization can be interactive\npca = PCA(n_components=2) # reducing to 2 dimensions for easy visualization\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x-component'   : weights[:,0],\n    'y-component'   : weights[:,1]\n})\n\n\nimport plotly.express as px\n# make scatter plot of embeddings\nfig = px.scatter(embedding_df,\n                 x = \"x-component\",\n                 y = \"y-component\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 5,\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nimage.png\n\n\nSome interesting patters: At the farthest left in our model, we see words such as Trump’s, Obama’s, and Hilary. It’s quite self-explanatory why these go together. In the region [-5, -3] x [-0.15, 0], we see multiple places, like Australia, Germanys, and Kenya, next to this rectangle, we see other tangential words like Australian, and kremlin. Overall, the model seems to classify things based off a clear pattern. It’s still kind of hard to see how it makes these decision, but you can get a good idea of it by visualizing the embeddings."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Utilizing Machine Learning and Keras to Identify Fake News\n\n\n\n\n\n\nMachine Learning\n\n\nHW\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project: Recipe Generator Web App - Eat Your Veggies!\n\n\n\n\n\n\nfinal project\n\n\n\n\n\n\n\n\n\nMar 21, 2024\n\n\nGianna Pedroza, Neomi Goodman, Narayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Simple Message Bank Webapp\n\n\n\n\n\n\nWeb Development\n\n\nSQL\n\n\nHW\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 0\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\nNo matching items"
  }
]