[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/HW0/index.html",
    "href": "posts/HW0/index.html",
    "title": "HW0",
    "section": "",
    "text": "Introduction and Getting Started\nToday, we’re going to use python to construct an interesting data visualization of the Palmer Penguins dataset. You can read the data into python by running the following code:\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nAdditionally, we’re going to want to import both seaborn and matplotlib to create our plots. To do this, run the following code.\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nAnalyzing the Dataset\nNow let’s take a look at the dataset. To get an idea of what it looks like, let’s inspect the first five rows of our penguins DataFrame, using the .head() function as seen below.\n\npenguins.head(5)\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nAs we can see, the data holds recorded measurements of Culmen length and depth, flipper length, and body mass, with indentifiers such as the study name, sample number, species, region, island, stage, individual id, clutch completion, egg date, and sex. In all, there 17 columns for each entry.\n\n\nVisualizing the Correlations in the Dataset\nLet’s try and find a correlation between measurements. Take flipper length and body mass. An interesting hypothesis would be that as flipper length increases, so does body mass. Let’s visualize this using a scatterplot. We’ll use seaborn and matplotlib along with the “penguins” dataframe we made earlier. The code is as follows:\n\nsns.scatterplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\")\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nWe take data from our penguins DataFrame, specifically from the “Flipper Length (mm)” and “Body Mass (g)” columns. We use seaborn as a shortcut to using matplotlib functions, like scatterplot as above. Pass penguins, “Flipper Length (mm),” and “Body Mass (g)” as arguments to sns.scatterplot. Choose a preferred title, and plot using plt.show. As a further modification, we can add a line of best fit by using sns.regplot, which autofits a line of regression. It’s produced as follows:\n\nsns.regplot(data=penguins, x=\"Flipper Length (mm)\", y=\"Body Mass (g)\", scatter_kws={\"color\":\"blue\"}, line_kws={\"color\":\"red\"})\nplt.title(\"Penguin Flipper Length Affect On Body Mass\")\nplt.show\n\n\n\n\n\n\n\n\nHere, scatter_kws and line_kws are both dictionaries that allow us to change the colors of the datapoints and line of best fit. Body mass and flipper length seem to be directly proportional. This makes sense, since the larger the flipper, the heavier it is, thus increasing body mass."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Visualizing and Understanding Trends in Temperature by Country",
    "section": "",
    "text": "Introduction\nLet’s practice working with databases and visualizing trends in data using the NOAA-GHCN dataset. To start, we’re going to want to import a bunch of useful libraries, including (but not limited to) sqlite3, pandas, numpy, and plotly. Plotly express is what we’ll use to easily create some cool looking plots, while plotly.io is something I’ll use on my end to safely render my images.\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\n\nCreating a Database\nThe first thing we’re going to want to do is create a database. This will make it much quicker for us to work with the data, and allow us to modify DataFrames to fit our specific needs. The code to this is written below. We utilize slite3 to “connect” to a database in our directory called climate.db (this creates a new database if climate.db doesn’t already exist). Then we access some csv files that should also be in our directory (these are available to download elsewhere), specifically temps.csv, station-metadata.csv, and fips-10-4-to-iso-country-codes.csv. We use the .read_csv() function to read these into DataFrames, as per usual. Note that we use the chunksize argument for temps.csv – this allows us to create an iterator to more easily load a DataFrame for temps, since this is a really large dataset.\n\nconn = sqlite3.connect(\"climate.db\") \ntemps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\nstations = pd.read_csv(\"station-metadata.csv\")\ncountries = pd.read_csv(\"fips-10-4-to-iso-country-codes.csv\")\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nfor i, df in enumerate(temps_iter):\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\ncountries.to_sql(\"countries\", conn, if_exists= \"replace\", index=False)\n\n279\n\n\nI’m using another function called prepare_df() to clean up the process of making a DataFrame specifically out of temp.csv. In it we use the stack function of to put everything together, and also adjust the “Month” column to just include the month number. Next we iterate through temps.csv, apply prepare_df to the chunks of data we load, then create an sql table out of it all, appending once chunk at a time. We call this table “temperatures.” Creating tables called “stations” and “countries” is easier since we don’t need an iterator. We just use the to_sql() funciton to make our DataFrames into sql tables.\nNext I’m going to use the very useful cursor from sql to look a little bit deeper into our database, as shown below. We use cursor.execute() along with some sql language to look at the details. As you can see, our database has the three tables we wanted it to have, and correctly identifies what datatypes the data in it are.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT sql FROM sqlite_master WHERE type='table';\")\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\n\nconn.close()\n\nAlways remember to close the database connection after you’re done working with it, as good practice.\n\n\nCreating a More Advanced SQL Query\nNow let’s write an sql query function to help us create a DataFrame we can analyze from the database. I’ll import one I’ve already written from another python file for us to go over.\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"Returns a DataFrame built from a database with columns in order NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp\n\n    Args:\n        db_file (string): String representing name of database .db file\n        country (string): Name of the country to be analyzed\n        year_begin (integer): Number representing first year to gather info from\n        year_end(integer): Number representing last year to gather info from\n        month (integer): Number representing month to gather info from\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n        FROM temperatures T\n        LEFT JOIN stations S ON T.id = S.id\n        LEFT JOIN countries C ON substr(T.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE C.name = '{country}' AND T.year&gt;={year_begin} AND T.year&lt;={year_end} AND T.month = {month}\n        \"\"\"   \n        df = pd.read_sql_query(cmd, conn)\n        df.rename(columns={\"Name\":\"Country\"}, inplace=True)\n    return df\n\n\n\nThis query function is called query_climate_database(), and uses the given arguments. You can read the docstring for a short description of what happens. This function uses sql to return a custom DataFrame containing these columns in order: NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp. Basically, we draw from the different tables in our database to create a new DataFrame that is carries information from csv. We start by opening back up our connection to our database that was created earlier. The main complexities in this process are the JOINS. We use a LEFT JOIN to combine the repeated parts of the temperature and stations table, adding another column to termperatures. Then we use another LEFT JOIN to merge temperatures and countries, using the fact that the first two characters of the index of temperatures is the same as the FIPS index. After these we create a DataFrame from the drawn data. Additionally, we change the name of the “Name” column from the countries table to “Country” for better readability.\nHere’s an example of this query in action:\n\nquery_climate_database(db_file = \"climate.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\nWe have created a DataFrame that displays information regarding the stations and temperature in India over the years, specifically in the month of January.\n\n\nCreating a Geographic Scatterplot to Visualize Data\nLet’s go further by creating a geographic visualization of some data. Our goal through this is to answer the question: “How does the average yearly change in temperature vary within a given country?” We can answer this by creating a geographic scatterplot, among other things. Consider the following functions:\n\nfrom sklearn.linear_model import LinearRegression\n\ndef what_month(month):\n    \"\"\"Returns the month name corresponding with number in chronological order, i.e. 1--&gt;January, 2--&gt;February, and so on\n\n    Args:\n        month (integer): Number representing numerical value of month\n    \"\"\"\n    names = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n    return names[month-1]\n\ndef coef(data_group):\n    \"\"\"Returns the first coefficient of a linear regression model of a station\n    \n    Args:\n        data_group: Data that we perform linear regression on\n    \"\"\"\n    x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n    y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"Returns an interactive geographic scatter plot with a point for each station\n\n    Args:\n        db_file (string): String representing name of database .db file\n        country (string): Name of the country to be analyzed\n        year_begin (integer): Number representing first year to gather info from\n        year_end(integer): Number representing last year to gather info from\n        month (integer): Number representing month to gather info from\n        min_obs (integer): Number of required years of data in a certain month to be included in the plot\n        **kwargs: Any other keyword arguments to assist in the creation of the plot\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    coefs = df.groupby([\"NAME\", \"Month\"]).apply(coef)\n    coefs = coefs.reset_index(name=\"Estimated Yearly Increase (°C)\")\n    filtered_df = df.groupby([\"NAME\", \"Month\"]).filter(lambda x: x[\"Year\"].nunique() &gt;= min_obs)\n    filtered_df = pd.merge(filtered_df, coefs, on=\"NAME\")\n    filtered_df[\"Estimated Yearly Increase (°C)\"] = filtered_df[\"Estimated Yearly Increase (°C)\"].round(4)\n\n    fig = px.scatter_mapbox(filtered_df,\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\",\n                            hover_name = \"NAME\",\n                            color = \"Estimated Yearly Increase (°C)\",\n                            color_continuous_midpoint=0,\n                            title = f\"Estimates of yearly increase in temperature in {what_month(month)} for stations in {country}, years {year_begin}-{year_end}\",\n                            **kwargs)\n    \n    fig.update_layout(coloraxis_colorbar=dict(title='Estimated Yearly Increase in Temperature (°C)'))\n    \n    return fig\n\nThese functions are what we’ll use to create our scatterplot, specifically temperature_coefficient_plot(), with the other two, coef() and what_month() being helpers. Read the docstrings for a quick description of what’s going on. In the former, we have six explicit arguments, db_file, country, year_begin, year_end, month, and min_obs, all of which are that same as our query function except min_obs, which is meant to represent the number of years needed for us to include a specific station into our data visualization. We begin the function by calling our previously defined query function to create a relevant DataFrame. Next, we use the coefs() function. This utilizes linear regression (we’re using scikit-learn’s model, which is imported at the top of the preceding cell) and returns the first coefficient resulting from the process. We use this coefficient as our measure of estimating yearly increases in temperatures. In temperature_coefficient_plot(), we create another DataFrame called coefs using the .apply() method for custom aggregation procedures. Then we merge this DataFrame with another one, called filtered_df which, before the merging, had filtered out all stations which didn’t meant the min_obs requirement, done using the .filter() method and a lambda function as well as the function .nunique, which returns the number of unique entries in a column in a DataFrame. We then use plotly to create a geographic scatterplot very easily. Not all of the usual arguments are here, since we want to allow the user some leeway to play around with visualization. That’s why we have the implicit **kwargs argument.\nNow let’s look at an example. The following outlines a visualization of stations in India that meet the min_obs criteria, and estimates yearly increases in temperatures. We can see all these by simply hovering over any outlined data points.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nHow about another example? What follows is a geographic example of India in the same time period but in December, which some would say is the coldest month.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot(\"climate.db\", \"India\", 1980, 2020, 12, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\nMore Practice: Two Other Visualizations and a Query\nLet’s write a new query function. We can do this by adjusting our original one the mainly focus on stations and countries, and we can change the syntax for both the JOINS. The code is as follows.\n\ndef query_stations_countries(db_file, year, month):\n    \"\"\"Returns a DataFrame built from a database \n\n    Args:\n        db_file (string): String representing name of database .db file\n        year (integer): Number representing year to gather info from\n        month (integer): Number representing month to gather info from\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month\n        FROM stations S\n        LEFT JOIN temperatures T ON S.id = T.id\n        LEFT JOIN countries C ON substr(S.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE S.longitude&lt;0 AND S.longitude&gt;-180 AND T.year&gt;={year} AND T.month = {month} \n        \"\"\"   \n        df = pd.read_sql_query(cmd, conn)\n        df.rename(columns={\"Name\":\"Country\"}, inplace=True)\n        return df\n\nAs you can see, it’s mostly similar, but a bit different in how the joins are organized. Here, stations (S) seems to be the table we’re adding everything on to. Additionally, we’re no longer focusing on one specific country, but a region defind by longitudes.\nNow let’s create some new visualizations. The first one will be a histogram, and we can use the first query to get our information. This one will be a sort of space-time analysis of data. It uses our original query.\n\ndef temperature_spacetime_plot(db_file, country, year_begin, year_end, month, **kwargs):\n    \"\"\"Returns a temperature space-time plot\n\n    Args:\n        db_file: database name\n        country: country we focus on\n        year_begin: beginning of data collection time period\n        year_end: end of data collection time period\n        month: month of data collection we focus on\n        **kwargs: keyword arguments\n    \"\"\"\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    df[\"Rank\"] = df.groupby([\"NAME\", \"Month\"])[\"Temp\"].rank().astype(int)\n    df[\"Len\"] = df.groupby([\"NAME\", \"Month\"])[\"Rank\"].transform(len)\n    df[\"Record\"] = np.where(df[\"Rank\"] == 1, \"Coldest\", np.where(df[\"Rank\"] == df[\"Len\"], \"Warmest\", \"N/A\"))\n    warm_cold_df = df[df[\"Record\"].isin([\"Coldest\", \"Warmest\"])]\n    warm_cold_df = warm_cold_df.drop([\"Rank\", \"Len\"], axis=1)\n\n    fig = px.histogram(warm_cold_df,\n                       x = \"Year\",\n                       color = \"Year\",\n                       facet_row = \"Record\",\n                       facet_row_spacing= 0.01,\n                       barmode = \"overlay\",\n                       width = 600,\n                       height = 600,\n                       **kwargs)\n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.update_layout(title_text=f\"Number of Record Temperatures Occurring During {what_month(month)} Between {year_begin}-{year_end} in {country}\")\n\n    return fig\n\nAgain, we’ll be analyzing India. This plots attempts to answer the question, “How can we quanitfy the effect of global warming through temperatures?” This graph returns “record” temperatures since 1980, i.e. months where the temperature was either “warmest” or “coldest” in a given year. Though there isn’t any obvious correlation, we can clearly see “warmest” records spike in the 2010s. Though the number is lower in 2020, keep in mind a global pandemic went on and people stopped going outside – this may be an outlier year.\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_spacetime_plot(db_file=\"climate.db\", country=\"India\", year_begin=1980, year_end=2020, month=12,\n                                   opacity = 0.5,\n                                   nbins = 25)\n\nfig.show()\n\n\n\n\nNow let’s look at another visualization. This one will be a sort of density/heat map, giving us a way to identify which countries in the world have the most weather measuring stations. Through this, we’ll attempt to answer the question, “Which countries are most aware of temperature, i.e. which longitudes/latitudes are most observant?” Specifically, we’ll focus on the west, since those countries tend to impact us Americans more.\nWe take only the stations located in the western hemishpere, which is longitudes less than 0 but greater than negative 180.\n\ndef query_stations_countries(db_file, year, month):\n    \"\"\"Returns a DataFrame built from a database \n\n    Args:\n        db_file (string): String representing name of database .db file\n        year (integer): Number representing year to gather info from\n        month (integer): Number representing month to gather info from\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month\n        FROM stations S\n        LEFT JOIN temperatures T ON S.id = T.id\n        LEFT JOIN countries C ON substr(S.id, 1, 2) = C.\"FIPS 10-4\"\n        WHERE S.longitude&lt;0 AND S.longitude&gt;-180 AND T.year&gt;={year} AND T.month = {month} \n        \"\"\"   \n        df = pd.read_sql_query(cmd, conn)\n        df.rename(columns={\"Name\":\"Country\"}, inplace=True)\n        return df\n\ndef stations_density_plot(db_file, year, month, **kwargs):\n    \"\"\"Returns a station density plot\n\n    Args:\n        db_file: database name\n        year: year we draw data from\n        month: Month we draw data from, doesn't really matter in this case since stations don't pop up or leave within a month\n        **kwargs: keyword arguments\n    \n    \"\"\"\n    df = query_stations_countries(db_file, year, month)\n    \n    fig = px.density_mapbox(df,\n                      lat = \"LATITUDE\",\n                      lon = \"LONGITUDE\",\n                      color_continuous_midpoint=0,\n                      radius = 4,\n                      zoom = 2,\n                      height = 300,\n                      title = f\"Weather Station Densities In the Western Hemisphere\",\n                      **kwargs)\n    \n    fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = stations_density_plot(db_file=\"climate.db\", year=2020, month=1, \n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nThe US does seem to be pretty packed with stations. That bodes well for the future. We are rapidly becoming conscious of temperature changes and the effects of global warming."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Visualizing and Understanding Trends in Temperature by Country\n\n\n\n\n\n\nData Visualization\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 29, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nHW0\n\n\n\n\n\n\nweek 0\n\n\nHW\n\n\n\n\n\n\n\n\n\nJan 23, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nNarayanan Kannan\n\n\n\n\n\n\nNo matching items"
  }
]